{"cells":[{"cell_type":"markdown","metadata":{"id":"pEIv3gY-aNRk"},"source":["## Setting up the notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45322,"status":"ok","timestamp":1730871669978,"user":{"displayName":"Manala Tyobeka","userId":"00026225086725183959"},"user_tz":-120},"id":"SORAxH2kaAUS","outputId":"90c2e613-373f-480e-f25f-236b545d1f49"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pip==24.0 in /usr/local/lib/python3.10/dist-packages (24.0)\n","Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.10/dist-packages (1.23.5)\n","Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (2.6.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.23.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.1)\n","Requirement already satisfied: protobuf\u003e=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n","Collecting subword-nmt\n","  Using cached subword_nmt-0.3.8-py3-none-any.whl.metadata (9.2 kB)\n","Collecting mock (from subword-nmt)\n","  Using cached mock-5.1.0-py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from subword-nmt) (4.66.6)\n","Using cached subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n","Using cached mock-5.1.0-py3-none-any.whl (30 kB)\n","Installing collected packages: mock, subword-nmt\n","Successfully installed mock-5.1.0 subword-nmt-0.3.8\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n","Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.5)\n","Requirement already satisfied: click!=8.0.0,\u003e=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Requirement already satisfied: docker-pycreds\u003e=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied: gitpython!=3.1.29,\u003e=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,\u003c6,\u003e=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: psutil\u003e=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n","Requirement already satisfied: requests\u003c3,\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n","Requirement already satisfied: sentry-sdk\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.17.0)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n","Requirement already satisfied: typing-extensions\u003c5,\u003e=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n","Requirement already satisfied: six\u003e=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds\u003e=0.4.0-\u003ewandb) (1.16.0)\n","Requirement already satisfied: gitdb\u003c5,\u003e=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,\u003e=1.0.0-\u003ewandb) (4.0.11)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.0.0-\u003ewandb) (3.4.0)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.0.0-\u003ewandb) (3.10)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.0.0-\u003ewandb) (2.2.3)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.0.0-\u003ewandb) (2024.8.30)\n","Requirement already satisfied: smmap\u003c6,\u003e=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb\u003c5,\u003e=4.0.1-\u003egitpython!=3.1.29,\u003e=1.0.0-\u003ewandb) (5.0.1)\n"]}],"source":["# installing packages\n","!pip install pip==24.0\n","!pip install numpy==1.23.5\n","!pip install tensorboardX\n","!pip install subword-nmt\n","!pip install sentencepiece\n","!pip install wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dvjld6oSaYSO"},"outputs":[],"source":["# importing packages\n","import numpy\n","import os\n","import tensorboardX\n","import sentencepiece as spm\n","import wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":136},"executionInfo":{"elapsed":21430,"status":"ok","timestamp":1730871699659,"user":{"displayName":"Manala Tyobeka","userId":"00026225086725183959"},"user_tz":-120},"id":"v0gzvGPiabH_","outputId":"5510a2cc-759e-4bb1-a931-a74771f85815"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"]},{"data":{"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) =\u003e {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() =\u003e {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() =\u003e reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data =\u003e {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "],"text/plain":["\u003cIPython.core.display.Javascript object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W\u0026B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":[" 路路路路路路路路路路\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# setting up wandb for logging model performance during training\n","# api key = b5b05b603ec81167154bcdaec184b83d9e96049b\n","wandb.login()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24256,"status":"ok","timestamp":1730871723911,"user":{"displayName":"Manala Tyobeka","userId":"00026225086725183959"},"user_tz":-120},"id":"8pUxWcbZaZDM","outputId":"b1f2dab1-83ee-4ea6-a7b9-fb778d1266b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# mounting google drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p32nlIrXagar"},"outputs":[],"source":["source_code = 'eng'\n","target_code = 'nso'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":365316,"status":"ok","timestamp":1730872089222,"user":{"displayName":"Manala Tyobeka","userId":"00026225086725183959"},"user_tz":-120},"id":"EKtFVYlralgj","outputId":"0057accf-aba8-48d6-c249-2da573606cdf"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Research/eng-to-nso/fairseq\n","Obtaining file:///content/drive/MyDrive/Research/eng-to-nso/fairseq\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n","  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.17.1)\n","Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (3.0.11)\n","Collecting hydra-core\u003c1.1,\u003e=1.0.7 (from fairseq==0.12.2)\n","  Downloading hydra_core-1.0.7-py3-none-any.whl.metadata (3.7 kB)\n","Collecting omegaconf\u003c2.1 (from fairseq==0.12.2)\n","  Downloading omegaconf-2.0.6-py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: numpy\u003e=1.21.3 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.23.5)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2024.9.11)\n","Collecting sacrebleu\u003e=1.4.12 (from fairseq==0.12.2)\n","  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n","\u001b[2K     \u001b[90m\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch\u003e=1.13 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.5.0+cu121)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (4.66.6)\n","Collecting bitarray (from fairseq==0.12.2)\n","  Downloading bitarray-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\n","Requirement already satisfied: torchaudio\u003e=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.5.0+cu121)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.5.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (24.1)\n","Collecting antlr4-python3-runtime==4.8 (from hydra-core\u003c1.1,\u003e=1.0.7-\u003efairseq==0.12.2)\n","  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n","\u001b[2K     \u001b[90m\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: PyYAML\u003e=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf\u003c2.1-\u003efairseq==0.12.2) (6.0.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf\u003c2.1-\u003efairseq==0.12.2) (4.12.2)\n","Collecting portalocker (from sacrebleu\u003e=1.4.12-\u003efairseq==0.12.2)\n","  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: tabulate\u003e=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu\u003e=1.4.12-\u003efairseq==0.12.2) (0.9.0)\n","Collecting colorama (from sacrebleu\u003e=1.4.12-\u003efairseq==0.12.2)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu\u003e=1.4.12-\u003efairseq==0.12.2) (5.3.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13-\u003efairseq==0.12.2) (3.16.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13-\u003efairseq==0.12.2) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13-\u003efairseq==0.12.2) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13-\u003efairseq==0.12.2) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.13-\u003efairseq==0.12.2) (1.13.1)\n","Requirement already satisfied: mpmath\u003c1.4,\u003e=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1-\u003etorch\u003e=1.13-\u003efairseq==0.12.2) (1.3.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi-\u003efairseq==0.12.2) (2.22)\n","Requirement already satisfied: scipy\u003e=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-\u003efairseq==0.12.2) (1.13.1)\n","Requirement already satisfied: joblib\u003e=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-\u003efairseq==0.12.2) (1.4.2)\n","Requirement already satisfied: threadpoolctl\u003e=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-\u003efairseq==0.12.2) (3.5.0)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch\u003e=1.13-\u003efairseq==0.12.2) (3.0.2)\n","Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n","\u001b[2K   \u001b[90m\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n","Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n","\u001b[2K   \u001b[90m\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bitarray-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (278 kB)\n","\u001b[2K   \u001b[90m\u001b[0m \u001b[32m278.3/278.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n","Building wheels for collected packages: fairseq, antlr4-python3-runtime\n","  Building editable for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fairseq: filename=fairseq-0.12.2-0.editable-cp310-cp310-linux_x86_64.whl size=9581 sha256=a5f0bc28cd7bd55305254db904657ca742390ebba614c6fcf5d61c17151c2433\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-i6rvxwla/wheels/26/b6/b0/27c000b9f95619eae152cc65edcf10b30500efb7e43dc6281f\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141214 sha256=f13acbd48c3bef481fa7f6f3ff3d1e9862f52f325fa444e06cc8fbabc7711eac\n","  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n","Successfully built fairseq antlr4-python3-runtime\n","\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML\u003e=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n","\u001b[0mInstalling collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq\n","Successfully installed antlr4-python3-runtime-4.8 bitarray-3.0.0 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.10.1 sacrebleu-2.4.3\n"]}],"source":["# change working directory\n","os.chdir(f'/content/drive/MyDrive/Research/eng-to-{target_code}')\n","\n","# installing fairseq\n","#!git clone https://github.com/pytorch/fairseq.git\n","%cd fairseq\n","!pip install --editable ./"]},{"cell_type":"markdown","metadata":{"id":"5tmOGRAFauY9"},"source":["## Training NMT model with Byte-Pair Encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PhupCTBja11s"},"outputs":[],"source":["# change working directory\n","os.chdir(f'/content/drive/MyDrive/Research/eng-to-{target_code}/bpe')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":76781,"status":"ok","timestamp":1729758748525,"user":{"displayName":"Manala Tyobeka","userId":"00026225086725183959"},"user_tz":-120},"id":"za5i3Iq-a--5","outputId":"a72c762d-134a-4037-d647-0f9d3db39da0"},"outputs":[{"name":"stdout","output_type":"stream","text":["2024-10-24 07:43:46.984834: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-10-24 07:43:47.229471: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-10-24 07:43:47.300481: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-10-24 07:43:47.691819: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-10-24 07:43:49.213929: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-10-24 07:43:51 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n","2024-10-24 07:46:33 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': 'fairseq-standard-subword-tok-eng-to-nso', 'azureml_logging': False, 'seed': 2024, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 25, 'validate_interval_updates': 2000, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3600, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0003], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints-bpe', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 25, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 5, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project='fairseq-standard-subword-tok-eng-to-nso', azureml_logging=False, seed=2024, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, batch_size=None, required_batch_size_multiple=1, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=25, validate_interval_updates=2000, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid='3600', batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=100, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0003], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='checkpoints-bpe', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=25, save_interval_updates=2000, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='bleu', maximize_best_checkpoint_metric=True, patience=5, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='data-bin', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 0, \"max_len_b\": 100, \"len_pen\": 1}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe='@@ ', eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.999)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=1000, warmup_init_lr=-1, pad=1, eos=2, unk=3, activation_fn='relu', share_decoder_input_output_embed=True, share_all_embeddings=True, encoder_layers=3, encoder_attention_heads=4, encoder_embed_dim=256, encoder_ffn_embed_dim=1024, decoder_layers=3, decoder_attention_heads=4, decoder_embed_dim=256, decoder_ffn_embed_dim=1024, dropout=0.25, no_seed_provided=False, encoder_embed_path=None, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=256, decoder_input_dim=256, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 0, \"max_len_b\": 100, \"len_pen\": 1}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0003]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 1000, 'warmup_init_lr': -1.0, 'lr': [0.0003]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n","2024-10-24 07:46:34 | INFO | fairseq.tasks.translation | [eng] dictionary: 4160 types\n","2024-10-24 07:46:34 | INFO | fairseq.tasks.translation | [nso] dictionary: 4160 types\n","2024-10-24 07:46:34 | INFO | fairseq_cli.train | TransformerModel(\n","  (encoder): TransformerEncoderBase(\n","    (dropout_module): FairseqDropout()\n","    (embed_tokens): Embedding(4160, 256, padding_idx=1)\n","    (embed_positions): SinusoidalPositionalEmbedding()\n","    (layers): ModuleList(\n","      (0-2): 3 x TransformerEncoderLayerBase(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (decoder): TransformerDecoderBase(\n","    (dropout_module): FairseqDropout()\n","    (embed_tokens): Embedding(4160, 256, padding_idx=1)\n","    (embed_positions): SinusoidalPositionalEmbedding()\n","    (layers): ModuleList(\n","      (0-2): 3 x TransformerDecoderLayerBase(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (output_projection): Linear(in_features=256, out_features=4160, bias=False)\n","  )\n",")\n","2024-10-24 07:46:34 | INFO | fairseq_cli.train | task: TranslationTask\n","2024-10-24 07:46:34 | INFO | fairseq_cli.train | model: TransformerModel\n","2024-10-24 07:46:34 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n","2024-10-24 07:46:34 | INFO | fairseq_cli.train | num. shared model params: 6,594,560 (num. trained: 6,594,560)\n","2024-10-24 07:46:34 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n","2024-10-24 07:46:35 | INFO | fairseq.data.data_utils | loaded 6,336 examples from: data-bin/valid.eng-nso.eng\n","2024-10-24 07:46:36 | INFO | fairseq.data.data_utils | loaded 6,336 examples from: data-bin/valid.eng-nso.nso\n","2024-10-24 07:46:36 | INFO | fairseq.tasks.translation | data-bin valid eng-nso 6336 examples\n","2024-10-24 07:46:36 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight \u003c- decoder.embed_tokens.weight\n","2024-10-24 07:46:36 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight \u003c- decoder.output_projection.weight\n","2024-10-24 07:46:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2024-10-24 07:46:36 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n","2024-10-24 07:46:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2024-10-24 07:46:36 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n","2024-10-24 07:46:36 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n","2024-10-24 07:46:36 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints-bpe/checkpoint_last.pt\n","2024-10-24 07:46:36 | INFO | fairseq.trainer | No existing checkpoint found checkpoints-bpe/checkpoint_last.pt\n","2024-10-24 07:46:36 | INFO | fairseq.trainer | loading train data for epoch 1\n","2024-10-24 07:46:37 | INFO | fairseq.data.data_utils | loaded 20,994 examples from: data-bin/train.eng-nso.eng\n","2024-10-24 07:46:38 | INFO | fairseq.data.data_utils | loaded 20,994 examples from: data-bin/train.eng-nso.nso\n","2024-10-24 07:46:38 | INFO | fairseq.tasks.translation | data-bin train eng-nso 20994 examples\n","2024-10-24 07:46:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:46:38 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2024-10-24 07:46:38 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2024-10-24 07:46:38 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2024-10-24 07:46:38 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n","2024-10-24 07:46:39 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n","2024-10-24 07:46:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:46:39 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2024-10-24 07:46:39 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2024-10-24 07:46:39 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2024-10-24 07:46:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 001:   0% 0/213 [00:00\u003c?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtyobeka-mandisa\u001b[0m (\u001b[33mtyobeka-mandisa-university-of-cape-town\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.5\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/Research/eng-to-nso/bpe/wandb/run-20241024_074645-inxtt2xw\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcheckpoints-bpe\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 猸锔 View project at \u001b[34m\u001b[4mhttps://wandb.ai/tyobeka-mandisa-university-of-cape-town/fairseq-standard-subword-tok-eng-to-nso\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/tyobeka-mandisa-university-of-cape-town/fairseq-standard-subword-tok-eng-to-nso/runs/inxtt2xw\u001b[0m\n","2024-10-24 07:46:47 | INFO | fairseq.trainer | begin training epoch 1\n","2024-10-24 07:46:47 | INFO | fairseq_cli.train | Start iterating over samples\n","/content/drive/MyDrive/Research/eng-to-nso/fairseq/fairseq/tasks/fairseq_task.py:531: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","/content/drive/MyDrive/Research/eng-to-nso/fairseq/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n","  warnings.warn(\n","2024-10-24 07:47:08 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n","2024-10-24 07:47:08 | INFO | train | epoch 001 | loss 10.937 | nll_loss 10.73 | ppl 1698.45 | wps 38329.1 | ups 11.78 | wpb 3256.8 | bsz 98.6 | num_updates 213 | lr 6.39e-05 | gnorm 1.514 | train_wall 19 | gb_free 14.1 | wall 31\n","2024-10-24 07:47:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:47:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 002:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:47:08 | INFO | fairseq.trainer | begin training epoch 2\n","2024-10-24 07:47:08 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 07:47:27 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n","2024-10-24 07:47:27 | INFO | train | epoch 002 | loss 9.524 | nll_loss 9.092 | ppl 545.69 | wps 35998.4 | ups 11.05 | wpb 3256.8 | bsz 98.6 | num_updates 426 | lr 0.0001278 | gnorm 0.896 | train_wall 17 | gb_free 14.1 | wall 51\n","2024-10-24 07:47:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:47:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 003:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:47:27 | INFO | fairseq.trainer | begin training epoch 3\n","2024-10-24 07:47:27 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 07:47:46 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n","2024-10-24 07:47:46 | INFO | train | epoch 003 | loss 9.141 | nll_loss 8.626 | ppl 395.1 | wps 36852.8 | ups 11.32 | wpb 3256.8 | bsz 98.6 | num_updates 639 | lr 0.0001917 | gnorm 0.961 | train_wall 16 | gb_free 14.2 | wall 70\n","2024-10-24 07:47:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:47:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 004:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:47:46 | INFO | fairseq.trainer | begin training epoch 4\n","2024-10-24 07:47:46 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 07:48:05 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n","2024-10-24 07:48:05 | INFO | train | epoch 004 | loss 8.478 | nll_loss 7.856 | ppl 231.62 | wps 36804.5 | ups 11.3 | wpb 3256.8 | bsz 98.6 | num_updates 852 | lr 0.0002556 | gnorm 1.134 | train_wall 16 | gb_free 14.2 | wall 88\n","2024-10-24 07:48:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:48:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 005:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:48:05 | INFO | fairseq.trainer | begin training epoch 5\n","2024-10-24 07:48:05 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 07:48:23 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n","2024-10-24 07:48:23 | INFO | train | epoch 005 | loss 7.838 | nll_loss 7.104 | ppl 137.61 | wps 37000.7 | ups 11.36 | wpb 3256.8 | bsz 98.6 | num_updates 1065 | lr 0.000290701 | gnorm 0.963 | train_wall 16 | gb_free 14.1 | wall 107\n","2024-10-24 07:48:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:48:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 006:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:48:24 | INFO | fairseq.trainer | begin training epoch 6\n","2024-10-24 07:48:24 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 07:48:43 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n","2024-10-24 07:48:43 | INFO | train | epoch 006 | loss 7.423 | nll_loss 6.617 | ppl 98.15 | wps 36336 | ups 11.16 | wpb 3256.8 | bsz 98.6 | num_updates 1278 | lr 0.000265372 | gnorm 0.987 | train_wall 17 | gb_free 14.2 | wall 126\n","2024-10-24 07:48:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:48:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 007:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:48:43 | INFO | fairseq.trainer | begin training epoch 7\n","2024-10-24 07:48:43 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 07:49:01 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n","2024-10-24 07:49:01 | INFO | train | epoch 007 | loss 7.126 | nll_loss 6.268 | ppl 77.09 | wps 38170.2 | ups 11.72 | wpb 3256.8 | bsz 98.6 | num_updates 1491 | lr 0.000245687 | gnorm 0.968 | train_wall 16 | gb_free 14.1 | wall 144\n","2024-10-24 07:49:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:49:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 008:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:49:01 | INFO | fairseq.trainer | begin training epoch 8\n","2024-10-24 07:49:01 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 07:49:19 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n","2024-10-24 07:49:19 | INFO | train | epoch 008 | loss 6.877 | nll_loss 5.978 | ppl 63.02 | wps 37845.5 | ups 11.62 | wpb 3256.8 | bsz 98.6 | num_updates 1704 | lr 0.000229819 | gnorm 0.905 | train_wall 17 | gb_free 14.1 | wall 163\n","2024-10-24 07:49:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:49:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 009:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:49:19 | INFO | fairseq.trainer | begin training epoch 9\n","2024-10-24 07:49:19 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 07:49:37 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n","2024-10-24 07:49:37 | INFO | train | epoch 009 | loss 6.702 | nll_loss 5.772 | ppl 54.65 | wps 38593.4 | ups 11.85 | wpb 3256.8 | bsz 98.6 | num_updates 1917 | lr 0.000216676 | gnorm 0.931 | train_wall 16 | gb_free 14 | wall 181\n","2024-10-24 07:49:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:49:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 010:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:49:37 | INFO | fairseq.trainer | begin training epoch 10\n","2024-10-24 07:49:37 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 010:  38% 81/213 [00:06\u003c00:10, 12.82it/s]2024-10-24 07:49:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 07:49:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 010 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   1% 1/84 [00:01\u003c02:24,  1.74s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:   2% 2/84 [00:03\u003c01:59,  1.46s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:   4% 3/84 [00:03\u003c01:29,  1.11s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:   5% 4/84 [00:04\u003c01:31,  1.15s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:   6% 5/84 [00:05\u003c01:26,  1.09s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:   7% 6/84 [00:06\u003c01:13,  1.06it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   8% 7/84 [00:07\u003c01:04,  1.19it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  10% 8/84 [00:07\u003c00:55,  1.36it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  11% 9/84 [00:08\u003c00:57,  1.31it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  12% 10/84 [00:09\u003c00:58,  1.27it/s]\u001b[A\n","epoch 010:  38% 81/213 [00:17\u003c00:10, 12.82it/s, loss=6.557, nll_loss=5.605, ppl=48.66, wps=40806.3, ups=12.37, wpb=3299.1, bsz=102.8, num_updates=2000, lr=0.000212132, gnorm=0.911, train_wall=8, gb_free=14.1, wall=187]\n","epoch 010 | valid on 'valid' subset:  14% 12/84 [00:10\u003c00:53,  1.33it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  15% 13/84 [00:11\u003c00:50,  1.41it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  17% 14/84 [00:12\u003c00:51,  1.35it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  18% 15/84 [00:13\u003c00:56,  1.23it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  19% 16/84 [00:13\u003c00:50,  1.35it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  20% 17/84 [00:14\u003c00:53,  1.26it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  21% 18/84 [00:15\u003c00:52,  1.26it/s]\u001b[A2024-10-24 07:50:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 07:50:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 07:50:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 010 | valid on 'valid' subset:  23% 19/84 [00:16\u003c00:56,  1.15it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  24% 20/84 [00:17\u003c00:56,  1.13it/s]\u001b[A2024-10-24 07:50:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 07:50:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 07:50:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 010 | valid on 'valid' subset:  25% 21/84 [00:18\u003c01:03,  1.01s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  26% 22/84 [00:20\u003c01:07,  1.09s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  27% 23/84 [00:21\u003c01:03,  1.04s/it]\u001b[A2024-10-24 07:50:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 07:50:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 07:50:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 010 | valid on 'valid' subset:  29% 24/84 [00:22\u003c01:10,  1.17s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  30% 25/84 [00:23\u003c01:04,  1.10s/it]\u001b[A2024-10-24 07:50:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 07:50:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 07:50:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 010 | valid on 'valid' subset:  31% 26/84 [00:24\u003c01:01,  1.06s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  32% 27/84 [00:25\u003c00:57,  1.00s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  33% 28/84 [00:26\u003c00:53,  1.05it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  35% 29/84 [00:26\u003c00:47,  1.16it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  36% 30/84 [00:27\u003c00:45,  1.18it/s]\u001b[A2024-10-24 07:50:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 07:50:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 07:50:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 010 | valid on 'valid' subset:  37% 31/84 [00:28\u003c00:46,  1.13it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  38% 32/84 [00:29\u003c00:44,  1.16it/s]\u001b[A2024-10-24 07:50:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 07:50:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 07:50:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 010 | valid on 'valid' subset:  39% 33/84 [00:30\u003c00:45,  1.12it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  40% 34/84 [00:31\u003c00:44,  1.12it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  42% 35/84 [00:32\u003c00:43,  1.14it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  43% 36/84 [00:33\u003c00:45,  1.06it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  44% 37/84 [00:34\u003c00:47,  1.01s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  45% 38/84 [00:35\u003c00:50,  1.09s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  46% 39/84 [00:36\u003c00:46,  1.04s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  48% 40/84 [00:37\u003c00:50,  1.14s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  49% 41/84 [00:39\u003c00:49,  1.14s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  50% 42/84 [00:40\u003c00:46,  1.10s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  51% 43/84 [00:40\u003c00:42,  1.04s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  52% 44/84 [00:41\u003c00:41,  1.03s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  54% 45/84 [00:42\u003c00:38,  1.01it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  55% 46/84 [00:43\u003c00:35,  1.06it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  56% 47/84 [00:44\u003c00:35,  1.05it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  57% 48/84 [00:45\u003c00:33,  1.08it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  58% 49/84 [00:46\u003c00:32,  1.08it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  60% 50/84 [00:47\u003c00:31,  1.08it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  61% 51/84 [00:48\u003c00:30,  1.08it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  62% 52/84 [00:49\u003c00:31,  1.02it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  63% 53/84 [00:50\u003c00:33,  1.07s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  64% 54/84 [00:51\u003c00:33,  1.11s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  65% 55/84 [00:53\u003c00:35,  1.21s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  67% 56/84 [00:54\u003c00:34,  1.24s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  68% 57/84 [00:55\u003c00:31,  1.18s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  69% 58/84 [00:56\u003c00:28,  1.10s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  70% 59/84 [00:57\u003c00:26,  1.07s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  71% 60/84 [00:58\u003c00:24,  1.02s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  73% 61/84 [00:59\u003c00:23,  1.02s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  74% 62/84 [01:00\u003c00:21,  1.03it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  75% 63/84 [01:01\u003c00:20,  1.03it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  76% 64/84 [01:02\u003c00:19,  1.05it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  77% 65/84 [01:03\u003c00:17,  1.06it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  79% 66/84 [01:04\u003c00:16,  1.07it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  80% 67/84 [01:05\u003c00:16,  1.02it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  81% 68/84 [01:06\u003c00:16,  1.06s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  82% 69/84 [01:07\u003c00:16,  1.11s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  83% 70/84 [01:08\u003c00:16,  1.18s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  85% 71/84 [01:10\u003c00:15,  1.21s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  86% 72/84 [01:11\u003c00:14,  1.19s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  87% 73/84 [01:12\u003c00:12,  1.11s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  88% 74/84 [01:13\u003c00:10,  1.04s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  89% 75/84 [01:14\u003c00:08,  1.02it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  90% 76/84 [01:14\u003c00:07,  1.04it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  92% 77/84 [01:15\u003c00:06,  1.06it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  93% 78/84 [01:16\u003c00:05,  1.08it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  94% 79/84 [01:17\u003c00:04,  1.07it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  95% 80/84 [01:18\u003c00:03,  1.09it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  96% 81/84 [01:19\u003c00:02,  1.10it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  98% 82/84 [01:20\u003c00:01,  1.10it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  99% 83/84 [01:21\u003c00:00,  1.06it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset: 100% 84/84 [01:22\u003c00:00,  1.01it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 07:51:06 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.333 | nll_loss 5.276 | ppl 38.74 | bleu 3.96 | wps 2493.2 | wpb 2417.9 | bsz 75.4 | num_updates 2000\n","2024-10-24 07:51:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 2000 updates\n","2024-10-24 07:51:06 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_10_2000.pt\n","2024-10-24 07:51:07 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_10_2000.pt\n","2024-10-24 07:51:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpe/checkpoint_10_2000.pt (epoch 10 @ 2000 updates, score 3.96) (writing took 1.458015049000096 seconds)\n","2024-10-24 07:51:20 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n","2024-10-24 07:51:20 | INFO | train | epoch 010 | loss 6.541 | nll_loss 5.584 | ppl 47.98 | wps 6727.7 | ups 2.07 | wpb 3256.8 | bsz 98.6 | num_updates 2130 | lr 0.000205557 | gnorm 0.907 | train_wall 18 | gb_free 14.1 | wall 284\n","2024-10-24 07:51:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:51:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 011:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:51:20 | INFO | fairseq.trainer | begin training epoch 11\n","2024-10-24 07:51:20 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 07:51:41 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n","2024-10-24 07:51:41 | INFO | train | epoch 011 | loss 6.413 | nll_loss 5.435 | ppl 43.26 | wps 33785.7 | ups 10.37 | wpb 3256.8 | bsz 98.6 | num_updates 2343 | lr 0.000195991 | gnorm 0.912 | train_wall 18 | gb_free 14.1 | wall 304\n","2024-10-24 07:51:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:51:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 012:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:51:41 | INFO | fairseq.trainer | begin training epoch 12\n","2024-10-24 07:51:41 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 07:51:59 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n","2024-10-24 07:51:59 | INFO | train | epoch 012 | loss 6.307 | nll_loss 5.31 | ppl 39.68 | wps 36988.6 | ups 11.36 | wpb 3256.8 | bsz 98.6 | num_updates 2556 | lr 0.000187647 | gnorm 0.943 | train_wall 17 | gb_free 14.1 | wall 323\n","2024-10-24 07:51:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:51:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 013:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:52:00 | INFO | fairseq.trainer | begin training epoch 13\n","2024-10-24 07:52:00 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 07:52:17 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n","2024-10-24 07:52:17 | INFO | train | epoch 013 | loss 6.203 | nll_loss 5.188 | ppl 36.46 | wps 38721.8 | ups 11.89 | wpb 3256.8 | bsz 98.6 | num_updates 2769 | lr 0.000180285 | gnorm 0.934 | train_wall 16 | gb_free 14 | wall 341\n","2024-10-24 07:52:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:52:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 014:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:52:18 | INFO | fairseq.trainer | begin training epoch 14\n","2024-10-24 07:52:18 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 07:52:35 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n","2024-10-24 07:52:35 | INFO | train | epoch 014 | loss 6.118 | nll_loss 5.088 | ppl 34.01 | wps 38609.3 | ups 11.85 | wpb 3256.8 | bsz 98.6 | num_updates 2982 | lr 0.000173727 | gnorm 0.971 | train_wall 16 | gb_free 14.1 | wall 359\n","2024-10-24 07:52:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:52:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 015:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:52:35 | INFO | fairseq.trainer | begin training epoch 15\n","2024-10-24 07:52:35 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 07:52:54 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n","2024-10-24 07:52:54 | INFO | train | epoch 015 | loss 6.041 | nll_loss 4.997 | ppl 31.93 | wps 37699.6 | ups 11.58 | wpb 3256.8 | bsz 98.6 | num_updates 3195 | lr 0.000167836 | gnorm 0.982 | train_wall 17 | gb_free 14.1 | wall 377\n","2024-10-24 07:52:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:52:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 016:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:52:54 | INFO | fairseq.trainer | begin training epoch 16\n","2024-10-24 07:52:54 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 07:53:12 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n","2024-10-24 07:53:12 | INFO | train | epoch 016 | loss 5.969 | nll_loss 4.912 | ppl 30.11 | wps 38242.1 | ups 11.74 | wpb 3256.8 | bsz 98.6 | num_updates 3408 | lr 0.000162507 | gnorm 0.985 | train_wall 16 | gb_free 14.1 | wall 396\n","2024-10-24 07:53:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:53:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 017:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:53:12 | INFO | fairseq.trainer | begin training epoch 17\n","2024-10-24 07:53:12 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 07:53:31 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n","2024-10-24 07:53:31 | INFO | train | epoch 017 | loss 5.907 | nll_loss 4.839 | ppl 28.62 | wps 36891.2 | ups 11.33 | wpb 3256.8 | bsz 98.6 | num_updates 3621 | lr 0.000157655 | gnorm 1.001 | train_wall 17 | gb_free 14.1 | wall 414\n","2024-10-24 07:53:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:53:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 018:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:53:31 | INFO | fairseq.trainer | begin training epoch 18\n","2024-10-24 07:53:31 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 07:53:49 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n","2024-10-24 07:53:49 | INFO | train | epoch 018 | loss 5.837 | nll_loss 4.757 | ppl 27.03 | wps 37388.6 | ups 11.48 | wpb 3256.8 | bsz 98.6 | num_updates 3834 | lr 0.000153213 | gnorm 0.979 | train_wall 17 | gb_free 14.1 | wall 433\n","2024-10-24 07:53:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:53:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 019:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:53:49 | INFO | fairseq.trainer | begin training epoch 19\n","2024-10-24 07:53:49 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 019:  77% 164/213 [00:14\u003c00:04, 10.10it/s, loss=5.802, nll_loss=4.715, ppl=26.27, wps=35960.4, ups=11.14, wpb=3227.3, bsz=96.5, num_updates=3900, lr=0.000151911, gnorm=0.988, train_wall=8, gb_free=14, wall=439]2024-10-24 07:54:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 07:54:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 019 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:   1% 1/84 [00:00\u003c01:16,  1.09it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:   2% 2/84 [00:01\u003c01:00,  1.36it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:   4% 3/84 [00:02\u003c00:51,  1.57it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:   5% 4/84 [00:02\u003c00:49,  1.60it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:   6% 5/84 [00:03\u003c00:45,  1.73it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:   7% 6/84 [00:03\u003c00:45,  1.70it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:   8% 7/84 [00:04\u003c00:40,  1.91it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  10% 8/84 [00:04\u003c00:37,  2.02it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  11% 9/84 [00:05\u003c00:45,  1.66it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  12% 10/84 [00:06\u003c00:49,  1.50it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  13% 11/84 [00:06\u003c00:44,  1.64it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  14% 12/84 [00:07\u003c00:48,  1.48it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  15% 13/84 [00:08\u003c00:52,  1.34it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  17% 14/84 [00:08\u003c00:45,  1.54it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  18% 15/84 [00:09\u003c00:48,  1.41it/s]\u001b[A\n","epoch 019:  77% 164/213 [00:25\u003c00:04, 10.10it/s, loss=5.794, nll_loss=4.706, ppl=26.11, wps=38793.5, ups=12.01, wpb=3229.3, bsz=95.5, num_updates=4000, lr=0.00015, gnorm=1.004, train_wall=8, gb_free=14.1, wall=447]  \n","epoch 019 | valid on 'valid' subset:  20% 17/84 [00:11\u003c00:47,  1.41it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  21% 18/84 [00:11\u003c00:44,  1.49it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  23% 19/84 [00:12\u003c00:50,  1.29it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  24% 20/84 [00:13\u003c00:49,  1.28it/s]\u001b[A2024-10-24 07:54:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 07:54:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 07:54:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 019 | valid on 'valid' subset:  25% 21/84 [00:14\u003c00:48,  1.30it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  26% 22/84 [00:15\u003c00:54,  1.14it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  27% 23/84 [00:16\u003c00:53,  1.13it/s]\u001b[A2024-10-24 07:54:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 07:54:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 07:54:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 019 | valid on 'valid' subset:  29% 24/84 [00:17\u003c01:03,  1.06s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  30% 25/84 [00:18\u003c00:59,  1.01s/it]\u001b[A2024-10-24 07:54:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 07:54:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 07:54:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 019 | valid on 'valid' subset:  31% 26/84 [00:19\u003c00:59,  1.03s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  32% 27/84 [00:20\u003c00:58,  1.02s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  33% 28/84 [00:21\u003c00:53,  1.04it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  35% 29/84 [00:22\u003c00:49,  1.11it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  36% 30/84 [00:22\u003c00:45,  1.19it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  37% 31/84 [00:23\u003c00:46,  1.13it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  38% 32/84 [00:24\u003c00:44,  1.16it/s]\u001b[A2024-10-24 07:54:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 07:54:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 07:54:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 019 | valid on 'valid' subset:  39% 33/84 [00:25\u003c00:45,  1.12it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  40% 34/84 [00:26\u003c00:45,  1.11it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  42% 35/84 [00:27\u003c00:39,  1.25it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  43% 36/84 [00:28\u003c00:40,  1.18it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  44% 37/84 [00:29\u003c00:40,  1.16it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  45% 38/84 [00:29\u003c00:38,  1.20it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  46% 39/84 [00:30\u003c00:35,  1.25it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  48% 40/84 [00:31\u003c00:41,  1.06it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  49% 41/84 [00:32\u003c00:43,  1.02s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  50% 42/84 [00:34\u003c00:47,  1.13s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  51% 43/84 [00:35\u003c00:45,  1.10s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  52% 44/84 [00:36\u003c00:39,  1.01it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  54% 45/84 [00:36\u003c00:36,  1.06it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  55% 46/84 [00:37\u003c00:34,  1.09it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  56% 47/84 [00:38\u003c00:32,  1.14it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  57% 48/84 [00:39\u003c00:29,  1.23it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  58% 49/84 [00:40\u003c00:28,  1.23it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  60% 50/84 [00:40\u003c00:25,  1.34it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  61% 51/84 [00:41\u003c00:26,  1.25it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  62% 52/84 [00:42\u003c00:24,  1.32it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  63% 53/84 [00:43\u003c00:25,  1.24it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  64% 54/84 [00:43\u003c00:22,  1.32it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  65% 55/84 [00:44\u003c00:22,  1.27it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  67% 56/84 [00:45\u003c00:24,  1.16it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  68% 57/84 [00:46\u003c00:26,  1.02it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  69% 58/84 [00:48\u003c00:27,  1.05s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  70% 59/84 [00:49\u003c00:30,  1.20s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  71% 60/84 [00:50\u003c00:29,  1.22s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  73% 61/84 [00:52\u003c00:27,  1.18s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  74% 62/84 [00:52\u003c00:23,  1.06s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  75% 63/84 [00:53\u003c00:21,  1.03s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  76% 64/84 [00:54\u003c00:19,  1.02it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  77% 65/84 [00:55\u003c00:18,  1.05it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  79% 66/84 [00:56\u003c00:16,  1.10it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  80% 67/84 [00:57\u003c00:15,  1.13it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  81% 68/84 [00:58\u003c00:14,  1.12it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  82% 69/84 [00:58\u003c00:13,  1.13it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  83% 70/84 [00:59\u003c00:12,  1.08it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  85% 71/84 [01:00\u003c00:12,  1.07it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  86% 72/84 [01:02\u003c00:11,  1.01it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  87% 73/84 [01:03\u003c00:11,  1.06s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  88% 74/84 [01:04\u003c00:11,  1.12s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  89% 75/84 [01:05\u003c00:10,  1.19s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  90% 76/84 [01:07\u003c00:09,  1.22s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  92% 77/84 [01:08\u003c00:07,  1.11s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  93% 78/84 [01:09\u003c00:06,  1.07s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  94% 79/84 [01:09\u003c00:05,  1.04s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  95% 80/84 [01:10\u003c00:04,  1.01s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  96% 81/84 [01:11\u003c00:02,  1.03it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  98% 82/84 [01:12\u003c00:01,  1.04it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  99% 83/84 [01:13\u003c00:00,  1.07it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset: 100% 84/84 [01:14\u003c00:00,  1.11it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 07:55:18 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 5.668 | nll_loss 4.463 | ppl 22.06 | bleu 9.3 | wps 2738.2 | wpb 2417.9 | bsz 75.4 | num_updates 4000 | best_bleu 9.3\n","2024-10-24 07:55:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 4000 updates\n","2024-10-24 07:55:18 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_19_4000.pt\n","2024-10-24 07:55:18 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_19_4000.pt\n","2024-10-24 07:55:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpe/checkpoint_19_4000.pt (epoch 19 @ 4000 updates, score 9.3) (writing took 1.1800950330000433 seconds)\n","2024-10-24 07:55:23 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n","2024-10-24 07:55:23 | INFO | train | epoch 019 | loss 5.785 | nll_loss 4.696 | ppl 25.91 | wps 7370.4 | ups 2.26 | wpb 3256.8 | bsz 98.6 | num_updates 4047 | lr 0.000149126 | gnorm 0.999 | train_wall 17 | gb_free 14.1 | wall 527\n","2024-10-24 07:55:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:55:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 020:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:55:24 | INFO | fairseq.trainer | begin training epoch 20\n","2024-10-24 07:55:24 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 07:55:44 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n","2024-10-24 07:55:44 | INFO | train | epoch 020 | loss 5.733 | nll_loss 4.634 | ppl 24.83 | wps 34285.4 | ups 10.53 | wpb 3256.8 | bsz 98.6 | num_updates 4260 | lr 0.00014535 | gnorm 1.018 | train_wall 18 | gb_free 14.1 | wall 547\n","2024-10-24 07:55:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:55:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 021:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:55:44 | INFO | fairseq.trainer | begin training epoch 21\n","2024-10-24 07:55:44 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 07:56:02 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n","2024-10-24 07:56:02 | INFO | train | epoch 021 | loss 5.682 | nll_loss 4.574 | ppl 23.82 | wps 38106.8 | ups 11.7 | wpb 3256.8 | bsz 98.6 | num_updates 4473 | lr 0.000141848 | gnorm 1.017 | train_wall 17 | gb_free 14 | wall 566\n","2024-10-24 07:56:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:56:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 022:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:56:02 | INFO | fairseq.trainer | begin training epoch 22\n","2024-10-24 07:56:02 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 07:56:20 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n","2024-10-24 07:56:20 | INFO | train | epoch 022 | loss 5.635 | nll_loss 4.519 | ppl 22.92 | wps 38284.8 | ups 11.76 | wpb 3256.8 | bsz 98.6 | num_updates 4686 | lr 0.000138586 | gnorm 1.019 | train_wall 17 | gb_free 14.1 | wall 584\n","2024-10-24 07:56:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:56:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 023:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:56:20 | INFO | fairseq.trainer | begin training epoch 23\n","2024-10-24 07:56:20 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 07:56:38 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n","2024-10-24 07:56:38 | INFO | train | epoch 023 | loss 5.6 | nll_loss 4.477 | ppl 22.26 | wps 37935 | ups 11.65 | wpb 3256.8 | bsz 98.6 | num_updates 4899 | lr 0.00013554 | gnorm 1.044 | train_wall 17 | gb_free 14 | wall 602\n","2024-10-24 07:56:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:56:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 024:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:56:38 | INFO | fairseq.trainer | begin training epoch 24\n","2024-10-24 07:56:38 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 07:56:58 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n","2024-10-24 07:56:58 | INFO | train | epoch 024 | loss 5.551 | nll_loss 4.42 | ppl 21.41 | wps 34840.7 | ups 10.7 | wpb 3256.8 | bsz 98.6 | num_updates 5112 | lr 0.000132686 | gnorm 1.043 | train_wall 18 | gb_free 14.1 | wall 622\n","2024-10-24 07:56:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:56:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 025:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:56:58 | INFO | fairseq.trainer | begin training epoch 25\n","2024-10-24 07:56:58 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 025:  99% 211/213 [00:18\u003c00:00, 12.11it/s, loss=5.539, nll_loss=4.404, ppl=21.17, wps=38199.7, ups=11.89, wpb=3213.3, bsz=93.9, num_updates=5300, lr=0.000130312, gnorm=1.039, train_wall=8, gb_free=14.1, wall=638]2024-10-24 07:57:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 07:57:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 025 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:   1% 1/84 [00:01\u003c01:23,  1.01s/it]\u001b[A\n","epoch 025 | valid on 'valid' subset:   2% 2/84 [00:01\u003c00:56,  1.45it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:   4% 3/84 [00:01\u003c00:43,  1.85it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:   5% 4/84 [00:02\u003c00:40,  1.99it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:   6% 5/84 [00:02\u003c00:36,  2.15it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:   7% 6/84 [00:03\u003c00:46,  1.68it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:   8% 7/84 [00:03\u003c00:42,  1.81it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  10% 8/84 [00:04\u003c00:38,  1.95it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  11% 9/84 [00:05\u003c00:45,  1.64it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  12% 10/84 [00:05\u003c00:44,  1.67it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  13% 11/84 [00:06\u003c00:45,  1.60it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  14% 12/84 [00:07\u003c00:48,  1.47it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  15% 13/84 [00:07\u003c00:47,  1.50it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  17% 14/84 [00:08\u003c00:42,  1.66it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  18% 15/84 [00:09\u003c00:49,  1.39it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  19% 16/84 [00:09\u003c00:44,  1.53it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  20% 17/84 [00:10\u003c00:45,  1.48it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  21% 18/84 [00:11\u003c00:43,  1.52it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  23% 19/84 [00:12\u003c00:46,  1.39it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  24% 20/84 [00:12\u003c00:47,  1.33it/s]\u001b[A2024-10-24 07:57:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 07:57:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 07:57:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 025 | valid on 'valid' subset:  25% 21/84 [00:13\u003c00:48,  1.29it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  26% 22/84 [00:15\u003c00:58,  1.06it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  27% 23/84 [00:15\u003c00:54,  1.13it/s]\u001b[A2024-10-24 07:57:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 07:57:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 07:57:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 025:  99% 211/213 [00:36\u003c00:00, 12.11it/s, loss=5.539, nll_loss=4.404, ppl=21.17, wps=38199.7, ups=11.89, wpb=3213.3, bsz=93.9, num_updates=5300, lr=0.000130312, gnorm=1.039, train_wall=8, gb_free=14.1, wall=638]\n","epoch 025 | valid on 'valid' subset:  30% 25/84 [00:17\u003c00:51,  1.14it/s]\u001b[A2024-10-24 07:57:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 07:57:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 07:57:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 025 | valid on 'valid' subset:  31% 26/84 [00:18\u003c00:47,  1.23it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  32% 27/84 [00:18\u003c00:42,  1.33it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  33% 28/84 [00:19\u003c00:38,  1.45it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  35% 29/84 [00:20\u003c00:39,  1.38it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  36% 30/84 [00:20\u003c00:36,  1.48it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  37% 31/84 [00:21\u003c00:39,  1.36it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  38% 32/84 [00:22\u003c00:37,  1.37it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  39% 33/84 [00:23\u003c00:40,  1.25it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  40% 34/84 [00:24\u003c00:41,  1.22it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  42% 35/84 [00:24\u003c00:37,  1.30it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  43% 36/84 [00:25\u003c00:39,  1.20it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  44% 37/84 [00:26\u003c00:36,  1.29it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  45% 38/84 [00:27\u003c00:37,  1.22it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  46% 39/84 [00:28\u003c00:35,  1.28it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  48% 40/84 [00:29\u003c00:41,  1.05it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  49% 41/84 [00:30\u003c00:42,  1.02it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  50% 42/84 [00:31\u003c00:45,  1.08s/it]\u001b[A\n","epoch 025 | valid on 'valid' subset:  51% 43/84 [00:32\u003c00:41,  1.00s/it]\u001b[A\n","epoch 025 | valid on 'valid' subset:  52% 44/84 [00:33\u003c00:37,  1.06it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  54% 45/84 [00:34\u003c00:33,  1.16it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  55% 46/84 [00:34\u003c00:30,  1.23it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  56% 47/84 [00:35\u003c00:28,  1.31it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  57% 48/84 [00:36\u003c00:25,  1.41it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  58% 49/84 [00:36\u003c00:25,  1.39it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  60% 50/84 [00:37\u003c00:23,  1.47it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  61% 51/84 [00:38\u003c00:25,  1.29it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  62% 52/84 [00:39\u003c00:22,  1.41it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  63% 53/84 [00:40\u003c00:24,  1.27it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  64% 54/84 [00:40\u003c00:21,  1.37it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  65% 55/84 [00:41\u003c00:21,  1.38it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  67% 56/84 [00:41\u003c00:19,  1.45it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  68% 57/84 [00:42\u003c00:20,  1.33it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  69% 58/84 [00:43\u003c00:20,  1.30it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  70% 59/84 [00:44\u003c00:20,  1.24it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  71% 60/84 [00:45\u003c00:19,  1.25it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  73% 61/84 [00:46\u003c00:18,  1.24it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  74% 62/84 [00:46\u003c00:17,  1.24it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  75% 63/84 [00:48\u003c00:20,  1.03it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  76% 64/84 [00:49\u003c00:21,  1.06s/it]\u001b[A\n","epoch 025 | valid on 'valid' subset:  77% 65/84 [00:50\u003c00:18,  1.01it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  79% 66/84 [00:51\u003c00:17,  1.05it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  80% 67/84 [00:51\u003c00:14,  1.18it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  81% 68/84 [00:52\u003c00:12,  1.26it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  82% 69/84 [00:53\u003c00:11,  1.33it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  83% 70/84 [00:54\u003c00:11,  1.26it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  85% 71/84 [00:54\u003c00:10,  1.24it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  86% 72/84 [00:55\u003c00:09,  1.33it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  87% 73/84 [00:56\u003c00:07,  1.41it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  88% 74/84 [00:56\u003c00:06,  1.44it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  89% 75/84 [00:57\u003c00:06,  1.36it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  90% 76/84 [00:58\u003c00:06,  1.27it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  92% 77/84 [00:59\u003c00:05,  1.32it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  93% 78/84 [01:00\u003c00:04,  1.23it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  94% 79/84 [01:01\u003c00:04,  1.19it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  95% 80/84 [01:02\u003c00:03,  1.13it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  96% 81/84 [01:03\u003c00:03,  1.02s/it]\u001b[A\n","epoch 025 | valid on 'valid' subset:  98% 82/84 [01:04\u003c00:02,  1.12s/it]\u001b[A\n","epoch 025 | valid on 'valid' subset:  99% 83/84 [01:05\u003c00:01,  1.15s/it]\u001b[A\n","epoch 025 | valid on 'valid' subset: 100% 84/84 [01:06\u003c00:00,  1.09s/it]\u001b[A\n","                                                                        \u001b[A2024-10-24 07:58:24 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 5.447 | nll_loss 4.186 | ppl 18.2 | bleu 11.69 | wps 3053.2 | wpb 2417.9 | bsz 75.4 | num_updates 5325 | best_bleu 11.69\n","2024-10-24 07:58:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 5325 updates\n","2024-10-24 07:58:24 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_best.pt\n","2024-10-24 07:58:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_best.pt\n","2024-10-24 07:58:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpe/checkpoint_best.pt (epoch 25 @ 5325 updates, score 11.69) (writing took 0.8168194849999963 seconds)\n","2024-10-24 07:58:25 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n","2024-10-24 07:58:25 | INFO | train | epoch 025 | loss 5.513 | nll_loss 4.375 | ppl 20.75 | wps 8023.9 | ups 2.46 | wpb 3256.8 | bsz 98.6 | num_updates 5325 | lr 0.000130005 | gnorm 1.046 | train_wall 17 | gb_free 14 | wall 708\n","2024-10-24 07:58:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:58:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 026:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:58:25 | INFO | fairseq.trainer | begin training epoch 26\n","2024-10-24 07:58:25 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 07:58:44 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n","2024-10-24 07:58:44 | INFO | train | epoch 026 | loss 5.477 | nll_loss 4.332 | ppl 20.14 | wps 35715 | ups 10.97 | wpb 3256.8 | bsz 98.6 | num_updates 5538 | lr 0.000127481 | gnorm 1.047 | train_wall 18 | gb_free 14.1 | wall 728\n","2024-10-24 07:58:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:58:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 027:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:58:44 | INFO | fairseq.trainer | begin training epoch 27\n","2024-10-24 07:58:44 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 07:59:03 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n","2024-10-24 07:59:03 | INFO | train | epoch 027 | loss 5.443 | nll_loss 4.292 | ppl 19.59 | wps 37539.2 | ups 11.53 | wpb 3256.8 | bsz 98.6 | num_updates 5751 | lr 0.000125098 | gnorm 1.058 | train_wall 17 | gb_free 14.2 | wall 746\n","2024-10-24 07:59:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:59:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 028:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:59:03 | INFO | fairseq.trainer | begin training epoch 28\n","2024-10-24 07:59:03 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 07:59:21 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)\n","2024-10-24 07:59:21 | INFO | train | epoch 028 | loss 5.411 | nll_loss 4.254 | ppl 19.08 | wps 37165.5 | ups 11.41 | wpb 3256.8 | bsz 98.6 | num_updates 5964 | lr 0.000122844 | gnorm 1.061 | train_wall 17 | gb_free 14.1 | wall 765\n","2024-10-24 07:59:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 07:59:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 029:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 07:59:21 | INFO | fairseq.trainer | begin training epoch 29\n","2024-10-24 07:59:21 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 029:  16% 34/213 [00:03\u003c00:16, 10.65it/s]2024-10-24 07:59:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 07:59:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 029 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:   1% 1/84 [00:00\u003c01:20,  1.03it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:   2% 2/84 [00:01\u003c01:03,  1.29it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:   4% 3/84 [00:02\u003c00:52,  1.55it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:   5% 4/84 [00:02\u003c00:47,  1.70it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:   6% 5/84 [00:03\u003c00:43,  1.82it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:   7% 6/84 [00:03\u003c00:50,  1.55it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:   8% 7/84 [00:04\u003c00:44,  1.74it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  10% 8/84 [00:04\u003c00:39,  1.92it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  11% 9/84 [00:05\u003c00:45,  1.65it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  12% 10/84 [00:06\u003c00:50,  1.46it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  13% 11/84 [00:07\u003c00:55,  1.31it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  14% 12/84 [00:07\u003c00:50,  1.42it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  15% 13/84 [00:08\u003c00:54,  1.30it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  17% 14/84 [00:09\u003c00:47,  1.46it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  18% 15/84 [00:10\u003c00:48,  1.44it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  19% 16/84 [00:10\u003c00:43,  1.55it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  20% 17/84 [00:11\u003c00:50,  1.33it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  21% 18/84 [00:12\u003c00:48,  1.36it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  23% 19/84 [00:13\u003c00:54,  1.19it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  24% 20/84 [00:13\u003c00:48,  1.31it/s]\u001b[A2024-10-24 07:59:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 07:59:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 07:59:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 029 | valid on 'valid' subset:  25% 21/84 [00:15\u003c00:58,  1.07it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  26% 22/84 [00:16\u003c01:02,  1.01s/it]\u001b[A\n","epoch 029 | valid on 'valid' subset:  27% 23/84 [00:17\u003c00:57,  1.06it/s]\u001b[A2024-10-24 07:59:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 07:59:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 07:59:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 029 | valid on 'valid' subset:  29% 24/84 [00:18\u003c00:55,  1.09it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  30% 25/84 [00:18\u003c00:46,  1.27it/s]\u001b[A2024-10-24 07:59:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 07:59:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 07:59:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 029:  16% 34/213 [00:23\u003c00:16, 10.65it/s, loss=5.405, nll_loss=4.247, ppl=18.99, wps=35842.4, ups=11.25, wpb=3186.3, bsz=97.9, num_updates=6000, lr=0.000122474, gnorm=1.068, train_wall=8, gb_free=14.1, wall=769]\n","epoch 029 | valid on 'valid' subset:  32% 27/84 [00:19\u003c00:39,  1.44it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  33% 28/84 [00:20\u003c00:36,  1.55it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  35% 29/84 [00:21\u003c00:40,  1.37it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  36% 30/84 [00:21\u003c00:36,  1.48it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  37% 31/84 [00:22\u003c00:36,  1.46it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  38% 32/84 [00:23\u003c00:35,  1.48it/s]\u001b[A2024-10-24 07:59:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 07:59:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 07:59:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 029 | valid on 'valid' subset:  39% 33/84 [00:24\u003c00:38,  1.31it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  40% 34/84 [00:24\u003c00:36,  1.39it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  42% 35/84 [00:25\u003c00:32,  1.51it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  43% 36/84 [00:26\u003c00:34,  1.41it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  44% 37/84 [00:26\u003c00:31,  1.51it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  45% 38/84 [00:27\u003c00:30,  1.52it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  46% 39/84 [00:27\u003c00:28,  1.58it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  48% 40/84 [00:28\u003c00:30,  1.42it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  49% 41/84 [00:29\u003c00:32,  1.32it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  50% 42/84 [00:30\u003c00:35,  1.18it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  51% 43/84 [00:31\u003c00:35,  1.16it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  52% 44/84 [00:32\u003c00:39,  1.01it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  54% 45/84 [00:33\u003c00:35,  1.08it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  55% 46/84 [00:34\u003c00:34,  1.10it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  56% 47/84 [00:35\u003c00:33,  1.11it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  57% 48/84 [00:36\u003c00:31,  1.14it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  58% 49/84 [00:36\u003c00:28,  1.24it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  60% 50/84 [00:37\u003c00:24,  1.40it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  61% 51/84 [00:38\u003c00:23,  1.43it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  62% 52/84 [00:38\u003c00:21,  1.46it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  63% 53/84 [00:39\u003c00:21,  1.45it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  64% 54/84 [00:39\u003c00:19,  1.52it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  65% 55/84 [00:40\u003c00:19,  1.48it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  67% 56/84 [00:41\u003c00:18,  1.53it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  68% 57/84 [00:42\u003c00:20,  1.32it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  69% 58/84 [00:42\u003c00:18,  1.42it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  70% 59/84 [00:43\u003c00:17,  1.44it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  71% 60/84 [00:44\u003c00:17,  1.40it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  73% 61/84 [00:45\u003c00:17,  1.34it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  74% 62/84 [00:45\u003c00:16,  1.36it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  75% 63/84 [00:47\u003c00:19,  1.09it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  76% 64/84 [00:48\u003c00:18,  1.08it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  77% 65/84 [00:49\u003c00:17,  1.07it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  79% 66/84 [00:49\u003c00:16,  1.09it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  80% 67/84 [00:50\u003c00:14,  1.20it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  81% 68/84 [00:51\u003c00:13,  1.22it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  82% 69/84 [00:52\u003c00:11,  1.28it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  83% 70/84 [00:53\u003c00:11,  1.20it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  85% 71/84 [00:53\u003c00:10,  1.27it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  86% 72/84 [00:54\u003c00:08,  1.37it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  87% 73/84 [00:55\u003c00:08,  1.31it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  88% 74/84 [00:56\u003c00:07,  1.26it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  89% 75/84 [00:56\u003c00:06,  1.36it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  90% 76/84 [00:57\u003c00:06,  1.30it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  92% 77/84 [00:58\u003c00:05,  1.29it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  93% 78/84 [00:59\u003c00:04,  1.25it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  94% 79/84 [01:00\u003c00:04,  1.12it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  95% 80/84 [01:01\u003c00:03,  1.01it/s]\u001b[A\n","epoch 029 | valid on 'valid' subset:  96% 81/84 [01:02\u003c00:03,  1.06s/it]\u001b[A\n","epoch 029 | valid on 'valid' subset:  98% 82/84 [01:04\u003c00:02,  1.17s/it]\u001b[A\n","epoch 029 | valid on 'valid' subset:  99% 83/84 [01:05\u003c00:01,  1.21s/it]\u001b[A\n","epoch 029 | valid on 'valid' subset: 100% 84/84 [01:06\u003c00:00,  1.18s/it]\u001b[A\n","                                                                        \u001b[A2024-10-24 08:00:32 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 5.379 | nll_loss 4.102 | ppl 17.17 | bleu 12.4 | wps 3072.3 | wpb 2417.9 | bsz 75.4 | num_updates 6000 | best_bleu 12.4\n","2024-10-24 08:00:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 6000 updates\n","2024-10-24 08:00:32 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_29_6000.pt\n","2024-10-24 08:00:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_29_6000.pt\n","2024-10-24 08:00:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpe/checkpoint_29_6000.pt (epoch 29 @ 6000 updates, score 12.4) (writing took 1.1540081410000766 seconds)\n","2024-10-24 08:00:50 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)\n","2024-10-24 08:00:50 | INFO | train | epoch 029 | loss 5.375 | nll_loss 4.212 | ppl 18.54 | wps 7847.2 | ups 2.41 | wpb 3256.8 | bsz 98.6 | num_updates 6177 | lr 0.000120707 | gnorm 1.058 | train_wall 19 | gb_free 14.1 | wall 853\n","2024-10-24 08:00:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:00:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 030:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:00:50 | INFO | fairseq.trainer | begin training epoch 30\n","2024-10-24 08:00:50 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:01:08 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)\n","2024-10-24 08:01:08 | INFO | train | epoch 030 | loss 5.346 | nll_loss 4.177 | ppl 18.09 | wps 37469.5 | ups 11.5 | wpb 3256.8 | bsz 98.6 | num_updates 6390 | lr 0.000118678 | gnorm 1.072 | train_wall 17 | gb_free 14 | wall 872\n","2024-10-24 08:01:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:01:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 031:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:01:08 | INFO | fairseq.trainer | begin training epoch 31\n","2024-10-24 08:01:08 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:01:26 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)\n","2024-10-24 08:01:26 | INFO | train | epoch 031 | loss 5.32 | nll_loss 4.147 | ppl 17.71 | wps 38186.5 | ups 11.73 | wpb 3256.8 | bsz 98.6 | num_updates 6603 | lr 0.000116748 | gnorm 1.087 | train_wall 17 | gb_free 14.1 | wall 890\n","2024-10-24 08:01:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:01:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 032:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:01:26 | INFO | fairseq.trainer | begin training epoch 32\n","2024-10-24 08:01:26 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:01:44 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)\n","2024-10-24 08:01:44 | INFO | train | epoch 032 | loss 5.292 | nll_loss 4.113 | ppl 17.31 | wps 38216.4 | ups 11.73 | wpb 3256.8 | bsz 98.6 | num_updates 6816 | lr 0.00011491 | gnorm 1.099 | train_wall 16 | gb_free 14.1 | wall 908\n","2024-10-24 08:01:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:01:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 033:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:01:45 | INFO | fairseq.trainer | begin training epoch 33\n","2024-10-24 08:01:45 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:02:03 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)\n","2024-10-24 08:02:03 | INFO | train | epoch 033 | loss 5.265 | nll_loss 4.082 | ppl 16.94 | wps 37803.9 | ups 11.61 | wpb 3256.8 | bsz 98.6 | num_updates 7029 | lr 0.000113155 | gnorm 1.087 | train_wall 17 | gb_free 14.1 | wall 927\n","2024-10-24 08:02:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:02:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 034:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:02:03 | INFO | fairseq.trainer | begin training epoch 34\n","2024-10-24 08:02:03 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:02:22 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)\n","2024-10-24 08:02:22 | INFO | train | epoch 034 | loss 5.242 | nll_loss 4.054 | ppl 16.61 | wps 35423.8 | ups 10.88 | wpb 3256.8 | bsz 98.6 | num_updates 7242 | lr 0.000111479 | gnorm 1.099 | train_wall 17 | gb_free 14.1 | wall 946\n","2024-10-24 08:02:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:02:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 035:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:02:23 | INFO | fairseq.trainer | begin training epoch 35\n","2024-10-24 08:02:23 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:02:41 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)\n","2024-10-24 08:02:41 | INFO | train | epoch 035 | loss 5.218 | nll_loss 4.026 | ppl 16.29 | wps 37257.4 | ups 11.44 | wpb 3256.8 | bsz 98.6 | num_updates 7455 | lr 0.000109875 | gnorm 1.117 | train_wall 17 | gb_free 14.1 | wall 965\n","2024-10-24 08:02:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:02:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 036:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:02:41 | INFO | fairseq.trainer | begin training epoch 36\n","2024-10-24 08:02:41 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:02:59 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)\n","2024-10-24 08:02:59 | INFO | train | epoch 036 | loss 5.191 | nll_loss 3.995 | ppl 15.94 | wps 37976.3 | ups 11.66 | wpb 3256.8 | bsz 98.6 | num_updates 7668 | lr 0.000108338 | gnorm 1.106 | train_wall 17 | gb_free 14.2 | wall 983\n","2024-10-24 08:02:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:02:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 037:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:02:59 | INFO | fairseq.trainer | begin training epoch 37\n","2024-10-24 08:02:59 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:03:17 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)\n","2024-10-24 08:03:17 | INFO | train | epoch 037 | loss 5.173 | nll_loss 3.973 | ppl 15.71 | wps 38391.7 | ups 11.79 | wpb 3256.8 | bsz 98.6 | num_updates 7881 | lr 0.000106864 | gnorm 1.113 | train_wall 16 | gb_free 14 | wall 1001\n","2024-10-24 08:03:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:03:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 038:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:03:18 | INFO | fairseq.trainer | begin training epoch 38\n","2024-10-24 08:03:18 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 038:  55% 118/213 [00:10\u003c00:09,  9.95it/s, loss=5.154, nll_loss=3.951, ppl=15.46, wps=38046.3, ups=11.8, wpb=3223.6, bsz=99.2, num_updates=7900, lr=0.000106735, gnorm=1.101, train_wall=8, gb_free=14.1, wall=1003]2024-10-24 08:03:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 08:03:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 038 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:   1% 1/84 [00:00\u003c01:07,  1.23it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:   2% 2/84 [00:01\u003c00:47,  1.74it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:   4% 3/84 [00:01\u003c00:38,  2.11it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:   5% 4/84 [00:02\u003c00:36,  2.17it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:   6% 5/84 [00:02\u003c00:34,  2.27it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:   7% 6/84 [00:02\u003c00:33,  2.35it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:   8% 7/84 [00:03\u003c00:32,  2.40it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  10% 8/84 [00:03\u003c00:31,  2.42it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  11% 9/84 [00:04\u003c00:41,  1.80it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  12% 10/84 [00:04\u003c00:37,  1.95it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  13% 11/84 [00:05\u003c00:38,  1.91it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  14% 12/84 [00:05\u003c00:35,  2.05it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  15% 13/84 [00:06\u003c00:33,  2.12it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  17% 14/84 [00:06\u003c00:30,  2.27it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  18% 15/84 [00:07\u003c00:31,  2.17it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  19% 16/84 [00:07\u003c00:30,  2.24it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  20% 17/84 [00:08\u003c00:39,  1.70it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  21% 18/84 [00:08\u003c00:34,  1.91it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  23% 19/84 [00:09\u003c00:34,  1.87it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  24% 20/84 [00:09\u003c00:30,  2.10it/s]\u001b[A2024-10-24 08:03:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:03:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:03:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 038 | valid on 'valid' subset:  25% 21/84 [00:10\u003c00:30,  2.05it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  26% 22/84 [00:10\u003c00:32,  1.90it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  27% 23/84 [00:11\u003c00:31,  1.92it/s]\u001b[A2024-10-24 08:03:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:03:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:03:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 038 | valid on 'valid' subset:  29% 24/84 [00:12\u003c00:36,  1.66it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  30% 25/84 [00:12\u003c00:36,  1.64it/s]\u001b[A2024-10-24 08:03:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:03:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:03:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 038 | valid on 'valid' subset:  31% 26/84 [00:13\u003c00:43,  1.34it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  32% 27/84 [00:14\u003c00:41,  1.38it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  33% 28/84 [00:15\u003c00:38,  1.44it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  35% 29/84 [00:16\u003c00:41,  1.34it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  36% 30/84 [00:16\u003c00:42,  1.27it/s]\u001b[A2024-10-24 08:03:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:03:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:03:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 038 | valid on 'valid' subset:  37% 31/84 [00:17\u003c00:41,  1.29it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  38% 32/84 [00:18\u003c00:34,  1.51it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  39% 33/84 [00:18\u003c00:33,  1.51it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  40% 34/84 [00:19\u003c00:32,  1.56it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  42% 35/84 [00:19\u003c00:28,  1.69it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  43% 36/84 [00:20\u003c00:29,  1.64it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  44% 37/84 [00:20\u003c00:27,  1.72it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  45% 38/84 [00:21\u003c00:26,  1.71it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  46% 39/84 [00:22\u003c00:24,  1.81it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  48% 40/84 [00:22\u003c00:24,  1.79it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  49% 41/84 [00:23\u003c00:22,  1.88it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  50% 42/84 [00:23\u003c00:23,  1.81it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  51% 43/84 [00:24\u003c00:22,  1.82it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  52% 44/84 [00:24\u003c00:22,  1.79it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  54% 45/84 [00:25\u003c00:20,  1.87it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  55% 46/84 [00:25\u003c00:21,  1.73it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  56% 47/84 [00:27\u003c00:27,  1.34it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  57% 48/84 [00:27\u003c00:25,  1.39it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  58% 49/84 [00:28\u003c00:26,  1.30it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  60% 50/84 [00:29\u003c00:25,  1.33it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  61% 51/84 [00:30\u003c00:25,  1.27it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  62% 52/84 [00:31\u003c00:25,  1.26it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  63% 53/84 [00:31\u003c00:25,  1.22it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  64% 54/84 [00:32\u003c00:23,  1.26it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  65% 55/84 [00:33\u003c00:22,  1.28it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  67% 56/84 [00:33\u003c00:19,  1.43it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  68% 57/84 [00:34\u003c00:18,  1.47it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  69% 58/84 [00:35\u003c00:16,  1.55it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  70% 59/84 [00:35\u003c00:15,  1.57it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  71% 60/84 [00:36\u003c00:14,  1.63it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  73% 61/84 [00:36\u003c00:14,  1.62it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  74% 62/84 [00:37\u003c00:13,  1.68it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  75% 63/84 [00:38\u003c00:12,  1.65it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  76% 64/84 [00:38\u003c00:12,  1.60it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  77% 65/84 [00:39\u003c00:11,  1.61it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  79% 66/84 [00:40\u003c00:12,  1.48it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  80% 67/84 [00:40\u003c00:11,  1.50it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  81% 68/84 [00:41\u003c00:10,  1.57it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  82% 69/84 [00:42\u003c00:09,  1.58it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  83% 70/84 [00:42\u003c00:08,  1.60it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  85% 71/84 [00:43\u003c00:08,  1.55it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  86% 72/84 [00:44\u003c00:08,  1.45it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  87% 73/84 [00:45\u003c00:08,  1.30it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  88% 74/84 [00:46\u003c00:09,  1.10it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  89% 75/84 [00:47\u003c00:08,  1.10it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  90% 76/84 [00:48\u003c00:08,  1.05s/it]\u001b[A\n","epoch 038 | valid on 'valid' subset:  92% 77/84 [00:49\u003c00:07,  1.01s/it]\u001b[A\n","epoch 038 | valid on 'valid' subset:  93% 78/84 [00:50\u003c00:05,  1.09it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  94% 79/84 [00:51\u003c00:04,  1.03it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  95% 80/84 [00:52\u003c00:03,  1.11it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  96% 81/84 [00:52\u003c00:02,  1.17it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  98% 82/84 [00:53\u003c00:01,  1.17it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset:  99% 83/84 [00:54\u003c00:00,  1.18it/s]\u001b[A\n","epoch 038 | valid on 'valid' subset: 100% 84/84 [00:55\u003c00:00,  1.20it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 08:04:23 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 5.243 | nll_loss 3.922 | ppl 15.16 | bleu 13.33 | wps 3695.5 | wpb 2417.9 | bsz 75.4 | num_updates 8000 | best_bleu 13.33\n","2024-10-24 08:04:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 8000 updates\n","2024-10-24 08:04:23 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_38_8000.pt\n","2024-10-24 08:04:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_38_8000.pt\n","2024-10-24 08:04:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpe/checkpoint_38_8000.pt (epoch 38 @ 8000 updates, score 13.33) (writing took 1.213356752999971 seconds)\n","2024-10-24 08:04:35 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)\n","2024-10-24 08:04:35 | INFO | train | epoch 038 | loss 5.148 | nll_loss 3.944 | ppl 15.39 | wps 8911.1 | ups 2.74 | wpb 3256.8 | bsz 98.6 | num_updates 8094 | lr 0.000105448 | gnorm 1.12 | train_wall 19 | gb_free 14.1 | wall 1079\n","2024-10-24 08:04:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:04:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 039:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:04:35 | INFO | fairseq.trainer | begin training epoch 39\n","2024-10-24 08:04:35 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:04:54 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)\n","2024-10-24 08:04:54 | INFO | train | epoch 039 | loss 5.13 | nll_loss 3.922 | ppl 15.16 | wps 37192.3 | ups 11.42 | wpb 3256.8 | bsz 98.6 | num_updates 8307 | lr 0.000104088 | gnorm 1.119 | train_wall 17 | gb_free 14.1 | wall 1098\n","2024-10-24 08:04:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:04:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 040:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:04:54 | INFO | fairseq.trainer | begin training epoch 40\n","2024-10-24 08:04:54 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:05:12 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)\n","2024-10-24 08:05:12 | INFO | train | epoch 040 | loss 5.105 | nll_loss 3.893 | ppl 14.86 | wps 38816.4 | ups 11.92 | wpb 3256.8 | bsz 98.6 | num_updates 8520 | lr 0.000102778 | gnorm 1.127 | train_wall 16 | gb_free 14.1 | wall 1115\n","2024-10-24 08:05:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:05:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 041:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:05:12 | INFO | fairseq.trainer | begin training epoch 41\n","2024-10-24 08:05:12 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:05:30 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)\n","2024-10-24 08:05:30 | INFO | train | epoch 041 | loss 5.084 | nll_loss 3.868 | ppl 14.6 | wps 38260.4 | ups 11.75 | wpb 3256.8 | bsz 98.6 | num_updates 8733 | lr 0.000101517 | gnorm 1.13 | train_wall 16 | gb_free 14.1 | wall 1134\n","2024-10-24 08:05:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:05:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 042:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:05:30 | INFO | fairseq.trainer | begin training epoch 42\n","2024-10-24 08:05:30 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:05:48 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)\n","2024-10-24 08:05:48 | INFO | train | epoch 042 | loss 5.068 | nll_loss 3.848 | ppl 14.4 | wps 38660.6 | ups 11.87 | wpb 3256.8 | bsz 98.6 | num_updates 8946 | lr 0.000100301 | gnorm 1.139 | train_wall 16 | gb_free 14.1 | wall 1152\n","2024-10-24 08:05:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:05:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 043:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:05:48 | INFO | fairseq.trainer | begin training epoch 43\n","2024-10-24 08:05:48 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:06:06 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)\n","2024-10-24 08:06:06 | INFO | train | epoch 043 | loss 5.047 | nll_loss 3.824 | ppl 14.16 | wps 37491.8 | ups 11.51 | wpb 3256.8 | bsz 98.6 | num_updates 9159 | lr 9.91282e-05 | gnorm 1.141 | train_wall 17 | gb_free 14.1 | wall 1170\n","2024-10-24 08:06:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:06:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 044:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:06:07 | INFO | fairseq.trainer | begin training epoch 44\n","2024-10-24 08:06:07 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:06:25 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)\n","2024-10-24 08:06:25 | INFO | train | epoch 044 | loss 5.029 | nll_loss 3.803 | ppl 13.96 | wps 37402.7 | ups 11.48 | wpb 3256.8 | bsz 98.6 | num_updates 9372 | lr 9.79953e-05 | gnorm 1.133 | train_wall 17 | gb_free 14.1 | wall 1189\n","2024-10-24 08:06:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:06:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 045:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:06:25 | INFO | fairseq.trainer | begin training epoch 45\n","2024-10-24 08:06:25 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:06:43 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)\n","2024-10-24 08:06:43 | INFO | train | epoch 045 | loss 5.009 | nll_loss 3.779 | ppl 13.73 | wps 37621.4 | ups 11.55 | wpb 3256.8 | bsz 98.6 | num_updates 9585 | lr 9.69003e-05 | gnorm 1.149 | train_wall 17 | gb_free 14 | wall 1207\n","2024-10-24 08:06:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:06:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 046:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:06:44 | INFO | fairseq.trainer | begin training epoch 46\n","2024-10-24 08:06:44 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:07:02 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)\n","2024-10-24 08:07:02 | INFO | train | epoch 046 | loss 5.006 | nll_loss 3.776 | ppl 13.7 | wps 37897.6 | ups 11.64 | wpb 3256.8 | bsz 98.6 | num_updates 9798 | lr 9.58413e-05 | gnorm 1.192 | train_wall 17 | gb_free 14 | wall 1225\n","2024-10-24 08:07:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:07:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 047:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:07:02 | INFO | fairseq.trainer | begin training epoch 47\n","2024-10-24 08:07:02 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 047:  94% 201/213 [00:17\u003c00:00, 12.69it/s, loss=4.976, nll_loss=3.741, ppl=13.37, wps=37962.5, ups=11.83, wpb=3207.7, bsz=96.4, num_updates=9900, lr=9.53463e-05, gnorm=1.15, train_wall=8, gb_free=14.1, wall=1234]2024-10-24 08:07:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 08:07:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 047 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:   1% 1/84 [00:00\u003c00:56,  1.46it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:   2% 2/84 [00:01\u003c00:44,  1.84it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:   4% 3/84 [00:01\u003c00:36,  2.22it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:   5% 4/84 [00:01\u003c00:38,  2.09it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:   6% 5/84 [00:02\u003c00:36,  2.19it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:   7% 6/84 [00:02\u003c00:34,  2.29it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:   8% 7/84 [00:03\u003c00:32,  2.34it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  10% 8/84 [00:03\u003c00:32,  2.32it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  11% 9/84 [00:04\u003c00:38,  1.93it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  12% 10/84 [00:04\u003c00:38,  1.94it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  13% 11/84 [00:05\u003c00:47,  1.52it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  14% 12/84 [00:06\u003c00:44,  1.61it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  15% 13/84 [00:07\u003c00:48,  1.47it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  17% 14/84 [00:07\u003c00:44,  1.57it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  18% 15/84 [00:08\u003c00:49,  1.41it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  19% 16/84 [00:09\u003c00:46,  1.46it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  20% 17/84 [00:10\u003c00:53,  1.24it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  21% 18/84 [00:10\u003c00:45,  1.45it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  23% 19/84 [00:11\u003c00:47,  1.37it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  24% 20/84 [00:12\u003c00:40,  1.57it/s]\u001b[A2024-10-24 08:07:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:07:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:07:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 047 | valid on 'valid' subset:  25% 21/84 [00:12\u003c00:39,  1.60it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  26% 22/84 [00:13\u003c00:48,  1.28it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  27% 23/84 [00:14\u003c00:44,  1.38it/s]\u001b[A2024-10-24 08:07:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:07:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:07:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 047:  94% 201/213 [00:32\u003c00:00, 12.69it/s, loss=4.976, nll_loss=3.739, ppl=13.35, wps=39025.4, ups=11.95, wpb=3266.4, bsz=100, num_updates=10000, lr=9.48683e-05, gnorm=1.158, train_wall=8, gb_free=14.1, wall=1243]\n","epoch 047 | valid on 'valid' subset:  30% 25/84 [00:15\u003c00:44,  1.32it/s]\u001b[A2024-10-24 08:07:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:07:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:07:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 047 | valid on 'valid' subset:  31% 26/84 [00:17\u003c00:51,  1.12it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  32% 27/84 [00:17\u003c00:48,  1.18it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  33% 28/84 [00:18\u003c00:44,  1.27it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  35% 29/84 [00:19\u003c00:45,  1.22it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  36% 30/84 [00:20\u003c00:40,  1.33it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  37% 31/84 [00:20\u003c00:41,  1.28it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  38% 32/84 [00:21\u003c00:42,  1.22it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  39% 33/84 [00:22\u003c00:43,  1.18it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  40% 34/84 [00:23\u003c00:46,  1.08it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  42% 35/84 [00:24\u003c00:42,  1.16it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  43% 36/84 [00:25\u003c00:48,  1.01s/it]\u001b[A\n","epoch 047 | valid on 'valid' subset:  44% 37/84 [00:26\u003c00:41,  1.12it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  45% 38/84 [00:27\u003c00:37,  1.22it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  46% 39/84 [00:27\u003c00:32,  1.40it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  48% 40/84 [00:28\u003c00:30,  1.45it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  49% 41/84 [00:28\u003c00:28,  1.52it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  50% 42/84 [00:29\u003c00:27,  1.52it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  51% 43/84 [00:30\u003c00:26,  1.57it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  52% 44/84 [00:30\u003c00:24,  1.61it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  54% 45/84 [00:31\u003c00:23,  1.65it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  55% 46/84 [00:31\u003c00:23,  1.62it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  56% 47/84 [00:32\u003c00:25,  1.43it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  57% 48/84 [00:33\u003c00:23,  1.54it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  58% 49/84 [00:34\u003c00:25,  1.36it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  60% 50/84 [00:34\u003c00:23,  1.45it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  61% 51/84 [00:35\u003c00:22,  1.45it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  62% 52/84 [00:36\u003c00:21,  1.52it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  63% 53/84 [00:37\u003c00:22,  1.36it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  64% 54/84 [00:37\u003c00:22,  1.35it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  65% 55/84 [00:38\u003c00:22,  1.28it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  67% 56/84 [00:39\u003c00:22,  1.24it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  68% 57/84 [00:40\u003c00:23,  1.17it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  69% 58/84 [00:41\u003c00:21,  1.18it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  70% 59/84 [00:42\u003c00:21,  1.17it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  71% 60/84 [00:42\u003c00:18,  1.29it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  73% 61/84 [00:43\u003c00:17,  1.34it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  74% 62/84 [00:43\u003c00:15,  1.45it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  75% 63/84 [00:44\u003c00:14,  1.50it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  76% 64/84 [00:45\u003c00:13,  1.53it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  77% 65/84 [00:45\u003c00:12,  1.53it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  79% 66/84 [00:46\u003c00:13,  1.36it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  80% 67/84 [00:47\u003c00:11,  1.44it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  81% 68/84 [00:48\u003c00:11,  1.41it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  82% 69/84 [00:48\u003c00:10,  1.44it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  83% 70/84 [00:49\u003c00:09,  1.42it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  85% 71/84 [00:50\u003c00:08,  1.49it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  86% 72/84 [00:50\u003c00:07,  1.53it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  87% 73/84 [00:51\u003c00:07,  1.51it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  88% 74/84 [00:52\u003c00:07,  1.36it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  89% 75/84 [00:53\u003c00:06,  1.32it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  90% 76/84 [00:54\u003c00:06,  1.22it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  92% 77/84 [00:55\u003c00:06,  1.06it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  93% 78/84 [00:56\u003c00:05,  1.03it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  94% 79/84 [00:57\u003c00:05,  1.01s/it]\u001b[A\n","epoch 047 | valid on 'valid' subset:  95% 80/84 [00:58\u003c00:04,  1.06s/it]\u001b[A\n","epoch 047 | valid on 'valid' subset:  96% 81/84 [00:59\u003c00:02,  1.05it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  98% 82/84 [01:00\u003c00:01,  1.06it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset:  99% 83/84 [01:01\u003c00:00,  1.08it/s]\u001b[A\n","epoch 047 | valid on 'valid' subset: 100% 84/84 [01:01\u003c00:00,  1.12it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 08:08:21 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 5.112 | nll_loss 3.757 | ppl 13.52 | bleu 15.24 | wps 3285.4 | wpb 2417.9 | bsz 75.4 | num_updates 10000 | best_bleu 15.24\n","2024-10-24 08:08:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 10000 updates\n","2024-10-24 08:08:21 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_47_10000.pt\n","2024-10-24 08:08:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_47_10000.pt\n","2024-10-24 08:08:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpe/checkpoint_47_10000.pt (epoch 47 @ 10000 updates, score 15.24) (writing took 1.2344188540000687 seconds)\n","2024-10-24 08:08:23 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)\n","2024-10-24 08:08:23 | INFO | train | epoch 047 | loss 4.979 | nll_loss 3.744 | ppl 13.4 | wps 8516.8 | ups 2.62 | wpb 3256.8 | bsz 98.6 | num_updates 10011 | lr 9.48162e-05 | gnorm 1.152 | train_wall 16 | gb_free 14 | wall 1307\n","2024-10-24 08:08:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:08:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 048:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:08:23 | INFO | fairseq.trainer | begin training epoch 48\n","2024-10-24 08:08:23 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:08:44 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)\n","2024-10-24 08:08:44 | INFO | train | epoch 048 | loss 4.958 | nll_loss 3.72 | ppl 13.17 | wps 33924 | ups 10.42 | wpb 3256.8 | bsz 98.6 | num_updates 10224 | lr 9.38233e-05 | gnorm 1.138 | train_wall 19 | gb_free 14.1 | wall 1327\n","2024-10-24 08:08:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:08:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 049:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:08:44 | INFO | fairseq.trainer | begin training epoch 49\n","2024-10-24 08:08:44 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:09:02 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)\n","2024-10-24 08:09:02 | INFO | train | epoch 049 | loss 4.942 | nll_loss 3.701 | ppl 13 | wps 38006.6 | ups 11.67 | wpb 3256.8 | bsz 98.6 | num_updates 10437 | lr 9.2861e-05 | gnorm 1.152 | train_wall 16 | gb_free 14.1 | wall 1346\n","2024-10-24 08:09:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:09:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 050:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:09:02 | INFO | fairseq.trainer | begin training epoch 50\n","2024-10-24 08:09:02 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 050:  99% 211/213 [00:18\u003c00:00, 11.27it/s, loss=4.947, nll_loss=3.706, ppl=13.05, wps=41024, ups=12.7, wpb=3231.3, bsz=93.8, num_updates=10600, lr=9.21443e-05, gnorm=1.157, train_wall=7, gb_free=14.1, wall=1359]2024-10-24 08:09:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 08:09:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 050 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:   1% 1/84 [00:00\u003c01:16,  1.08it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:   2% 2/84 [00:01\u003c00:52,  1.57it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:   4% 3/84 [00:01\u003c00:43,  1.86it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:   5% 4/84 [00:02\u003c00:38,  2.09it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:   6% 5/84 [00:02\u003c00:36,  2.17it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:   7% 6/84 [00:02\u003c00:33,  2.30it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:   8% 7/84 [00:03\u003c00:33,  2.31it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  10% 8/84 [00:03\u003c00:33,  2.30it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  11% 9/84 [00:04\u003c00:33,  2.23it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  12% 10/84 [00:04\u003c00:31,  2.35it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  13% 11/84 [00:05\u003c00:37,  1.93it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  14% 12/84 [00:05\u003c00:35,  2.05it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  15% 13/84 [00:06\u003c00:35,  1.99it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  17% 14/84 [00:06\u003c00:32,  2.13it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  18% 15/84 [00:07\u003c00:35,  1.96it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  19% 16/84 [00:07\u003c00:33,  2.02it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  20% 17/84 [00:08\u003c00:34,  1.92it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  21% 18/84 [00:08\u003c00:32,  2.06it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  23% 19/84 [00:09\u003c00:32,  1.98it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  24% 20/84 [00:09\u003c00:30,  2.12it/s]\u001b[A2024-10-24 08:09:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:09:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:09:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 050 | valid on 'valid' subset:  25% 21/84 [00:10\u003c00:31,  2.00it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  26% 22/84 [00:11\u003c00:40,  1.54it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  27% 23/84 [00:11\u003c00:39,  1.54it/s]\u001b[A2024-10-24 08:09:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:09:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:09:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 050 | valid on 'valid' subset:  29% 24/84 [00:12\u003c00:42,  1.42it/s]\u001b[A\n","epoch 050:  99% 211/213 [00:32\u003c00:00, 11.27it/s, loss=4.947, nll_loss=3.706, ppl=13.05, wps=41024, ups=12.7, wpb=3231.3, bsz=93.8, num_updates=10600, lr=9.21443e-05, gnorm=1.157, train_wall=7, gb_free=14.1, wall=1359]2024-10-24 08:09:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:09:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:09:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 050 | valid on 'valid' subset:  31% 26/84 [00:14\u003c00:42,  1.37it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  32% 27/84 [00:15\u003c00:42,  1.33it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  33% 28/84 [00:15\u003c00:40,  1.39it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  35% 29/84 [00:16\u003c00:41,  1.34it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  36% 30/84 [00:17\u003c00:35,  1.52it/s]\u001b[A2024-10-24 08:09:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:09:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:09:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 050 | valid on 'valid' subset:  37% 31/84 [00:17\u003c00:34,  1.52it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  38% 32/84 [00:18\u003c00:30,  1.69it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  39% 33/84 [00:18\u003c00:30,  1.69it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  40% 34/84 [00:19\u003c00:29,  1.71it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  42% 35/84 [00:19\u003c00:27,  1.80it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  43% 36/84 [00:20\u003c00:27,  1.76it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  44% 37/84 [00:20\u003c00:26,  1.76it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  45% 38/84 [00:21\u003c00:26,  1.76it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  46% 39/84 [00:21\u003c00:24,  1.82it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  48% 40/84 [00:22\u003c00:24,  1.78it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  49% 41/84 [00:23\u003c00:23,  1.82it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  50% 42/84 [00:23\u003c00:23,  1.76it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  51% 43/84 [00:24\u003c00:24,  1.68it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  52% 44/84 [00:24\u003c00:23,  1.68it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  54% 45/84 [00:25\u003c00:22,  1.76it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  55% 46/84 [00:26\u003c00:21,  1.73it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  56% 47/84 [00:27\u003c00:26,  1.41it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  57% 48/84 [00:27\u003c00:25,  1.40it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  58% 49/84 [00:28\u003c00:25,  1.38it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  60% 50/84 [00:29\u003c00:24,  1.41it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  61% 51/84 [00:30\u003c00:24,  1.33it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  62% 52/84 [00:30\u003c00:25,  1.28it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  63% 53/84 [00:31\u003c00:25,  1.23it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  64% 54/84 [00:32\u003c00:24,  1.23it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  65% 55/84 [00:33\u003c00:22,  1.29it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  67% 56/84 [00:33\u003c00:19,  1.43it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  68% 57/84 [00:34\u003c00:18,  1.48it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  69% 58/84 [00:35\u003c00:16,  1.57it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  70% 59/84 [00:35\u003c00:16,  1.54it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  71% 60/84 [00:36\u003c00:14,  1.65it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  73% 61/84 [00:36\u003c00:13,  1.70it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  74% 62/84 [00:37\u003c00:12,  1.72it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  75% 63/84 [00:38\u003c00:13,  1.60it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  76% 64/84 [00:38\u003c00:11,  1.69it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  77% 65/84 [00:39\u003c00:11,  1.68it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  79% 66/84 [00:39\u003c00:11,  1.63it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  80% 67/84 [00:40\u003c00:10,  1.68it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  81% 68/84 [00:41\u003c00:09,  1.63it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  82% 69/84 [00:41\u003c00:08,  1.67it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  83% 70/84 [00:42\u003c00:08,  1.67it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  85% 71/84 [00:42\u003c00:07,  1.65it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  86% 72/84 [00:43\u003c00:08,  1.48it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  87% 73/84 [00:44\u003c00:08,  1.33it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  88% 74/84 [00:45\u003c00:07,  1.25it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  89% 75/84 [00:46\u003c00:07,  1.19it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  90% 76/84 [00:47\u003c00:06,  1.16it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  92% 77/84 [00:48\u003c00:07,  1.00s/it]\u001b[A\n","epoch 050 | valid on 'valid' subset:  93% 78/84 [00:49\u003c00:05,  1.03it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  94% 79/84 [00:50\u003c00:04,  1.11it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  95% 80/84 [00:51\u003c00:03,  1.18it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  96% 81/84 [00:51\u003c00:02,  1.26it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  98% 82/84 [00:52\u003c00:01,  1.24it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  99% 83/84 [00:53\u003c00:00,  1.21it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset: 100% 84/84 [00:54\u003c00:00,  1.21it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 08:10:15 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 5.082 | nll_loss 3.724 | ppl 13.21 | bleu 15.29 | wps 3778 | wpb 2417.9 | bsz 75.4 | num_updates 10650 | best_bleu 15.29\n","2024-10-24 08:10:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 10650 updates\n","2024-10-24 08:10:15 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_best.pt\n","2024-10-24 08:10:15 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_best.pt\n","2024-10-24 08:10:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpe/checkpoint_best.pt (epoch 50 @ 10650 updates, score 15.29) (writing took 0.8236943769998106 seconds)\n","2024-10-24 08:10:15 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)\n","2024-10-24 08:10:15 | INFO | train | epoch 050 | loss 4.931 | nll_loss 3.687 | ppl 12.88 | wps 9438.6 | ups 2.9 | wpb 3256.8 | bsz 98.6 | num_updates 10650 | lr 9.19277e-05 | gnorm 1.156 | train_wall 17 | gb_free 14 | wall 1419\n","2024-10-24 08:10:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:10:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 051:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:10:16 | INFO | fairseq.trainer | begin training epoch 51\n","2024-10-24 08:10:16 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:10:35 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)\n","2024-10-24 08:10:35 | INFO | train | epoch 051 | loss 4.916 | nll_loss 3.669 | ppl 12.72 | wps 35429.1 | ups 10.88 | wpb 3256.8 | bsz 98.6 | num_updates 10863 | lr 9.1022e-05 | gnorm 1.175 | train_wall 18 | gb_free 14.2 | wall 1439\n","2024-10-24 08:10:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:10:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 052:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:10:35 | INFO | fairseq.trainer | begin training epoch 52\n","2024-10-24 08:10:35 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:10:53 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)\n","2024-10-24 08:10:54 | INFO | train | epoch 052 | loss 4.905 | nll_loss 3.656 | ppl 12.61 | wps 37389.2 | ups 11.48 | wpb 3256.8 | bsz 98.6 | num_updates 11076 | lr 9.01425e-05 | gnorm 1.195 | train_wall 17 | gb_free 14.2 | wall 1457\n","2024-10-24 08:10:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:10:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 053:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:10:54 | INFO | fairseq.trainer | begin training epoch 53\n","2024-10-24 08:10:54 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:11:12 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)\n","2024-10-24 08:11:12 | INFO | train | epoch 053 | loss 4.887 | nll_loss 3.635 | ppl 12.43 | wps 38113.1 | ups 11.7 | wpb 3256.8 | bsz 98.6 | num_updates 11289 | lr 8.92881e-05 | gnorm 1.173 | train_wall 16 | gb_free 14.1 | wall 1475\n","2024-10-24 08:11:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:11:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 054:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:11:12 | INFO | fairseq.trainer | begin training epoch 54\n","2024-10-24 08:11:12 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:11:30 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)\n","2024-10-24 08:11:30 | INFO | train | epoch 054 | loss 4.87 | nll_loss 3.615 | ppl 12.25 | wps 38862.7 | ups 11.93 | wpb 3256.8 | bsz 98.6 | num_updates 11502 | lr 8.84575e-05 | gnorm 1.167 | train_wall 16 | gb_free 14.1 | wall 1493\n","2024-10-24 08:11:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:11:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 055:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:11:30 | INFO | fairseq.trainer | begin training epoch 55\n","2024-10-24 08:11:30 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:11:47 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)\n","2024-10-24 08:11:47 | INFO | train | epoch 055 | loss 4.857 | nll_loss 3.6 | ppl 12.13 | wps 38894.1 | ups 11.94 | wpb 3256.8 | bsz 98.6 | num_updates 11715 | lr 8.76496e-05 | gnorm 1.191 | train_wall 16 | gb_free 14 | wall 1511\n","2024-10-24 08:11:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:11:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 056:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:11:48 | INFO | fairseq.trainer | begin training epoch 56\n","2024-10-24 08:11:48 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:12:05 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)\n","2024-10-24 08:12:05 | INFO | train | epoch 056 | loss 4.845 | nll_loss 3.585 | ppl 12 | wps 38754 | ups 11.9 | wpb 3256.8 | bsz 98.6 | num_updates 11928 | lr 8.68635e-05 | gnorm 1.181 | train_wall 16 | gb_free 14.1 | wall 1529\n","2024-10-24 08:12:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:12:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 057:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:12:05 | INFO | fairseq.trainer | begin training epoch 57\n","2024-10-24 08:12:05 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 057:  33% 70/213 [00:06\u003c00:14, 10.01it/s]2024-10-24 08:12:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 08:12:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 057 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:   1% 1/84 [00:00\u003c01:03,  1.30it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:   2% 2/84 [00:01\u003c00:45,  1.79it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:   4% 3/84 [00:01\u003c00:37,  2.16it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:   5% 4/84 [00:01\u003c00:35,  2.27it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:   6% 5/84 [00:02\u003c00:33,  2.36it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:   7% 6/84 [00:02\u003c00:31,  2.49it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:   8% 7/84 [00:03\u003c00:31,  2.46it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  10% 8/84 [00:03\u003c00:30,  2.48it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  11% 9/84 [00:03\u003c00:30,  2.42it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  12% 10/84 [00:04\u003c00:30,  2.41it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  13% 11/84 [00:05\u003c00:36,  2.02it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  14% 12/84 [00:05\u003c00:33,  2.15it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  15% 13/84 [00:05\u003c00:33,  2.13it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  17% 14/84 [00:06\u003c00:30,  2.28it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  18% 15/84 [00:06\u003c00:31,  2.16it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  19% 16/84 [00:07\u003c00:30,  2.20it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  20% 17/84 [00:07\u003c00:34,  1.97it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  21% 18/84 [00:08\u003c00:32,  2.05it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  23% 19/84 [00:09\u003c00:36,  1.79it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  24% 20/84 [00:09\u003c00:31,  2.02it/s]\u001b[A2024-10-24 08:12:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:12:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:12:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 057 | valid on 'valid' subset:  25% 21/84 [00:09\u003c00:32,  1.93it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  26% 22/84 [00:10\u003c00:42,  1.47it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  27% 23/84 [00:11\u003c00:38,  1.57it/s]\u001b[A2024-10-24 08:12:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:12:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:12:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 057:  33% 70/213 [00:19\u003c00:14, 10.01it/s, loss=4.795, nll_loss=3.528, ppl=11.54, wps=36210.9, ups=11.23, wpb=3223.1, bsz=104.4, num_updates=12000, lr=8.66025e-05, gnorm=1.171, train_wall=8, gb_free=14.1, wall=1536]\n","epoch 057 | valid on 'valid' subset:  30% 25/84 [00:13\u003c00:40,  1.45it/s]\u001b[A2024-10-24 08:12:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:12:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:12:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 057 | valid on 'valid' subset:  31% 26/84 [00:14\u003c00:49,  1.18it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  32% 27/84 [00:15\u003c00:48,  1.17it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  33% 28/84 [00:15\u003c00:43,  1.27it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  35% 29/84 [00:16\u003c00:42,  1.30it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  36% 30/84 [00:16\u003c00:36,  1.47it/s]\u001b[A2024-10-24 08:12:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:12:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:12:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 057 | valid on 'valid' subset:  37% 31/84 [00:17\u003c00:34,  1.55it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  38% 32/84 [00:17\u003c00:30,  1.73it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  39% 33/84 [00:18\u003c00:35,  1.44it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  40% 34/84 [00:19\u003c00:32,  1.53it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  42% 35/84 [00:19\u003c00:29,  1.63it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  43% 36/84 [00:20\u003c00:34,  1.41it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  44% 37/84 [00:21\u003c00:31,  1.49it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  45% 38/84 [00:22\u003c00:31,  1.47it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  46% 39/84 [00:22\u003c00:27,  1.63it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  48% 40/84 [00:23\u003c00:27,  1.62it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  49% 41/84 [00:23\u003c00:25,  1.70it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  50% 42/84 [00:24\u003c00:24,  1.74it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  51% 43/84 [00:24\u003c00:22,  1.78it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  52% 44/84 [00:25\u003c00:22,  1.79it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  54% 45/84 [00:25\u003c00:21,  1.85it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  55% 46/84 [00:26\u003c00:23,  1.58it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  56% 47/84 [00:28\u003c00:30,  1.23it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  57% 48/84 [00:28\u003c00:27,  1.29it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  58% 49/84 [00:29\u003c00:29,  1.19it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  60% 50/84 [00:30\u003c00:27,  1.23it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  61% 51/84 [00:31\u003c00:27,  1.21it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  62% 52/84 [00:32\u003c00:26,  1.23it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  63% 53/84 [00:32\u003c00:23,  1.30it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  64% 54/84 [00:33\u003c00:21,  1.42it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  65% 55/84 [00:33\u003c00:19,  1.48it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  67% 56/84 [00:34\u003c00:17,  1.58it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  68% 57/84 [00:35\u003c00:17,  1.56it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  69% 58/84 [00:35\u003c00:15,  1.66it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  70% 59/84 [00:36\u003c00:15,  1.58it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  71% 60/84 [00:36\u003c00:14,  1.68it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  73% 61/84 [00:37\u003c00:13,  1.70it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  74% 62/84 [00:37\u003c00:12,  1.78it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  75% 63/84 [00:38\u003c00:12,  1.66it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  76% 64/84 [00:39\u003c00:11,  1.67it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  77% 65/84 [00:39\u003c00:11,  1.67it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  79% 66/84 [00:40\u003c00:12,  1.50it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  80% 67/84 [00:41\u003c00:13,  1.25it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  81% 68/84 [00:42\u003c00:13,  1.23it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  82% 69/84 [00:43\u003c00:12,  1.18it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  83% 70/84 [00:44\u003c00:12,  1.13it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  85% 71/84 [00:45\u003c00:11,  1.09it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  86% 72/84 [00:46\u003c00:11,  1.03it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  87% 73/84 [00:47\u003c00:10,  1.07it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  88% 74/84 [00:48\u003c00:10,  1.03s/it]\u001b[A\n","epoch 057 | valid on 'valid' subset:  89% 75/84 [00:49\u003c00:09,  1.01s/it]\u001b[A\n","epoch 057 | valid on 'valid' subset:  90% 76/84 [00:50\u003c00:07,  1.03it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  92% 77/84 [00:51\u003c00:06,  1.03it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  93% 78/84 [00:52\u003c00:05,  1.13it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  94% 79/84 [00:52\u003c00:04,  1.17it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  95% 80/84 [00:53\u003c00:03,  1.15it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  96% 81/84 [00:54\u003c00:02,  1.24it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  98% 82/84 [00:55\u003c00:01,  1.24it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset:  99% 83/84 [00:56\u003c00:00,  1.20it/s]\u001b[A\n","epoch 057 | valid on 'valid' subset: 100% 84/84 [00:56\u003c00:00,  1.24it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 08:13:09 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 5.052 | nll_loss 3.677 | ppl 12.79 | bleu 16.03 | wps 3582.6 | wpb 2417.9 | bsz 75.4 | num_updates 12000 | best_bleu 16.03\n","2024-10-24 08:13:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 12000 updates\n","2024-10-24 08:13:09 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_57_12000.pt\n","2024-10-24 08:13:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_57_12000.pt\n","2024-10-24 08:13:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpe/checkpoint_57_12000.pt (epoch 57 @ 12000 updates, score 16.03) (writing took 1.2436938060000102 seconds)\n","2024-10-24 08:13:24 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)\n","2024-10-24 08:13:24 | INFO | train | epoch 057 | loss 4.83 | nll_loss 3.567 | ppl 11.85 | wps 8789.2 | ups 2.7 | wpb 3256.8 | bsz 98.6 | num_updates 12141 | lr 8.60982e-05 | gnorm 1.194 | train_wall 19 | gb_free 14.2 | wall 1608\n","2024-10-24 08:13:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:13:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 058:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:13:24 | INFO | fairseq.trainer | begin training epoch 58\n","2024-10-24 08:13:24 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:13:42 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)\n","2024-10-24 08:13:42 | INFO | train | epoch 058 | loss 4.818 | nll_loss 3.553 | ppl 11.74 | wps 38266.4 | ups 11.75 | wpb 3256.8 | bsz 98.6 | num_updates 12354 | lr 8.53527e-05 | gnorm 1.198 | train_wall 17 | gb_free 14 | wall 1626\n","2024-10-24 08:13:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:13:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 059:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:13:43 | INFO | fairseq.trainer | begin training epoch 59\n","2024-10-24 08:13:43 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:14:00 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)\n","2024-10-24 08:14:00 | INFO | train | epoch 059 | loss 4.805 | nll_loss 3.538 | ppl 11.62 | wps 38397.7 | ups 11.79 | wpb 3256.8 | bsz 98.6 | num_updates 12567 | lr 8.46263e-05 | gnorm 1.192 | train_wall 16 | gb_free 14.1 | wall 1644\n","2024-10-24 08:14:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:14:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 060:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:14:01 | INFO | fairseq.trainer | begin training epoch 60\n","2024-10-24 08:14:01 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:14:19 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)\n","2024-10-24 08:14:19 | INFO | train | epoch 060 | loss 4.791 | nll_loss 3.522 | ppl 11.49 | wps 38013.8 | ups 11.67 | wpb 3256.8 | bsz 98.6 | num_updates 12780 | lr 8.39181e-05 | gnorm 1.188 | train_wall 17 | gb_free 14.1 | wall 1662\n","2024-10-24 08:14:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:14:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 061:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:14:19 | INFO | fairseq.trainer | begin training epoch 61\n","2024-10-24 08:14:19 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:14:37 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)\n","2024-10-24 08:14:37 | INFO | train | epoch 061 | loss 4.778 | nll_loss 3.506 | ppl 11.36 | wps 37704.2 | ups 11.58 | wpb 3256.8 | bsz 98.6 | num_updates 12993 | lr 8.32274e-05 | gnorm 1.194 | train_wall 16 | gb_free 14.1 | wall 1681\n","2024-10-24 08:14:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:14:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 062:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:14:37 | INFO | fairseq.trainer | begin training epoch 62\n","2024-10-24 08:14:37 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:14:55 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)\n","2024-10-24 08:14:55 | INFO | train | epoch 062 | loss 4.769 | nll_loss 3.495 | ppl 11.27 | wps 38493 | ups 11.82 | wpb 3256.8 | bsz 98.6 | num_updates 13206 | lr 8.25535e-05 | gnorm 1.217 | train_wall 16 | gb_free 14 | wall 1699\n","2024-10-24 08:14:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:14:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 063:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:14:55 | INFO | fairseq.trainer | begin training epoch 63\n","2024-10-24 08:14:55 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:15:13 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)\n","2024-10-24 08:15:13 | INFO | train | epoch 063 | loss 4.756 | nll_loss 3.48 | ppl 11.16 | wps 38301.4 | ups 11.76 | wpb 3256.8 | bsz 98.6 | num_updates 13419 | lr 8.18957e-05 | gnorm 1.207 | train_wall 16 | gb_free 14.1 | wall 1717\n","2024-10-24 08:15:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:15:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 064:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:15:13 | INFO | fairseq.trainer | begin training epoch 64\n","2024-10-24 08:15:13 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:15:31 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)\n","2024-10-24 08:15:31 | INFO | train | epoch 064 | loss 4.743 | nll_loss 3.464 | ppl 11.03 | wps 38461.5 | ups 11.81 | wpb 3256.8 | bsz 98.6 | num_updates 13632 | lr 8.12534e-05 | gnorm 1.204 | train_wall 16 | gb_free 14.1 | wall 1735\n","2024-10-24 08:15:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:15:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 065:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:15:31 | INFO | fairseq.trainer | begin training epoch 65\n","2024-10-24 08:15:31 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:15:49 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)\n","2024-10-24 08:15:49 | INFO | train | epoch 065 | loss 4.73 | nll_loss 3.449 | ppl 10.92 | wps 38553.6 | ups 11.84 | wpb 3256.8 | bsz 98.6 | num_updates 13845 | lr 8.06259e-05 | gnorm 1.198 | train_wall 16 | gb_free 14 | wall 1753\n","2024-10-24 08:15:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:15:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 066:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:15:50 | INFO | fairseq.trainer | begin training epoch 66\n","2024-10-24 08:15:50 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 066:  72% 154/213 [00:13\u003c00:04, 12.77it/s, loss=4.726, nll_loss=3.445, ppl=10.89, wps=35923.2, ups=10.92, wpb=3288.7, bsz=98.1, num_updates=13900, lr=8.04663e-05, gnorm=1.198, train_wall=8, gb_free=14, wall=1758]2024-10-24 08:16:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 08:16:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 066 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:   1% 1/84 [00:00\u003c00:54,  1.52it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:   2% 2/84 [00:01\u003c00:43,  1.88it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:   4% 3/84 [00:01\u003c00:40,  2.01it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:   5% 4/84 [00:02\u003c00:40,  1.97it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:   6% 5/84 [00:02\u003c00:40,  1.96it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:   7% 6/84 [00:03\u003c00:39,  1.99it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:   8% 7/84 [00:03\u003c00:39,  1.97it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  10% 8/84 [00:04\u003c00:40,  1.90it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  11% 9/84 [00:04\u003c00:41,  1.80it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  12% 10/84 [00:05\u003c00:42,  1.73it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  13% 11/84 [00:06\u003c00:47,  1.52it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  14% 12/84 [00:06\u003c00:45,  1.59it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  15% 13/84 [00:07\u003c00:44,  1.60it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  17% 14/84 [00:07\u003c00:38,  1.83it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  18% 15/84 [00:08\u003c00:37,  1.83it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  19% 16/84 [00:08\u003c00:35,  1.90it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  20% 17/84 [00:09\u003c00:37,  1.81it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  21% 18/84 [00:09\u003c00:33,  1.94it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  23% 19/84 [00:10\u003c00:34,  1.90it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  24% 20/84 [00:10\u003c00:30,  2.07it/s]\u001b[A2024-10-24 08:16:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:16:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:16:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 066:  72% 154/213 [00:25\u003c00:04, 12.77it/s, loss=4.723, nll_loss=3.44, ppl=10.86, wps=42046.2, ups=12.69, wpb=3314, bsz=99.6, num_updates=14000, lr=8.01784e-05, gnorm=1.192, train_wall=7, gb_free=14.1, wall=1766] \n","epoch 066 | valid on 'valid' subset:  26% 22/84 [00:11\u003c00:33,  1.87it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  27% 23/84 [00:12\u003c00:30,  2.01it/s]\u001b[A2024-10-24 08:16:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:16:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:16:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 066 | valid on 'valid' subset:  29% 24/84 [00:13\u003c00:31,  1.89it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  30% 25/84 [00:13\u003c00:30,  1.91it/s]\u001b[A2024-10-24 08:16:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:16:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:16:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 066 | valid on 'valid' subset:  31% 26/84 [00:14\u003c00:34,  1.69it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  32% 27/84 [00:14\u003c00:33,  1.71it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  33% 28/84 [00:15\u003c00:30,  1.85it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  35% 29/84 [00:15\u003c00:30,  1.79it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  36% 30/84 [00:16\u003c00:28,  1.89it/s]\u001b[A2024-10-24 08:16:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:16:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:16:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 066 | valid on 'valid' subset:  37% 31/84 [00:16\u003c00:29,  1.80it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  38% 32/84 [00:17\u003c00:29,  1.75it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  39% 33/84 [00:18\u003c00:31,  1.63it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  40% 34/84 [00:19\u003c00:34,  1.46it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  42% 35/84 [00:19\u003c00:34,  1.43it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  43% 36/84 [00:21\u003c00:43,  1.11it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  44% 37/84 [00:22\u003c00:42,  1.10it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  45% 38/84 [00:22\u003c00:40,  1.13it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  46% 39/84 [00:23\u003c00:34,  1.30it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  48% 40/84 [00:24\u003c00:32,  1.37it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  49% 41/84 [00:24\u003c00:29,  1.47it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  50% 42/84 [00:25\u003c00:27,  1.51it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  51% 43/84 [00:25\u003c00:25,  1.59it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  52% 44/84 [00:26\u003c00:26,  1.54it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  54% 45/84 [00:27\u003c00:24,  1.60it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  55% 46/84 [00:27\u003c00:22,  1.71it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  56% 47/84 [00:28\u003c00:25,  1.44it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  57% 48/84 [00:29\u003c00:23,  1.54it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  58% 49/84 [00:29\u003c00:22,  1.54it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  60% 50/84 [00:30\u003c00:21,  1.57it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  61% 51/84 [00:31\u003c00:21,  1.55it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  62% 52/84 [00:31\u003c00:20,  1.59it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  63% 53/84 [00:32\u003c00:19,  1.60it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  64% 54/84 [00:32\u003c00:18,  1.66it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  65% 55/84 [00:33\u003c00:19,  1.51it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  67% 56/84 [00:34\u003c00:19,  1.46it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  68% 57/84 [00:35\u003c00:21,  1.24it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  69% 58/84 [00:36\u003c00:21,  1.21it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  70% 59/84 [00:37\u003c00:21,  1.14it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  71% 60/84 [00:38\u003c00:20,  1.16it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  73% 61/84 [00:38\u003c00:19,  1.16it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  74% 62/84 [00:39\u003c00:17,  1.24it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  75% 63/84 [00:40\u003c00:16,  1.28it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  76% 64/84 [00:40\u003c00:14,  1.39it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  77% 65/84 [00:41\u003c00:13,  1.41it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  79% 66/84 [00:42\u003c00:12,  1.39it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  80% 67/84 [00:43\u003c00:13,  1.30it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  81% 68/84 [00:43\u003c00:11,  1.35it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  82% 69/84 [00:44\u003c00:10,  1.41it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  83% 70/84 [00:45\u003c00:09,  1.41it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  85% 71/84 [00:45\u003c00:08,  1.47it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  86% 72/84 [00:46\u003c00:08,  1.48it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  87% 73/84 [00:47\u003c00:07,  1.51it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  88% 74/84 [00:47\u003c00:06,  1.50it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  89% 75/84 [00:48\u003c00:05,  1.55it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  90% 76/84 [00:49\u003c00:05,  1.53it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  92% 77/84 [00:50\u003c00:05,  1.22it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  93% 78/84 [00:51\u003c00:05,  1.12it/s]\u001b[A\n","epoch 066 | valid on 'valid' subset:  94% 79/84 [00:52\u003c00:05,  1.00s/it]\u001b[A\n","epoch 066 | valid on 'valid' subset:  95% 80/84 [00:53\u003c00:04,  1.09s/it]\u001b[A\n","epoch 066 | valid on 'valid' subset:  96% 81/84 [00:55\u003c00:03,  1.13s/it]\u001b[A\n","epoch 066 | valid on 'valid' subset:  98% 82/84 [00:56\u003c00:02,  1.10s/it]\u001b[A\n","epoch 066 | valid on 'valid' subset:  99% 83/84 [00:57\u003c00:01,  1.04s/it]\u001b[A\n","epoch 066 | valid on 'valid' subset: 100% 84/84 [00:57\u003c00:00,  1.02it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 08:17:01 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 4.983 | nll_loss 3.587 | ppl 12.02 | bleu 17.44 | wps 3513.4 | wpb 2417.9 | bsz 75.4 | num_updates 14000 | best_bleu 17.44\n","2024-10-24 08:17:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 14000 updates\n","2024-10-24 08:17:01 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_66_14000.pt\n","2024-10-24 08:17:01 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_66_14000.pt\n","2024-10-24 08:17:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpe/checkpoint_66_14000.pt (epoch 66 @ 14000 updates, score 17.44) (writing took 1.2711284739998518 seconds)\n","2024-10-24 08:17:07 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)\n","2024-10-24 08:17:07 | INFO | train | epoch 066 | loss 4.721 | nll_loss 3.438 | ppl 10.84 | wps 8934 | ups 2.74 | wpb 3256.8 | bsz 98.6 | num_updates 14058 | lr 8.00128e-05 | gnorm 1.212 | train_wall 17 | gb_free 14 | wall 1831\n","2024-10-24 08:17:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:17:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 067:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:17:07 | INFO | fairseq.trainer | begin training epoch 67\n","2024-10-24 08:17:07 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:17:27 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)\n","2024-10-24 08:17:27 | INFO | train | epoch 067 | loss 4.71 | nll_loss 3.426 | ppl 10.75 | wps 34744.5 | ups 10.67 | wpb 3256.8 | bsz 98.6 | num_updates 14271 | lr 7.94134e-05 | gnorm 1.213 | train_wall 18 | gb_free 14 | wall 1851\n","2024-10-24 08:17:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:17:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 068:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:17:27 | INFO | fairseq.trainer | begin training epoch 68\n","2024-10-24 08:17:27 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:17:45 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)\n","2024-10-24 08:17:45 | INFO | train | epoch 068 | loss 4.7 | nll_loss 3.413 | ppl 10.65 | wps 37954.8 | ups 11.65 | wpb 3256.8 | bsz 98.6 | num_updates 14484 | lr 7.88274e-05 | gnorm 1.232 | train_wall 17 | gb_free 14.1 | wall 1869\n","2024-10-24 08:17:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:17:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 069:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:17:45 | INFO | fairseq.trainer | begin training epoch 69\n","2024-10-24 08:17:45 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:18:03 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)\n","2024-10-24 08:18:03 | INFO | train | epoch 069 | loss 4.688 | nll_loss 3.399 | ppl 10.55 | wps 38229.3 | ups 11.74 | wpb 3256.8 | bsz 98.6 | num_updates 14697 | lr 7.82541e-05 | gnorm 1.22 | train_wall 16 | gb_free 14.1 | wall 1887\n","2024-10-24 08:18:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:18:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 070:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:18:04 | INFO | fairseq.trainer | begin training epoch 70\n","2024-10-24 08:18:04 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:18:23 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)\n","2024-10-24 08:18:23 | INFO | train | epoch 070 | loss 4.679 | nll_loss 3.389 | ppl 10.47 | wps 36164.2 | ups 11.1 | wpb 3256.8 | bsz 98.6 | num_updates 14910 | lr 7.76931e-05 | gnorm 1.235 | train_wall 17 | gb_free 14 | wall 1906\n","2024-10-24 08:18:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:18:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 071:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:18:23 | INFO | fairseq.trainer | begin training epoch 71\n","2024-10-24 08:18:23 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:18:41 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)\n","2024-10-24 08:18:41 | INFO | train | epoch 071 | loss 4.666 | nll_loss 3.373 | ppl 10.36 | wps 38592.4 | ups 11.85 | wpb 3256.8 | bsz 98.6 | num_updates 15123 | lr 7.7144e-05 | gnorm 1.227 | train_wall 16 | gb_free 14.1 | wall 1924\n","2024-10-24 08:18:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:18:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 072:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:18:41 | INFO | fairseq.trainer | begin training epoch 72\n","2024-10-24 08:18:41 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:18:59 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)\n","2024-10-24 08:18:59 | INFO | train | epoch 072 | loss 4.661 | nll_loss 3.367 | ppl 10.32 | wps 38269.6 | ups 11.75 | wpb 3256.8 | bsz 98.6 | num_updates 15336 | lr 7.66064e-05 | gnorm 1.233 | train_wall 16 | gb_free 14.1 | wall 1942\n","2024-10-24 08:18:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:18:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 073:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:18:59 | INFO | fairseq.trainer | begin training epoch 73\n","2024-10-24 08:18:59 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:19:17 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)\n","2024-10-24 08:19:17 | INFO | train | epoch 073 | loss 4.647 | nll_loss 3.35 | ppl 10.2 | wps 38673.3 | ups 11.87 | wpb 3256.8 | bsz 98.6 | num_updates 15549 | lr 7.60799e-05 | gnorm 1.233 | train_wall 16 | gb_free 14 | wall 1960\n","2024-10-24 08:19:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:19:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 074:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:19:17 | INFO | fairseq.trainer | begin training epoch 74\n","2024-10-24 08:19:17 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:19:35 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)\n","2024-10-24 08:19:35 | INFO | train | epoch 074 | loss 4.637 | nll_loss 3.34 | ppl 10.12 | wps 37795.1 | ups 11.6 | wpb 3256.8 | bsz 98.6 | num_updates 15762 | lr 7.55641e-05 | gnorm 1.237 | train_wall 17 | gb_free 14.1 | wall 1979\n","2024-10-24 08:19:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:19:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 075:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:19:35 | INFO | fairseq.trainer | begin training epoch 75\n","2024-10-24 08:19:35 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 075:  99% 211/213 [00:18\u003c00:00, 11.64it/s, loss=4.65, nll_loss=3.353, ppl=10.22, wps=42236.4, ups=12.76, wpb=3309.9, bsz=96.8, num_updates=15900, lr=7.52355e-05, gnorm=1.224, train_wall=7, gb_free=14.1, wall=1990]2024-10-24 08:19:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 08:19:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 075 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:   1% 1/84 [00:00\u003c01:05,  1.27it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:   2% 2/84 [00:01\u003c00:46,  1.75it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:   4% 3/84 [00:01\u003c00:38,  2.09it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:   5% 4/84 [00:01\u003c00:34,  2.29it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:   6% 5/84 [00:02\u003c00:33,  2.34it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:   7% 6/84 [00:02\u003c00:32,  2.43it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:   8% 7/84 [00:03\u003c00:32,  2.40it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  10% 8/84 [00:03\u003c00:32,  2.35it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  11% 9/84 [00:04\u003c00:31,  2.37it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  12% 10/84 [00:04\u003c00:30,  2.44it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  13% 11/84 [00:04\u003c00:33,  2.16it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  14% 12/84 [00:05\u003c00:31,  2.27it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  15% 13/84 [00:05\u003c00:32,  2.16it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  17% 14/84 [00:06\u003c00:30,  2.32it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  18% 15/84 [00:06\u003c00:30,  2.23it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  19% 16/84 [00:07\u003c00:29,  2.30it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  20% 17/84 [00:07\u003c00:29,  2.26it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  21% 18/84 [00:08\u003c00:28,  2.31it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  23% 19/84 [00:08\u003c00:30,  2.12it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  24% 20/84 [00:08\u003c00:28,  2.21it/s]\u001b[A2024-10-24 08:20:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:20:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:20:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 075 | valid on 'valid' subset:  25% 21/84 [00:09\u003c00:29,  2.11it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  26% 22/84 [00:10\u003c00:31,  1.99it/s]\u001b[A\n","epoch 075:  99% 211/213 [00:29\u003c00:00, 11.64it/s, loss=4.65, nll_loss=3.353, ppl=10.22, wps=42236.4, ups=12.76, wpb=3309.9, bsz=96.8, num_updates=15900, lr=7.52355e-05, gnorm=1.224, train_wall=7, gb_free=14.1, wall=1990]2024-10-24 08:20:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:20:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:20:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 075 | valid on 'valid' subset:  29% 24/84 [00:11\u003c00:35,  1.68it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  30% 25/84 [00:12\u003c00:36,  1.60it/s]\u001b[A2024-10-24 08:20:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:20:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:20:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 075 | valid on 'valid' subset:  31% 26/84 [00:12\u003c00:38,  1.50it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  32% 27/84 [00:13\u003c00:39,  1.44it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  33% 28/84 [00:14\u003c00:36,  1.52it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  35% 29/84 [00:14\u003c00:38,  1.42it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  36% 30/84 [00:15\u003c00:35,  1.53it/s]\u001b[A2024-10-24 08:20:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:20:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:20:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 075 | valid on 'valid' subset:  37% 31/84 [00:16\u003c00:34,  1.53it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  38% 32/84 [00:16\u003c00:30,  1.71it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  39% 33/84 [00:17\u003c00:28,  1.77it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  40% 34/84 [00:17\u003c00:28,  1.78it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  42% 35/84 [00:18\u003c00:25,  1.89it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  43% 36/84 [00:19\u003c00:31,  1.52it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  44% 37/84 [00:19\u003c00:30,  1.54it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  45% 38/84 [00:20\u003c00:28,  1.60it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  46% 39/84 [00:20\u003c00:26,  1.71it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  48% 40/84 [00:21\u003c00:25,  1.76it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  49% 41/84 [00:21\u003c00:23,  1.82it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  50% 42/84 [00:22\u003c00:23,  1.80it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  51% 43/84 [00:22\u003c00:23,  1.75it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  52% 44/84 [00:23\u003c00:22,  1.76it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  54% 45/84 [00:24\u003c00:22,  1.76it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  55% 46/84 [00:24\u003c00:22,  1.69it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  56% 47/84 [00:25\u003c00:22,  1.68it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  57% 48/84 [00:26\u003c00:22,  1.57it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  58% 49/84 [00:27\u003c00:25,  1.36it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  60% 50/84 [00:27\u003c00:24,  1.39it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  61% 51/84 [00:28\u003c00:25,  1.32it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  62% 52/84 [00:29\u003c00:24,  1.31it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  63% 53/84 [00:30\u003c00:24,  1.28it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  64% 54/84 [00:30\u003c00:23,  1.30it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  65% 55/84 [00:31\u003c00:21,  1.32it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  67% 56/84 [00:32\u003c00:19,  1.44it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  68% 57/84 [00:32\u003c00:18,  1.45it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  69% 58/84 [00:33\u003c00:16,  1.56it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  70% 59/84 [00:34\u003c00:16,  1.56it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  71% 60/84 [00:34\u003c00:14,  1.62it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  73% 61/84 [00:35\u003c00:13,  1.68it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  74% 62/84 [00:35\u003c00:12,  1.76it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  75% 63/84 [00:36\u003c00:12,  1.63it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  76% 64/84 [00:36\u003c00:11,  1.69it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  77% 65/84 [00:37\u003c00:11,  1.69it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  79% 66/84 [00:38\u003c00:10,  1.69it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  80% 67/84 [00:38\u003c00:11,  1.51it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  81% 68/84 [00:39\u003c00:10,  1.53it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  82% 69/84 [00:40\u003c00:09,  1.55it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  83% 70/84 [00:40\u003c00:09,  1.52it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  85% 71/84 [00:41\u003c00:08,  1.53it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  86% 72/84 [00:42\u003c00:08,  1.42it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  87% 73/84 [00:43\u003c00:08,  1.33it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  88% 74/84 [00:44\u003c00:08,  1.23it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  89% 75/84 [00:45\u003c00:07,  1.18it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  90% 76/84 [00:45\u003c00:06,  1.17it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  92% 77/84 [00:47\u003c00:06,  1.04it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  93% 78/84 [00:47\u003c00:05,  1.10it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  94% 79/84 [00:48\u003c00:04,  1.16it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  95% 80/84 [00:49\u003c00:03,  1.24it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  96% 81/84 [00:50\u003c00:02,  1.29it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  98% 82/84 [00:50\u003c00:01,  1.25it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  99% 83/84 [00:51\u003c00:00,  1.21it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset: 100% 84/84 [00:52\u003c00:00,  1.23it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 08:20:46 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 4.945 | nll_loss 3.537 | ppl 11.61 | bleu 17.62 | wps 3883.1 | wpb 2417.9 | bsz 75.4 | num_updates 15975 | best_bleu 17.62\n","2024-10-24 08:20:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 15975 updates\n","2024-10-24 08:20:46 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_best.pt\n","2024-10-24 08:20:46 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_best.pt\n","2024-10-24 08:20:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpe/checkpoint_best.pt (epoch 75 @ 15975 updates, score 17.62) (writing took 0.8232060360001014 seconds)\n","2024-10-24 08:20:47 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)\n","2024-10-24 08:20:47 | INFO | train | epoch 075 | loss 4.625 | nll_loss 3.325 | ppl 10.02 | wps 9658.6 | ups 2.97 | wpb 3256.8 | bsz 98.6 | num_updates 15975 | lr 7.50587e-05 | gnorm 1.229 | train_wall 17 | gb_free 14.2 | wall 2050\n","2024-10-24 08:20:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:20:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 076:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:20:47 | INFO | fairseq.trainer | begin training epoch 76\n","2024-10-24 08:20:47 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 076:  11% 24/213 [00:02\u003c00:15, 12.59it/s]2024-10-24 08:20:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 08:20:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 076 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:   1% 1/84 [00:00\u003c01:02,  1.34it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:   2% 2/84 [00:01\u003c01:09,  1.18it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:   4% 3/84 [00:02\u003c00:59,  1.37it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:   5% 4/84 [00:02\u003c00:52,  1.52it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:   6% 5/84 [00:03\u003c00:48,  1.63it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:   7% 6/84 [00:03\u003c00:44,  1.77it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:   8% 7/84 [00:04\u003c00:43,  1.76it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  10% 8/84 [00:05\u003c00:58,  1.29it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  11% 9/84 [00:06\u003c00:54,  1.38it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  12% 10/84 [00:06\u003c00:52,  1.40it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  13% 11/84 [00:08\u003c01:05,  1.11it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  14% 12/84 [00:08\u003c00:54,  1.32it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  15% 13/84 [00:09\u003c00:50,  1.42it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  17% 14/84 [00:09\u003c00:42,  1.65it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  18% 15/84 [00:10\u003c00:40,  1.70it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  19% 16/84 [00:10\u003c00:39,  1.72it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  20% 17/84 [00:11\u003c00:38,  1.75it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  21% 18/84 [00:11\u003c00:40,  1.64it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  23% 19/84 [00:12\u003c00:39,  1.66it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  24% 20/84 [00:12\u003c00:33,  1.88it/s]\u001b[A2024-10-24 08:21:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:21:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:21:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 076 | valid on 'valid' subset:  25% 21/84 [00:13\u003c00:34,  1.84it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  26% 22/84 [00:13\u003c00:32,  1.91it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  27% 23/84 [00:14\u003c00:29,  2.05it/s]\u001b[A2024-10-24 08:21:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:21:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:21:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 076:  11% 24/213 [00:17\u003c00:15, 12.59it/s, loss=4.614, nll_loss=3.313, ppl=9.94, wps=5157.6, ups=1.59, wpb=3244.8, bsz=97.6, num_updates=16000, lr=7.5e-05, gnorm=1.234, train_wall=8, gb_free=14.1, wall=2053]\n","epoch 076 | valid on 'valid' subset:  30% 25/84 [00:15\u003c00:29,  1.98it/s]\u001b[A2024-10-24 08:21:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:21:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:21:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 076 | valid on 'valid' subset:  31% 26/84 [00:16\u003c00:30,  1.90it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  32% 27/84 [00:16\u003c00:29,  1.91it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  33% 28/84 [00:16\u003c00:27,  2.05it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  35% 29/84 [00:17\u003c00:29,  1.85it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  36% 30/84 [00:18\u003c00:30,  1.79it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  37% 31/84 [00:19\u003c00:33,  1.57it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  38% 32/84 [00:19\u003c00:32,  1.60it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  39% 33/84 [00:20\u003c00:34,  1.47it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  40% 34/84 [00:21\u003c00:36,  1.35it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  42% 35/84 [00:22\u003c00:35,  1.36it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  43% 36/84 [00:23\u003c00:40,  1.17it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  44% 37/84 [00:23\u003c00:36,  1.28it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  45% 38/84 [00:24\u003c00:34,  1.33it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  46% 39/84 [00:24\u003c00:30,  1.50it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  48% 40/84 [00:25\u003c00:29,  1.50it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  49% 41/84 [00:26\u003c00:26,  1.60it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  50% 42/84 [00:26\u003c00:26,  1.61it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  51% 43/84 [00:27\u003c00:24,  1.67it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  52% 44/84 [00:27\u003c00:24,  1.66it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  54% 45/84 [00:28\u003c00:22,  1.73it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  55% 46/84 [00:28\u003c00:20,  1.83it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  56% 47/84 [00:29\u003c00:20,  1.78it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  57% 48/84 [00:30\u003c00:20,  1.78it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  58% 49/84 [00:30\u003c00:22,  1.52it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  60% 50/84 [00:31\u003c00:21,  1.60it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  61% 51/84 [00:32\u003c00:20,  1.60it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  62% 52/84 [00:32\u003c00:19,  1.64it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  63% 53/84 [00:33\u003c00:19,  1.57it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  64% 54/84 [00:34\u003c00:20,  1.50it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  65% 55/84 [00:34\u003c00:21,  1.37it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  67% 56/84 [00:35\u003c00:20,  1.37it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  68% 57/84 [00:36\u003c00:21,  1.25it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  69% 58/84 [00:37\u003c00:20,  1.25it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  70% 59/84 [00:38\u003c00:21,  1.18it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  71% 60/84 [00:38\u003c00:18,  1.33it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  73% 61/84 [00:39\u003c00:16,  1.38it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  74% 62/84 [00:40\u003c00:14,  1.50it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  75% 63/84 [00:40\u003c00:13,  1.53it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  76% 64/84 [00:41\u003c00:12,  1.57it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  77% 65/84 [00:42\u003c00:12,  1.52it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  79% 66/84 [00:42\u003c00:11,  1.56it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  80% 67/84 [00:43\u003c00:11,  1.44it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  81% 68/84 [00:44\u003c00:10,  1.49it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  82% 69/84 [00:44\u003c00:09,  1.53it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  83% 70/84 [00:45\u003c00:09,  1.55it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  85% 71/84 [00:45\u003c00:08,  1.58it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  86% 72/84 [00:46\u003c00:07,  1.56it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  87% 73/84 [00:47\u003c00:07,  1.51it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  88% 74/84 [00:48\u003c00:06,  1.47it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  89% 75/84 [00:48\u003c00:06,  1.42it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  90% 76/84 [00:49\u003c00:06,  1.31it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  92% 77/84 [00:50\u003c00:06,  1.15it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  93% 78/84 [00:51\u003c00:05,  1.10it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  94% 79/84 [00:52\u003c00:04,  1.05it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  95% 80/84 [00:53\u003c00:03,  1.00it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  96% 81/84 [00:54\u003c00:02,  1.03it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  98% 82/84 [00:55\u003c00:01,  1.05it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset:  99% 83/84 [00:56\u003c00:00,  1.08it/s]\u001b[A\n","epoch 076 | valid on 'valid' subset: 100% 84/84 [00:57\u003c00:00,  1.12it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 08:21:47 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 4.929 | nll_loss 3.513 | ppl 11.41 | bleu 18.06 | wps 3547.9 | wpb 2417.9 | bsz 75.4 | num_updates 16000 | best_bleu 18.06\n","2024-10-24 08:21:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 16000 updates\n","2024-10-24 08:21:47 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_76_16000.pt\n","2024-10-24 08:21:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_76_16000.pt\n","2024-10-24 08:21:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpe/checkpoint_76_16000.pt (epoch 76 @ 16000 updates, score 18.06) (writing took 1.272259535000103 seconds)\n","2024-10-24 08:22:06 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)\n","2024-10-24 08:22:06 | INFO | train | epoch 076 | loss 4.622 | nll_loss 3.322 | ppl 10 | wps 8750.3 | ups 2.69 | wpb 3256.8 | bsz 98.6 | num_updates 16188 | lr 7.45632e-05 | gnorm 1.269 | train_wall 18 | gb_free 14 | wall 2130\n","2024-10-24 08:22:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:22:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 077:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:22:06 | INFO | fairseq.trainer | begin training epoch 77\n","2024-10-24 08:22:06 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:22:24 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)\n","2024-10-24 08:22:24 | INFO | train | epoch 077 | loss 4.612 | nll_loss 3.309 | ppl 9.91 | wps 38067.9 | ups 11.69 | wpb 3256.8 | bsz 98.6 | num_updates 16401 | lr 7.40775e-05 | gnorm 1.252 | train_wall 16 | gb_free 14.1 | wall 2148\n","2024-10-24 08:22:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:22:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 078:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:22:25 | INFO | fairseq.trainer | begin training epoch 78\n","2024-10-24 08:22:25 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:22:42 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)\n","2024-10-24 08:22:42 | INFO | train | epoch 078 | loss 4.6 | nll_loss 3.296 | ppl 9.82 | wps 38356.7 | ups 11.78 | wpb 3256.8 | bsz 98.6 | num_updates 16614 | lr 7.36011e-05 | gnorm 1.251 | train_wall 16 | gb_free 14.2 | wall 2166\n","2024-10-24 08:22:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:22:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 079:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:22:43 | INFO | fairseq.trainer | begin training epoch 79\n","2024-10-24 08:22:43 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:23:01 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)\n","2024-10-24 08:23:01 | INFO | train | epoch 079 | loss 4.594 | nll_loss 3.288 | ppl 9.77 | wps 37863.8 | ups 11.63 | wpb 3256.8 | bsz 98.6 | num_updates 16827 | lr 7.31338e-05 | gnorm 1.252 | train_wall 17 | gb_free 14.1 | wall 2184\n","2024-10-24 08:23:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:23:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 080:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:23:01 | INFO | fairseq.trainer | begin training epoch 80\n","2024-10-24 08:23:01 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:23:19 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)\n","2024-10-24 08:23:19 | INFO | train | epoch 080 | loss 4.585 | nll_loss 3.278 | ppl 9.7 | wps 37990.4 | ups 11.66 | wpb 3256.8 | bsz 98.6 | num_updates 17040 | lr 7.26752e-05 | gnorm 1.258 | train_wall 16 | gb_free 14.1 | wall 2203\n","2024-10-24 08:23:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:23:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 081:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:23:19 | INFO | fairseq.trainer | begin training epoch 81\n","2024-10-24 08:23:19 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:23:37 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)\n","2024-10-24 08:23:37 | INFO | train | epoch 081 | loss 4.572 | nll_loss 3.263 | ppl 9.6 | wps 37596.8 | ups 11.54 | wpb 3256.8 | bsz 98.6 | num_updates 17253 | lr 7.22252e-05 | gnorm 1.26 | train_wall 17 | gb_free 14.1 | wall 2221\n","2024-10-24 08:23:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:23:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 082:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:23:38 | INFO | fairseq.trainer | begin training epoch 82\n","2024-10-24 08:23:38 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:23:56 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)\n","2024-10-24 08:23:56 | INFO | train | epoch 082 | loss 4.569 | nll_loss 3.258 | ppl 9.57 | wps 38294.4 | ups 11.76 | wpb 3256.8 | bsz 98.6 | num_updates 17466 | lr 7.17835e-05 | gnorm 1.265 | train_wall 16 | gb_free 14.1 | wall 2239\n","2024-10-24 08:23:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:23:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 083:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:23:56 | INFO | fairseq.trainer | begin training epoch 83\n","2024-10-24 08:23:56 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:24:14 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)\n","2024-10-24 08:24:14 | INFO | train | epoch 083 | loss 4.56 | nll_loss 3.248 | ppl 9.5 | wps 38459.7 | ups 11.81 | wpb 3256.8 | bsz 98.6 | num_updates 17679 | lr 7.13497e-05 | gnorm 1.274 | train_wall 16 | gb_free 14.1 | wall 2257\n","2024-10-24 08:24:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:24:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 084:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:24:14 | INFO | fairseq.trainer | begin training epoch 84\n","2024-10-24 08:24:14 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:24:32 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)\n","2024-10-24 08:24:32 | INFO | train | epoch 084 | loss 4.549 | nll_loss 3.235 | ppl 9.42 | wps 38104.5 | ups 11.7 | wpb 3256.8 | bsz 98.6 | num_updates 17892 | lr 7.09238e-05 | gnorm 1.263 | train_wall 16 | gb_free 14.1 | wall 2275\n","2024-10-24 08:24:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:24:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 085:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:24:32 | INFO | fairseq.trainer | begin training epoch 85\n","2024-10-24 08:24:32 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 085:  50% 107/213 [00:11\u003c00:09, 11.18it/s, loss=4.552, nll_loss=3.238, ppl=9.43, wps=39788.9, ups=12.17, wpb=3269.9, bsz=99, num_updates=17900, lr=7.09079e-05, gnorm=1.262, train_wall=8, gb_free=14.2, wall=2277]2024-10-24 08:24:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 08:24:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 085 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:   1% 1/84 [00:00\u003c01:02,  1.33it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:   2% 2/84 [00:01\u003c00:48,  1.69it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:   4% 3/84 [00:01\u003c00:42,  1.89it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:   5% 4/84 [00:02\u003c00:41,  1.91it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:   6% 5/84 [00:02\u003c00:37,  2.13it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:   7% 6/84 [00:02\u003c00:33,  2.30it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:   8% 7/84 [00:03\u003c00:32,  2.35it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  10% 8/84 [00:03\u003c00:33,  2.29it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  11% 9/84 [00:04\u003c00:32,  2.28it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  12% 10/84 [00:04\u003c00:31,  2.36it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  13% 11/84 [00:05\u003c00:32,  2.23it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  14% 12/84 [00:05\u003c00:31,  2.32it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  15% 13/84 [00:06\u003c00:31,  2.22it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  17% 14/84 [00:06\u003c00:29,  2.38it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  18% 15/84 [00:06\u003c00:30,  2.26it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  19% 16/84 [00:07\u003c00:28,  2.36it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  20% 17/84 [00:07\u003c00:29,  2.27it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  21% 18/84 [00:08\u003c00:28,  2.32it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  23% 19/84 [00:08\u003c00:30,  2.16it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  24% 20/84 [00:09\u003c00:27,  2.33it/s]\u001b[A2024-10-24 08:24:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:24:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:24:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 085 | valid on 'valid' subset:  25% 21/84 [00:09\u003c00:28,  2.20it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  26% 22/84 [00:10\u003c00:30,  2.05it/s]\u001b[A\n","epoch 085:  50% 107/213 [00:22\u003c00:09, 11.18it/s, loss=4.517, nll_loss=3.198, ppl=9.18, wps=32019.1, ups=9.56, wpb=3347.9, bsz=102.7, num_updates=18000, lr=7.07107e-05, gnorm=1.231, train_wall=9, gb_free=14, wall=2287]2024-10-24 08:24:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:24:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:24:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 085 | valid on 'valid' subset:  29% 24/84 [00:11\u003c00:35,  1.71it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  30% 25/84 [00:12\u003c00:35,  1.65it/s]\u001b[A2024-10-24 08:24:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:24:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:24:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 085 | valid on 'valid' subset:  31% 26/84 [00:12\u003c00:37,  1.53it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  32% 27/84 [00:13\u003c00:37,  1.51it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  33% 28/84 [00:14\u003c00:35,  1.57it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  35% 29/84 [00:14\u003c00:38,  1.43it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  36% 30/84 [00:15\u003c00:34,  1.58it/s]\u001b[A2024-10-24 08:24:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:24:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:24:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 085 | valid on 'valid' subset:  37% 31/84 [00:16\u003c00:32,  1.62it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  38% 32/84 [00:16\u003c00:28,  1.80it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  39% 33/84 [00:17\u003c00:29,  1.74it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  40% 34/84 [00:17\u003c00:27,  1.79it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  42% 35/84 [00:18\u003c00:26,  1.87it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  43% 36/84 [00:18\u003c00:30,  1.55it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  44% 37/84 [00:19\u003c00:28,  1.68it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  45% 38/84 [00:19\u003c00:26,  1.72it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  46% 39/84 [00:20\u003c00:24,  1.87it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  48% 40/84 [00:20\u003c00:23,  1.85it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  49% 41/84 [00:21\u003c00:22,  1.91it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  50% 42/84 [00:22\u003c00:22,  1.85it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  51% 43/84 [00:22\u003c00:22,  1.86it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  52% 44/84 [00:23\u003c00:21,  1.83it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  54% 45/84 [00:23\u003c00:20,  1.88it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  55% 46/84 [00:24\u003c00:19,  1.94it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  56% 47/84 [00:24\u003c00:19,  1.89it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  57% 48/84 [00:25\u003c00:20,  1.78it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  58% 49/84 [00:26\u003c00:22,  1.58it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  60% 50/84 [00:26\u003c00:22,  1.50it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  61% 51/84 [00:27\u003c00:22,  1.45it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  62% 52/84 [00:28\u003c00:23,  1.37it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  63% 53/84 [00:29\u003c00:23,  1.32it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  64% 54/84 [00:29\u003c00:22,  1.32it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  65% 55/84 [00:30\u003c00:23,  1.24it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  67% 56/84 [00:31\u003c00:20,  1.34it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  68% 57/84 [00:32\u003c00:19,  1.40it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  69% 58/84 [00:32\u003c00:16,  1.53it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  70% 59/84 [00:33\u003c00:15,  1.57it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  71% 60/84 [00:33\u003c00:14,  1.67it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  73% 61/84 [00:34\u003c00:13,  1.67it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  74% 62/84 [00:34\u003c00:12,  1.77it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  75% 63/84 [00:35\u003c00:12,  1.73it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  76% 64/84 [00:35\u003c00:11,  1.76it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  77% 65/84 [00:36\u003c00:11,  1.71it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  79% 66/84 [00:37\u003c00:10,  1.71it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  80% 67/84 [00:38\u003c00:11,  1.53it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  81% 68/84 [00:38\u003c00:10,  1.55it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  82% 69/84 [00:39\u003c00:09,  1.57it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  83% 70/84 [00:39\u003c00:08,  1.58it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  85% 71/84 [00:40\u003c00:07,  1.64it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  86% 72/84 [00:41\u003c00:07,  1.67it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  87% 73/84 [00:41\u003c00:07,  1.49it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  88% 74/84 [00:42\u003c00:07,  1.30it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  89% 75/84 [00:43\u003c00:07,  1.28it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  90% 76/84 [00:44\u003c00:06,  1.27it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  92% 77/84 [00:45\u003c00:06,  1.08it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  93% 78/84 [00:46\u003c00:05,  1.06it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  94% 79/84 [00:47\u003c00:04,  1.04it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  95% 80/84 [00:48\u003c00:03,  1.08it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  96% 81/84 [00:49\u003c00:02,  1.18it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  98% 82/84 [00:50\u003c00:01,  1.18it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset:  99% 83/84 [00:50\u003c00:00,  1.18it/s]\u001b[A\n","epoch 085 | valid on 'valid' subset: 100% 84/84 [00:51\u003c00:00,  1.18it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 08:25:35 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 4.908 | nll_loss 3.485 | ppl 11.2 | bleu 18.26 | wps 3947.4 | wpb 2417.9 | bsz 75.4 | num_updates 18000 | best_bleu 18.26\n","2024-10-24 08:25:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 18000 updates\n","2024-10-24 08:25:35 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_85_18000.pt\n","2024-10-24 08:25:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_85_18000.pt\n","2024-10-24 08:25:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpe/checkpoint_85_18000.pt (epoch 85 @ 18000 updates, score 18.26) (writing took 1.2696788470002502 seconds)\n","2024-10-24 08:25:47 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)\n","2024-10-24 08:25:47 | INFO | train | epoch 085 | loss 4.54 | nll_loss 3.225 | ppl 9.35 | wps 9238.2 | ups 2.84 | wpb 3256.8 | bsz 98.6 | num_updates 18105 | lr 7.05053e-05 | gnorm 1.26 | train_wall 19 | gb_free 14.1 | wall 2351\n","2024-10-24 08:25:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:25:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 086:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:25:47 | INFO | fairseq.trainer | begin training epoch 86\n","2024-10-24 08:25:47 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:26:05 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)\n","2024-10-24 08:26:05 | INFO | train | epoch 086 | loss 4.531 | nll_loss 3.214 | ppl 9.28 | wps 37499.9 | ups 11.51 | wpb 3256.8 | bsz 98.6 | num_updates 18318 | lr 7.00942e-05 | gnorm 1.26 | train_wall 17 | gb_free 14.1 | wall 2369\n","2024-10-24 08:26:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:26:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 087:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:26:06 | INFO | fairseq.trainer | begin training epoch 87\n","2024-10-24 08:26:06 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:26:23 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)\n","2024-10-24 08:26:23 | INFO | train | epoch 087 | loss 4.522 | nll_loss 3.203 | ppl 9.21 | wps 38453.9 | ups 11.81 | wpb 3256.8 | bsz 98.6 | num_updates 18531 | lr 6.96902e-05 | gnorm 1.263 | train_wall 16 | gb_free 14 | wall 2387\n","2024-10-24 08:26:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:26:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 088:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:26:24 | INFO | fairseq.trainer | begin training epoch 88\n","2024-10-24 08:26:24 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:26:42 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)\n","2024-10-24 08:26:42 | INFO | train | epoch 088 | loss 4.515 | nll_loss 3.195 | ppl 9.16 | wps 37995.7 | ups 11.67 | wpb 3256.8 | bsz 98.6 | num_updates 18744 | lr 6.92931e-05 | gnorm 1.271 | train_wall 16 | gb_free 14.1 | wall 2405\n","2024-10-24 08:26:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:26:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 089:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:26:42 | INFO | fairseq.trainer | begin training epoch 89\n","2024-10-24 08:26:42 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:27:00 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)\n","2024-10-24 08:27:00 | INFO | train | epoch 089 | loss 4.507 | nll_loss 3.186 | ppl 9.1 | wps 38508.4 | ups 11.82 | wpb 3256.8 | bsz 98.6 | num_updates 18957 | lr 6.89027e-05 | gnorm 1.271 | train_wall 16 | gb_free 14.1 | wall 2423\n","2024-10-24 08:27:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:27:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 090:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:27:00 | INFO | fairseq.trainer | begin training epoch 90\n","2024-10-24 08:27:00 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:27:18 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)\n","2024-10-24 08:27:18 | INFO | train | epoch 090 | loss 4.499 | nll_loss 3.176 | ppl 9.04 | wps 38451.1 | ups 11.81 | wpb 3256.8 | bsz 98.6 | num_updates 19170 | lr 6.85189e-05 | gnorm 1.267 | train_wall 16 | gb_free 14.1 | wall 2441\n","2024-10-24 08:27:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:27:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 091:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:27:18 | INFO | fairseq.trainer | begin training epoch 91\n","2024-10-24 08:27:18 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:27:36 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)\n","2024-10-24 08:27:36 | INFO | train | epoch 091 | loss 4.49 | nll_loss 3.165 | ppl 8.97 | wps 37541 | ups 11.53 | wpb 3256.8 | bsz 98.6 | num_updates 19383 | lr 6.81414e-05 | gnorm 1.268 | train_wall 17 | gb_free 14.1 | wall 2460\n","2024-10-24 08:27:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:27:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 092:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:27:37 | INFO | fairseq.trainer | begin training epoch 92\n","2024-10-24 08:27:37 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:27:55 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)\n","2024-10-24 08:27:55 | INFO | train | epoch 092 | loss 4.486 | nll_loss 3.161 | ppl 8.94 | wps 37227.7 | ups 11.43 | wpb 3256.8 | bsz 98.6 | num_updates 19596 | lr 6.777e-05 | gnorm 1.268 | train_wall 17 | gb_free 14 | wall 2479\n","2024-10-24 08:27:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:27:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 093:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:27:55 | INFO | fairseq.trainer | begin training epoch 93\n","2024-10-24 08:27:55 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:28:13 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)\n","2024-10-24 08:28:13 | INFO | train | epoch 093 | loss 4.472 | nll_loss 3.144 | ppl 8.84 | wps 37600.9 | ups 11.55 | wpb 3256.8 | bsz 98.6 | num_updates 19809 | lr 6.74047e-05 | gnorm 1.267 | train_wall 17 | gb_free 14.1 | wall 2497\n","2024-10-24 08:28:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:28:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 094:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:28:14 | INFO | fairseq.trainer | begin training epoch 94\n","2024-10-24 08:28:14 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 094:  89% 189/213 [00:16\u003c00:01, 12.34it/s, loss=4.438, nll_loss=3.104, ppl=8.6, wps=39212.2, ups=12.18, wpb=3220.2, bsz=99, num_updates=19900, lr=6.72504e-05, gnorm=1.278, train_wall=7, gb_free=14, wall=2505]2024-10-24 08:28:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 08:28:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 094 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:   1% 1/84 [00:00\u003c01:00,  1.37it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:   2% 2/84 [00:01\u003c00:45,  1.79it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:   4% 3/84 [00:01\u003c00:40,  2.02it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:   5% 4/84 [00:01\u003c00:35,  2.27it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:   6% 5/84 [00:02\u003c00:33,  2.35it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:   7% 6/84 [00:02\u003c00:31,  2.50it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:   8% 7/84 [00:03\u003c00:30,  2.54it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  10% 8/84 [00:03\u003c00:30,  2.46it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  11% 9/84 [00:03\u003c00:29,  2.57it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  12% 10/84 [00:04\u003c00:29,  2.51it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  13% 11/84 [00:04\u003c00:32,  2.24it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  14% 12/84 [00:05\u003c00:30,  2.35it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  15% 13/84 [00:05\u003c00:31,  2.26it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  17% 14/84 [00:06\u003c00:29,  2.39it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  18% 15/84 [00:06\u003c00:31,  2.16it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  19% 16/84 [00:07\u003c00:31,  2.16it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  20% 17/84 [00:07\u003c00:35,  1.87it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  21% 18/84 [00:08\u003c00:35,  1.87it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  23% 19/84 [00:09\u003c00:38,  1.67it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  24% 20/84 [00:09\u003c00:35,  1.78it/s]\u001b[A2024-10-24 08:28:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:28:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:28:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 094 | valid on 'valid' subset:  25% 21/84 [00:10\u003c00:40,  1.56it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  26% 22/84 [00:11\u003c00:39,  1.56it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  27% 23/84 [00:11\u003c00:38,  1.60it/s]\u001b[A2024-10-24 08:28:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:28:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:28:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 094 | valid on 'valid' subset:  29% 24/84 [00:12\u003c00:40,  1.47it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  30% 25/84 [00:13\u003c00:39,  1.49it/s]\u001b[A2024-10-24 08:28:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:28:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:28:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 094 | valid on 'valid' subset:  31% 26/84 [00:13\u003c00:38,  1.52it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  32% 27/84 [00:14\u003c00:35,  1.60it/s]\u001b[A\n","epoch 094:  89% 189/213 [00:31\u003c00:01, 12.34it/s, loss=4.473, nll_loss=3.146, ppl=8.85, wps=35953.6, ups=11.33, wpb=3174.6, bsz=97, num_updates=20000, lr=6.7082e-05, gnorm=1.291, train_wall=8, gb_free=14.3, wall=2513]\n","epoch 094 | valid on 'valid' subset:  35% 29/84 [00:15\u003c00:30,  1.79it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  36% 30/84 [00:15\u003c00:28,  1.90it/s]\u001b[A2024-10-24 08:28:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:28:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:28:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 094 | valid on 'valid' subset:  37% 31/84 [00:16\u003c00:30,  1.74it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  38% 32/84 [00:16\u003c00:27,  1.89it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  39% 33/84 [00:17\u003c00:28,  1.78it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  40% 34/84 [00:17\u003c00:28,  1.78it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  42% 35/84 [00:18\u003c00:26,  1.82it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  43% 36/84 [00:19\u003c00:31,  1.51it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  44% 37/84 [00:19\u003c00:28,  1.64it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  45% 38/84 [00:20\u003c00:27,  1.65it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  46% 39/84 [00:20\u003c00:25,  1.79it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  48% 40/84 [00:21\u003c00:24,  1.78it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  49% 41/84 [00:22\u003c00:23,  1.81it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  50% 42/84 [00:22\u003c00:23,  1.78it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  51% 43/84 [00:23\u003c00:24,  1.68it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  52% 44/84 [00:24\u003c00:26,  1.49it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  54% 45/84 [00:24\u003c00:26,  1.46it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  55% 46/84 [00:25\u003c00:30,  1.24it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  56% 47/84 [00:26\u003c00:30,  1.22it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  57% 48/84 [00:27\u003c00:29,  1.22it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  58% 49/84 [00:28\u003c00:29,  1.17it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  60% 50/84 [00:29\u003c00:26,  1.26it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  61% 51/84 [00:29\u003c00:24,  1.37it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  62% 52/84 [00:30\u003c00:22,  1.45it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  63% 53/84 [00:30\u003c00:20,  1.51it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  64% 54/84 [00:31\u003c00:18,  1.61it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  65% 55/84 [00:32\u003c00:17,  1.62it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  67% 56/84 [00:32\u003c00:16,  1.69it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  68% 57/84 [00:33\u003c00:16,  1.62it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  69% 58/84 [00:33\u003c00:15,  1.69it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  70% 59/84 [00:34\u003c00:15,  1.65it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  71% 60/84 [00:35\u003c00:14,  1.69it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  73% 61/84 [00:35\u003c00:14,  1.64it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  74% 62/84 [00:36\u003c00:12,  1.75it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  75% 63/84 [00:36\u003c00:12,  1.67it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  76% 64/84 [00:37\u003c00:11,  1.73it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  77% 65/84 [00:37\u003c00:11,  1.72it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  79% 66/84 [00:38\u003c00:10,  1.70it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  80% 67/84 [00:39\u003c00:12,  1.39it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  81% 68/84 [00:40\u003c00:12,  1.33it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  82% 69/84 [00:41\u003c00:11,  1.30it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  83% 70/84 [00:42\u003c00:11,  1.22it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  85% 71/84 [00:43\u003c00:10,  1.22it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  86% 72/84 [00:43\u003c00:10,  1.19it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  87% 73/84 [00:44\u003c00:09,  1.19it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  88% 74/84 [00:45\u003c00:08,  1.22it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  89% 75/84 [00:46\u003c00:06,  1.33it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  90% 76/84 [00:46\u003c00:05,  1.39it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  92% 77/84 [00:47\u003c00:05,  1.32it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  93% 78/84 [00:48\u003c00:04,  1.39it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  94% 79/84 [00:48\u003c00:03,  1.39it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  95% 80/84 [00:49\u003c00:03,  1.33it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  96% 81/84 [00:50\u003c00:02,  1.36it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  98% 82/84 [00:51\u003c00:01,  1.33it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset:  99% 83/84 [00:52\u003c00:00,  1.28it/s]\u001b[A\n","epoch 094 | valid on 'valid' subset: 100% 84/84 [00:52\u003c00:00,  1.27it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 08:29:23 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 4.856 | nll_loss 3.418 | ppl 10.69 | bleu 19.16 | wps 3859.6 | wpb 2417.9 | bsz 75.4 | num_updates 20000 | best_bleu 19.16\n","2024-10-24 08:29:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 20000 updates\n","2024-10-24 08:29:23 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_94_20000.pt\n","2024-10-24 08:29:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_94_20000.pt\n","2024-10-24 08:29:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpe/checkpoint_94_20000.pt (epoch 94 @ 20000 updates, score 19.16) (writing took 1.2409107280000171 seconds)\n","2024-10-24 08:29:26 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)\n","2024-10-24 08:29:26 | INFO | train | epoch 094 | loss 4.47 | nll_loss 3.142 | ppl 8.83 | wps 9554.9 | ups 2.93 | wpb 3256.8 | bsz 98.6 | num_updates 20022 | lr 6.70452e-05 | gnorm 1.277 | train_wall 17 | gb_free 14.1 | wall 2570\n","2024-10-24 08:29:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:29:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 095:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:29:26 | INFO | fairseq.trainer | begin training epoch 95\n","2024-10-24 08:29:26 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:29:46 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)\n","2024-10-24 08:29:46 | INFO | train | epoch 095 | loss 4.463 | nll_loss 3.133 | ppl 8.77 | wps 34396.2 | ups 10.56 | wpb 3256.8 | bsz 98.6 | num_updates 20235 | lr 6.66914e-05 | gnorm 1.28 | train_wall 18 | gb_free 14 | wall 2590\n","2024-10-24 08:29:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:29:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 096:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:29:46 | INFO | fairseq.trainer | begin training epoch 96\n","2024-10-24 08:29:46 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:30:04 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)\n","2024-10-24 08:30:04 | INFO | train | epoch 096 | loss 4.455 | nll_loss 3.123 | ppl 8.71 | wps 38445.7 | ups 11.8 | wpb 3256.8 | bsz 98.6 | num_updates 20448 | lr 6.63431e-05 | gnorm 1.278 | train_wall 16 | gb_free 14.2 | wall 2608\n","2024-10-24 08:30:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:30:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 097:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:30:04 | INFO | fairseq.trainer | begin training epoch 97\n","2024-10-24 08:30:04 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:30:22 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)\n","2024-10-24 08:30:22 | INFO | train | epoch 097 | loss 4.45 | nll_loss 3.117 | ppl 8.68 | wps 38263.1 | ups 11.75 | wpb 3256.8 | bsz 98.6 | num_updates 20661 | lr 6.60003e-05 | gnorm 1.298 | train_wall 16 | gb_free 14 | wall 2626\n","2024-10-24 08:30:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:30:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 098:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:30:23 | INFO | fairseq.trainer | begin training epoch 98\n","2024-10-24 08:30:23 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:30:41 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)\n","2024-10-24 08:30:41 | INFO | train | epoch 098 | loss 4.441 | nll_loss 3.107 | ppl 8.62 | wps 37231 | ups 11.43 | wpb 3256.8 | bsz 98.6 | num_updates 20874 | lr 6.56627e-05 | gnorm 1.289 | train_wall 17 | gb_free 14 | wall 2645\n","2024-10-24 08:30:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:30:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 099:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:30:41 | INFO | fairseq.trainer | begin training epoch 99\n","2024-10-24 08:30:41 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:30:59 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)\n","2024-10-24 08:30:59 | INFO | train | epoch 099 | loss 4.436 | nll_loss 3.101 | ppl 8.58 | wps 37798.3 | ups 11.61 | wpb 3256.8 | bsz 98.6 | num_updates 21087 | lr 6.53302e-05 | gnorm 1.303 | train_wall 17 | gb_free 14.1 | wall 2663\n","2024-10-24 08:30:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:30:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 213\n","epoch 100:   0% 0/213 [00:00\u003c?, ?it/s]2024-10-24 08:31:00 | INFO | fairseq.trainer | begin training epoch 100\n","2024-10-24 08:31:00 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 100:  99% 211/213 [00:18\u003c00:00, 11.01it/s, loss=4.426, nll_loss=3.089, ppl=8.51, wps=39885.8, ups=11.96, wpb=3334.8, bsz=98.7, num_updates=21200, lr=6.51558e-05, gnorm=1.263, train_wall=8, gb_free=14.1, wall=2673]2024-10-24 08:31:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 08:31:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 100 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:   1% 1/84 [00:00\u003c01:03,  1.30it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:   2% 2/84 [00:01\u003c00:52,  1.57it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:   4% 3/84 [00:01\u003c00:45,  1.79it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:   5% 4/84 [00:02\u003c00:41,  1.94it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:   6% 5/84 [00:02\u003c00:36,  2.14it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:   7% 6/84 [00:02\u003c00:33,  2.30it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:   8% 7/84 [00:03\u003c00:33,  2.31it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  10% 8/84 [00:03\u003c00:33,  2.26it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  11% 9/84 [00:04\u003c00:31,  2.35it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  12% 10/84 [00:04\u003c00:30,  2.40it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  13% 11/84 [00:05\u003c00:33,  2.18it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  14% 12/84 [00:05\u003c00:32,  2.23it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  15% 13/84 [00:06\u003c00:36,  1.94it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  17% 14/84 [00:06\u003c00:32,  2.14it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  18% 15/84 [00:07\u003c00:33,  2.03it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  19% 16/84 [00:07\u003c00:32,  2.08it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  20% 17/84 [00:08\u003c00:33,  1.99it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  21% 18/84 [00:08\u003c00:31,  2.10it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  23% 19/84 [00:09\u003c00:33,  1.95it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  24% 20/84 [00:09\u003c00:30,  2.13it/s]\u001b[A2024-10-24 08:31:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:31:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:31:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 100 | valid on 'valid' subset:  25% 21/84 [00:10\u003c00:31,  1.97it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  26% 22/84 [00:10\u003c00:34,  1.79it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  27% 23/84 [00:11\u003c00:33,  1.81it/s]\u001b[A2024-10-24 08:31:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:31:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:31:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 100 | valid on 'valid' subset:  29% 24/84 [00:12\u003c00:37,  1.59it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  30% 25/84 [00:12\u003c00:37,  1.59it/s]\u001b[A2024-10-24 08:31:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:31:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:31:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 100 | valid on 'valid' subset:  31% 26/84 [00:13\u003c00:42,  1.37it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  32% 27/84 [00:14\u003c00:43,  1.32it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  33% 28/84 [00:15\u003c00:39,  1.40it/s]\u001b[A\n","epoch 100:  99% 211/213 [00:35\u003c00:00, 11.01it/s, loss=4.419, nll_loss=3.081, ppl=8.46, wps=37625.6, ups=11.73, wpb=3208.8, bsz=100.7, num_updates=21300, lr=6.50027e-05, gnorm=1.295, train_wall=8, gb_free=14, wall=2682] \n","epoch 100 | valid on 'valid' subset:  36% 30/84 [00:16\u003c00:40,  1.33it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  37% 31/84 [00:17\u003c00:39,  1.36it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  38% 32/84 [00:18\u003c00:34,  1.53it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  39% 33/84 [00:18\u003c00:31,  1.60it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  40% 34/84 [00:19\u003c00:30,  1.64it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  42% 35/84 [00:19\u003c00:28,  1.74it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  43% 36/84 [00:20\u003c00:33,  1.45it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  44% 37/84 [00:21\u003c00:30,  1.54it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  45% 38/84 [00:21\u003c00:29,  1.56it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  46% 39/84 [00:22\u003c00:26,  1.71it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  48% 40/84 [00:22\u003c00:25,  1.72it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  49% 41/84 [00:23\u003c00:24,  1.76it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  50% 42/84 [00:23\u003c00:24,  1.74it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  51% 43/84 [00:24\u003c00:26,  1.55it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  52% 44/84 [00:25\u003c00:24,  1.61it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  54% 45/84 [00:25\u003c00:22,  1.70it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  55% 46/84 [00:26\u003c00:22,  1.73it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  56% 47/84 [00:27\u003c00:21,  1.70it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  57% 48/84 [00:27\u003c00:23,  1.52it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  58% 49/84 [00:28\u003c00:26,  1.31it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  60% 50/84 [00:29\u003c00:25,  1.32it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  61% 51/84 [00:30\u003c00:27,  1.21it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  62% 52/84 [00:31\u003c00:27,  1.14it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  63% 53/84 [00:32\u003c00:26,  1.17it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  64% 54/84 [00:33\u003c00:24,  1.25it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  65% 55/84 [00:33\u003c00:21,  1.33it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  67% 56/84 [00:34\u003c00:19,  1.45it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  68% 57/84 [00:34\u003c00:17,  1.50it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  69% 58/84 [00:35\u003c00:16,  1.60it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  70% 59/84 [00:36\u003c00:15,  1.57it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  71% 60/84 [00:36\u003c00:14,  1.62it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  73% 61/84 [00:37\u003c00:14,  1.62it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  74% 62/84 [00:37\u003c00:13,  1.69it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  75% 63/84 [00:38\u003c00:13,  1.58it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  76% 64/84 [00:39\u003c00:12,  1.61it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  77% 65/84 [00:39\u003c00:12,  1.54it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  79% 66/84 [00:40\u003c00:11,  1.55it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  80% 67/84 [00:41\u003c00:10,  1.59it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  81% 68/84 [00:41\u003c00:10,  1.54it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  82% 69/84 [00:42\u003c00:09,  1.56it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  83% 70/84 [00:43\u003c00:09,  1.44it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  85% 71/84 [00:43\u003c00:09,  1.37it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  86% 72/84 [00:44\u003c00:09,  1.31it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  87% 73/84 [00:45\u003c00:08,  1.23it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  88% 74/84 [00:47\u003c00:09,  1.04it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  89% 75/84 [00:48\u003c00:08,  1.04it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  90% 76/84 [00:48\u003c00:07,  1.09it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  92% 77/84 [00:49\u003c00:05,  1.17it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  93% 78/84 [00:50\u003c00:04,  1.21it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  94% 79/84 [00:51\u003c00:04,  1.20it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  95% 80/84 [00:52\u003c00:03,  1.19it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  96% 81/84 [00:52\u003c00:02,  1.24it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  98% 82/84 [00:53\u003c00:01,  1.19it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  99% 83/84 [00:54\u003c00:00,  1.17it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset: 100% 84/84 [00:55\u003c00:00,  1.20it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 08:32:13 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 4.832 | nll_loss 3.389 | ppl 10.48 | bleu 19.6 | wps 3690.5 | wpb 2417.9 | bsz 75.4 | num_updates 21300 | best_bleu 19.6\n","2024-10-24 08:32:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 21300 updates\n","2024-10-24 08:32:13 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_best.pt\n","2024-10-24 08:32:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpe/checkpoints-bpe/checkpoint_best.pt\n","2024-10-24 08:32:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpe/checkpoint_best.pt (epoch 100 @ 21300 updates, score 19.6) (writing took 0.8422331809997559 seconds)\n","2024-10-24 08:32:14 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)\n","2024-10-24 08:32:14 | INFO | train | epoch 100 | loss 4.426 | nll_loss 3.09 | ppl 8.51 | wps 9276.9 | ups 2.85 | wpb 3256.8 | bsz 98.6 | num_updates 21300 | lr 6.50027e-05 | gnorm 1.284 | train_wall 17 | gb_free 14 | wall 2738\n","2024-10-24 08:32:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:32:14 | INFO | fairseq_cli.train | done training in 2729.7 seconds\n"]}],"source":["!fairseq-train data-bin \\\n","--arch transformer \\\n","--activation-fn relu \\\n","--share-decoder-input-output-embed \\\n","--share-all-embeddings \\\n","--encoder-layers 3 \\\n","--encoder-attention-heads 4 \\\n","--encoder-embed-dim 256 \\\n","--encoder-ffn-embed-dim 1024 \\\n","--decoder-layers 3 \\\n","--decoder-attention-heads 4 \\\n","--decoder-embed-dim 256 \\\n","--decoder-ffn-embed-dim 1024 \\\n","--dropout 0.25 \\\n","--seed 2024 \\\n","--optimizer 'adam' \\\n","--adam-betas '(0.9, 0.999)' \\\n","--lr-scheduler 'inverse_sqrt' \\\n","--patience 5 \\\n","--warmup-updates 1000 \\\n","--criterion 'label_smoothed_cross_entropy' \\\n","--label-smoothing 0.1 \\\n","--lr 0.0003 \\\n","--weight-decay 0.0 \\\n","--max-tokens 4096 \\\n","--max-tokens-valid 3600 \\\n","--required-batch-size-multiple 1 \\\n","--best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n","--max-epoch 100 \\\n","--validate-interval 25 \\\n","--save-interval 25 \\\n","--validate-interval-updates 2000 \\\n","--save-interval-updates 2000 \\\n","--log-interval 100 \\\n","--curriculum 0 \\\n","--no-epoch-checkpoints \\\n","--eval-bleu \\\n","--eval-bleu-args '{\"beam\": 5, \"max_len_a\": 0, \"max_len_b\": 100, \"len_pen\": 1}' \\\n","--eval-bleu-detok space \\\n","--eval-bleu-remove-bpe \\\n","--save-dir checkpoints-bpe \\\n","--ddp-backend=no_c10d \\\n","--wandb-project 'fairseq-standard-subword-tok-eng-to-nso'"]},{"cell_type":"markdown","metadata":{"id":"cIUaICtsUHwW"},"source":["## Training NMT with Unigram Language Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KqApisL1UNp2"},"outputs":[],"source":["# change working directory\n","os.chdir(f'/content/drive/MyDrive/Research/eng-to-{target_code}/ulm')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"em8nq3m6USS8","outputId":"586275f2-6bdf-41e2-f7e0-b5e95cbc2b93"},"outputs":[{"name":"stdout","output_type":"stream","text":["2024-10-24 08:38:41.096197: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-10-24 08:38:41.129155: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-10-24 08:38:41.139838: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-10-24 08:38:41.162741: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-10-24 08:38:42.724378: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-10-24 08:38:44 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n","2024-10-24 08:38:50 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': 'fairseq-standard-subword-tok-eng-to-nso', 'azureml_logging': False, 'seed': 2024, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 25, 'validate_interval_updates': 2000, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3600, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0003], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints-ulm', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 25, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 5, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project='fairseq-standard-subword-tok-eng-to-nso', azureml_logging=False, seed=2024, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, batch_size=None, required_batch_size_multiple=1, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=25, validate_interval_updates=2000, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid='3600', batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=100, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0003], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='checkpoints-ulm', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=25, save_interval_updates=2000, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='bleu', maximize_best_checkpoint_metric=True, patience=5, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='data-bin', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 0, \"max_len_b\": 100, \"len_pen\": 1}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe='sentencepiece', eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.999)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=1000, warmup_init_lr=-1, pad=1, eos=2, unk=3, activation_fn='relu', share_decoder_input_output_embed=True, share_all_embeddings=True, encoder_layers=3, encoder_attention_heads=4, encoder_embed_dim=256, encoder_ffn_embed_dim=1024, decoder_layers=3, decoder_attention_heads=4, decoder_embed_dim=256, decoder_ffn_embed_dim=1024, dropout=0.25, no_seed_provided=False, encoder_embed_path=None, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=256, decoder_input_dim=256, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 0, \"max_len_b\": 100, \"len_pen\": 1}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': 'sentencepiece', 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0003]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 1000, 'warmup_init_lr': -1.0, 'lr': [0.0003]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n","2024-10-24 08:38:50 | INFO | fairseq.tasks.translation | [eng] dictionary: 3992 types\n","2024-10-24 08:38:51 | INFO | fairseq.tasks.translation | [nso] dictionary: 3992 types\n","2024-10-24 08:38:51 | INFO | fairseq_cli.train | TransformerModel(\n","  (encoder): TransformerEncoderBase(\n","    (dropout_module): FairseqDropout()\n","    (embed_tokens): Embedding(3992, 256, padding_idx=1)\n","    (embed_positions): SinusoidalPositionalEmbedding()\n","    (layers): ModuleList(\n","      (0-2): 3 x TransformerEncoderLayerBase(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (decoder): TransformerDecoderBase(\n","    (dropout_module): FairseqDropout()\n","    (embed_tokens): Embedding(3992, 256, padding_idx=1)\n","    (embed_positions): SinusoidalPositionalEmbedding()\n","    (layers): ModuleList(\n","      (0-2): 3 x TransformerDecoderLayerBase(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (output_projection): Linear(in_features=256, out_features=3992, bias=False)\n","  )\n",")\n","2024-10-24 08:38:51 | INFO | fairseq_cli.train | task: TranslationTask\n","2024-10-24 08:38:51 | INFO | fairseq_cli.train | model: TransformerModel\n","2024-10-24 08:38:51 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n","2024-10-24 08:38:51 | INFO | fairseq_cli.train | num. shared model params: 6,551,552 (num. trained: 6,551,552)\n","2024-10-24 08:38:51 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n","2024-10-24 08:38:51 | INFO | fairseq.data.data_utils | loaded 6,336 examples from: data-bin/valid.eng-nso.eng\n","2024-10-24 08:38:51 | INFO | fairseq.data.data_utils | loaded 6,336 examples from: data-bin/valid.eng-nso.nso\n","2024-10-24 08:38:51 | INFO | fairseq.tasks.translation | data-bin valid eng-nso 6336 examples\n","2024-10-24 08:38:51 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight \u003c- decoder.embed_tokens.weight\n","2024-10-24 08:38:51 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight \u003c- decoder.output_projection.weight\n","2024-10-24 08:38:51 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2024-10-24 08:38:51 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n","2024-10-24 08:38:51 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2024-10-24 08:38:51 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n","2024-10-24 08:38:51 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n","2024-10-24 08:38:51 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints-ulm/checkpoint_last.pt\n","2024-10-24 08:38:51 | INFO | fairseq.trainer | No existing checkpoint found checkpoints-ulm/checkpoint_last.pt\n","2024-10-24 08:38:51 | INFO | fairseq.trainer | loading train data for epoch 1\n","2024-10-24 08:38:51 | INFO | fairseq.data.data_utils | loaded 20,994 examples from: data-bin/train.eng-nso.eng\n","2024-10-24 08:38:51 | INFO | fairseq.data.data_utils | loaded 20,994 examples from: data-bin/train.eng-nso.nso\n","2024-10-24 08:38:51 | INFO | fairseq.tasks.translation | data-bin train eng-nso 20994 examples\n","2024-10-24 08:38:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:38:51 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2024-10-24 08:38:51 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2024-10-24 08:38:51 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2024-10-24 08:38:51 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n","2024-10-24 08:38:51 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n","2024-10-24 08:38:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:38:51 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2024-10-24 08:38:51 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2024-10-24 08:38:51 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2024-10-24 08:38:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 001:   0% 0/219 [00:00\u003c?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtyobeka-mandisa\u001b[0m (\u001b[33mtyobeka-mandisa-university-of-cape-town\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.5\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/Research/eng-to-nso/ulm/wandb/run-20241024_083853-o2xdojoj\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcheckpoints-ulm\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 猸锔 View project at \u001b[34m\u001b[4mhttps://wandb.ai/tyobeka-mandisa-university-of-cape-town/fairseq-standard-subword-tok-eng-to-nso\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/tyobeka-mandisa-university-of-cape-town/fairseq-standard-subword-tok-eng-to-nso/runs/o2xdojoj\u001b[0m\n","2024-10-24 08:38:53 | INFO | fairseq.trainer | begin training epoch 1\n","2024-10-24 08:38:53 | INFO | fairseq_cli.train | Start iterating over samples\n","/content/drive/MyDrive/Research/eng-to-nso/fairseq/fairseq/tasks/fairseq_task.py:531: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","/content/drive/MyDrive/Research/eng-to-nso/fairseq/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n","  warnings.warn(\n","2024-10-24 08:39:13 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n","2024-10-24 08:39:13 | INFO | train | epoch 001 | loss 10.659 | nll_loss 10.423 | ppl 1373.08 | wps 38507.1 | ups 12.02 | wpb 3195.3 | bsz 95.9 | num_updates 219 | lr 6.57e-05 | gnorm 1.613 | train_wall 18 | gb_free 14.2 | wall 22\n","2024-10-24 08:39:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:39:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 002:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:39:13 | INFO | fairseq.trainer | begin training epoch 2\n","2024-10-24 08:39:13 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:39:34 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n","2024-10-24 08:39:34 | INFO | train | epoch 002 | loss 9.142 | nll_loss 8.658 | ppl 403.84 | wps 34123.1 | ups 10.68 | wpb 3195.3 | bsz 95.9 | num_updates 438 | lr 0.0001314 | gnorm 0.876 | train_wall 18 | gb_free 14.2 | wall 43\n","2024-10-24 08:39:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:39:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 003:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:39:34 | INFO | fairseq.trainer | begin training epoch 3\n","2024-10-24 08:39:34 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:39:53 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n","2024-10-24 08:39:53 | INFO | train | epoch 003 | loss 8.657 | nll_loss 8.074 | ppl 269.43 | wps 36879.3 | ups 11.54 | wpb 3195.3 | bsz 95.9 | num_updates 657 | lr 0.0001971 | gnorm 1.058 | train_wall 17 | gb_free 14.2 | wall 62\n","2024-10-24 08:39:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:39:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 004:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:39:53 | INFO | fairseq.trainer | begin training epoch 4\n","2024-10-24 08:39:53 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:40:13 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n","2024-10-24 08:40:13 | INFO | train | epoch 004 | loss 7.995 | nll_loss 7.301 | ppl 157.71 | wps 35275.4 | ups 11.04 | wpb 3195.3 | bsz 95.9 | num_updates 876 | lr 0.0002628 | gnorm 1.047 | train_wall 17 | gb_free 14.2 | wall 82\n","2024-10-24 08:40:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:40:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 005:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:40:13 | INFO | fairseq.trainer | begin training epoch 5\n","2024-10-24 08:40:13 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:40:33 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n","2024-10-24 08:40:33 | INFO | train | epoch 005 | loss 7.513 | nll_loss 6.734 | ppl 106.42 | wps 34992.9 | ups 10.95 | wpb 3195.3 | bsz 95.9 | num_updates 1095 | lr 0.000286691 | gnorm 0.977 | train_wall 17 | gb_free 14.1 | wall 102\n","2024-10-24 08:40:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:40:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 006:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:40:33 | INFO | fairseq.trainer | begin training epoch 6\n","2024-10-24 08:40:33 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:40:54 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n","2024-10-24 08:40:54 | INFO | train | epoch 006 | loss 7.184 | nll_loss 6.348 | ppl 81.48 | wps 33584.8 | ups 10.51 | wpb 3195.3 | bsz 95.9 | num_updates 1314 | lr 0.000261712 | gnorm 0.921 | train_wall 18 | gb_free 14.2 | wall 123\n","2024-10-24 08:40:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:40:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 007:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:40:54 | INFO | fairseq.trainer | begin training epoch 7\n","2024-10-24 08:40:54 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:41:14 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n","2024-10-24 08:41:14 | INFO | train | epoch 007 | loss 6.941 | nll_loss 6.064 | ppl 66.88 | wps 34508.1 | ups 10.8 | wpb 3195.3 | bsz 95.9 | num_updates 1533 | lr 0.000242298 | gnorm 0.894 | train_wall 18 | gb_free 14.1 | wall 143\n","2024-10-24 08:41:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:41:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 008:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:41:14 | INFO | fairseq.trainer | begin training epoch 8\n","2024-10-24 08:41:14 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:41:34 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n","2024-10-24 08:41:34 | INFO | train | epoch 008 | loss 6.753 | nll_loss 5.844 | ppl 57.44 | wps 35345.8 | ups 11.06 | wpb 3195.3 | bsz 95.9 | num_updates 1752 | lr 0.000226649 | gnorm 0.9 | train_wall 18 | gb_free 14.2 | wall 163\n","2024-10-24 08:41:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:41:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 009:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:41:34 | INFO | fairseq.trainer | begin training epoch 9\n","2024-10-24 08:41:34 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:41:52 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n","2024-10-24 08:41:52 | INFO | train | epoch 009 | loss 6.599 | nll_loss 5.665 | ppl 50.74 | wps 37161.4 | ups 11.63 | wpb 3195.3 | bsz 95.9 | num_updates 1971 | lr 0.000213687 | gnorm 0.896 | train_wall 17 | gb_free 14.1 | wall 182\n","2024-10-24 08:41:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:41:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 010:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:41:53 | INFO | fairseq.trainer | begin training epoch 10\n","2024-10-24 08:41:53 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 010:  13% 28/219 [00:02\u003c00:15, 12.30it/s]2024-10-24 08:41:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 08:41:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 010 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   1% 1/86 [00:00\u003c01:16,  1.12it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   2% 2/86 [00:01\u003c01:15,  1.11it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   3% 3/86 [00:02\u003c01:15,  1.10it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   5% 4/86 [00:03\u003c01:15,  1.08it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   6% 5/86 [00:04\u003c01:13,  1.11it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   7% 6/86 [00:05\u003c01:12,  1.11it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   8% 7/86 [00:06\u003c01:08,  1.16it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   9% 8/86 [00:07\u003c01:16,  1.02it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  10% 9/86 [00:08\u003c01:21,  1.06s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  12% 10/86 [00:09\u003c01:26,  1.13s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  13% 11/86 [00:10\u003c01:19,  1.07s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  14% 12/86 [00:11\u003c01:10,  1.04it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  15% 13/86 [00:12\u003c01:00,  1.21it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  16% 14/86 [00:13\u003c01:03,  1.14it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  17% 15/86 [00:13\u003c00:59,  1.19it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  19% 16/86 [00:14\u003c01:03,  1.10it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  20% 17/86 [00:15\u003c00:56,  1.23it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  21% 18/86 [00:16\u003c00:57,  1.17it/s]\u001b[A\n","epoch 010:  13% 28/219 [00:19\u003c00:15, 12.30it/s, loss=6.59, nll_loss=5.653, ppl=50.31, wps=38306.9, ups=11.84, wpb=3234.9, bsz=90.8, num_updates=2000, lr=0.000212132, gnorm=0.999, train_wall=8, gb_free=14.2, wall=184]2024-10-24 08:42:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:42:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:42:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 010 | valid on 'valid' subset:  23% 20/86 [00:18\u003c00:54,  1.21it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  24% 21/86 [00:18\u003c00:54,  1.20it/s]\u001b[A2024-10-24 08:42:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:42:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:42:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 010 | valid on 'valid' subset:  26% 22/86 [00:19\u003c00:56,  1.14it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  27% 23/86 [00:21\u003c01:01,  1.03it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  28% 24/86 [00:22\u003c01:03,  1.02s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  29% 25/86 [00:23\u003c01:10,  1.15s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  30% 26/86 [00:24\u003c01:05,  1.09s/it]\u001b[A2024-10-24 08:42:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:42:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:42:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 010 | valid on 'valid' subset:  31% 27/86 [00:26\u003c01:11,  1.22s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  33% 28/86 [00:27\u003c01:08,  1.17s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  34% 29/86 [00:28\u003c01:02,  1.10s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  35% 30/86 [00:29\u003c01:01,  1.09s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  36% 31/86 [00:30\u003c00:57,  1.05s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  37% 32/86 [00:31\u003c00:54,  1.01s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  38% 33/86 [00:32\u003c00:54,  1.02s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  40% 34/86 [00:32\u003c00:51,  1.02it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  41% 35/86 [00:33\u003c00:50,  1.01it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  42% 36/86 [00:34\u003c00:48,  1.04it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  43% 37/86 [00:35\u003c00:47,  1.03it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  44% 38/86 [00:37\u003c00:49,  1.02s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  45% 39/86 [00:38\u003c00:50,  1.07s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  47% 40/86 [00:39\u003c00:52,  1.14s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  48% 41/86 [00:40\u003c00:52,  1.17s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  49% 42/86 [00:42\u003c00:55,  1.26s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  50% 43/86 [00:43\u003c00:51,  1.21s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  51% 44/86 [00:44\u003c00:49,  1.17s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  52% 45/86 [00:45\u003c00:45,  1.10s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  53% 46/86 [00:46\u003c00:43,  1.08s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  55% 47/86 [00:47\u003c00:39,  1.02s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  56% 48/86 [00:48\u003c00:38,  1.02s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  57% 49/86 [00:49\u003c00:36,  1.02it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  58% 50/86 [00:50\u003c00:36,  1.00s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  59% 51/86 [00:51\u003c00:34,  1.01it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  60% 52/86 [00:52\u003c00:35,  1.03s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  62% 53/86 [00:53\u003c00:35,  1.07s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  63% 54/86 [00:54\u003c00:37,  1.18s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  64% 55/86 [00:56\u003c00:38,  1.23s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  65% 56/86 [00:57\u003c00:39,  1.31s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  66% 57/86 [00:59\u003c00:38,  1.33s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  67% 58/86 [01:00\u003c00:35,  1.25s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  69% 59/86 [01:01\u003c00:32,  1.20s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  70% 60/86 [01:02\u003c00:29,  1.15s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  71% 61/86 [01:03\u003c00:27,  1.09s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  72% 62/86 [01:04\u003c00:25,  1.07s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  73% 63/86 [01:05\u003c00:23,  1.03s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  74% 64/86 [01:06\u003c00:22,  1.02s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  76% 65/86 [01:07\u003c00:21,  1.01s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  77% 66/86 [01:08\u003c00:19,  1.01it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  78% 67/86 [01:09\u003c00:18,  1.03it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  79% 68/86 [01:10\u003c00:19,  1.07s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  80% 69/86 [01:11\u003c00:20,  1.18s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  81% 70/86 [01:13\u003c00:20,  1.29s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  83% 71/86 [01:14\u003c00:19,  1.33s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  84% 72/86 [01:16\u003c00:18,  1.32s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  85% 73/86 [01:17\u003c00:15,  1.22s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  86% 74/86 [01:18\u003c00:13,  1.16s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  87% 75/86 [01:19\u003c00:12,  1.10s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  88% 76/86 [01:19\u003c00:10,  1.03s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  90% 77/86 [01:20\u003c00:09,  1.00s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  91% 78/86 [01:21\u003c00:08,  1.02s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  92% 79/86 [01:22\u003c00:06,  1.00it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  93% 80/86 [01:23\u003c00:05,  1.02it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  94% 81/86 [01:24\u003c00:04,  1.04it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  95% 82/86 [01:25\u003c00:03,  1.07it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  97% 83/86 [01:26\u003c00:02,  1.01it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  98% 84/86 [01:27\u003c00:02,  1.05s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  99% 85/86 [01:29\u003c00:01,  1.12s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset: 100% 86/86 [01:30\u003c00:00,  1.12s/it]\u001b[A\n","                                                                        \u001b[A2024-10-24 08:43:25 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.323 | nll_loss 5.277 | ppl 38.78 | bleu 3.32 | wps 2285.1 | wpb 2386.9 | bsz 73.7 | num_updates 2000\n","2024-10-24 08:43:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 2000 updates\n","2024-10-24 08:43:25 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_10_2000.pt\n","2024-10-24 08:43:26 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_10_2000.pt\n","2024-10-24 08:43:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulm/checkpoint_10_2000.pt (epoch 10 @ 2000 updates, score 3.32) (writing took 1.674112683000203 seconds)\n","2024-10-24 08:43:45 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n","2024-10-24 08:43:45 | INFO | train | epoch 010 | loss 6.476 | nll_loss 5.521 | ppl 45.9 | wps 6226.4 | ups 1.95 | wpb 3195.3 | bsz 95.9 | num_updates 2190 | lr 0.000202721 | gnorm 0.926 | train_wall 19 | gb_free 14.1 | wall 294\n","2024-10-24 08:43:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:43:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 011:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:43:45 | INFO | fairseq.trainer | begin training epoch 11\n","2024-10-24 08:43:45 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:44:03 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n","2024-10-24 08:44:03 | INFO | train | epoch 011 | loss 6.36 | nll_loss 5.384 | ppl 41.76 | wps 37898.4 | ups 11.86 | wpb 3195.3 | bsz 95.9 | num_updates 2409 | lr 0.000193287 | gnorm 0.921 | train_wall 17 | gb_free 14.1 | wall 312\n","2024-10-24 08:44:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:44:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 012:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:44:03 | INFO | fairseq.trainer | begin training epoch 12\n","2024-10-24 08:44:03 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:44:22 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n","2024-10-24 08:44:22 | INFO | train | epoch 012 | loss 6.263 | nll_loss 5.27 | ppl 38.59 | wps 37867.3 | ups 11.85 | wpb 3195.3 | bsz 95.9 | num_updates 2628 | lr 0.000185058 | gnorm 0.938 | train_wall 17 | gb_free 14.1 | wall 331\n","2024-10-24 08:44:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:44:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 013:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:44:22 | INFO | fairseq.trainer | begin training epoch 13\n","2024-10-24 08:44:22 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:44:41 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n","2024-10-24 08:44:41 | INFO | train | epoch 013 | loss 6.168 | nll_loss 5.158 | ppl 35.7 | wps 37124 | ups 11.62 | wpb 3195.3 | bsz 95.9 | num_updates 2847 | lr 0.000177798 | gnorm 0.942 | train_wall 17 | gb_free 14.1 | wall 350\n","2024-10-24 08:44:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:44:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 014:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:44:41 | INFO | fairseq.trainer | begin training epoch 14\n","2024-10-24 08:44:41 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:44:59 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n","2024-10-24 08:44:59 | INFO | train | epoch 014 | loss 6.081 | nll_loss 5.056 | ppl 33.27 | wps 37307.8 | ups 11.68 | wpb 3195.3 | bsz 95.9 | num_updates 3066 | lr 0.000171331 | gnorm 0.947 | train_wall 17 | gb_free 14.1 | wall 369\n","2024-10-24 08:44:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:44:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 015:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:45:00 | INFO | fairseq.trainer | begin training epoch 15\n","2024-10-24 08:45:00 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:45:19 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n","2024-10-24 08:45:19 | INFO | train | epoch 015 | loss 6.004 | nll_loss 4.965 | ppl 31.23 | wps 36669.1 | ups 11.48 | wpb 3195.3 | bsz 95.9 | num_updates 3285 | lr 0.000165521 | gnorm 0.976 | train_wall 17 | gb_free 14 | wall 388\n","2024-10-24 08:45:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:45:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 016:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:45:19 | INFO | fairseq.trainer | begin training epoch 16\n","2024-10-24 08:45:19 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:45:37 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n","2024-10-24 08:45:37 | INFO | train | epoch 016 | loss 5.938 | nll_loss 4.887 | ppl 29.59 | wps 37383.6 | ups 11.7 | wpb 3195.3 | bsz 95.9 | num_updates 3504 | lr 0.000160265 | gnorm 0.99 | train_wall 17 | gb_free 14.1 | wall 406\n","2024-10-24 08:45:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:45:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 017:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:45:37 | INFO | fairseq.trainer | begin training epoch 17\n","2024-10-24 08:45:37 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:45:56 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n","2024-10-24 08:45:56 | INFO | train | epoch 017 | loss 5.866 | nll_loss 4.803 | ppl 27.92 | wps 37933.1 | ups 11.87 | wpb 3195.3 | bsz 95.9 | num_updates 3723 | lr 0.00015548 | gnorm 0.996 | train_wall 17 | gb_free 14.1 | wall 425\n","2024-10-24 08:45:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:45:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 018:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:45:56 | INFO | fairseq.trainer | begin training epoch 18\n","2024-10-24 08:45:56 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:46:16 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n","2024-10-24 08:46:16 | INFO | train | epoch 018 | loss 5.808 | nll_loss 4.735 | ppl 26.63 | wps 33851.9 | ups 10.59 | wpb 3195.3 | bsz 95.9 | num_updates 3942 | lr 0.000151099 | gnorm 0.998 | train_wall 19 | gb_free 14.1 | wall 445\n","2024-10-24 08:46:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:46:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 019:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:46:16 | INFO | fairseq.trainer | begin training epoch 19\n","2024-10-24 08:46:16 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 019:  26% 57/219 [00:04\u003c00:12, 13.18it/s]2024-10-24 08:46:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 08:46:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 019 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:   1% 1/86 [00:00\u003c00:48,  1.74it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:   2% 2/86 [00:01\u003c01:21,  1.03it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:   3% 3/86 [00:03\u003c01:36,  1.17s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:   5% 4/86 [00:04\u003c01:26,  1.05s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:   6% 5/86 [00:05\u003c01:32,  1.14s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:   7% 6/86 [00:06\u003c01:35,  1.19s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:   8% 7/86 [00:07\u003c01:26,  1.09s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:   9% 8/86 [00:08\u003c01:19,  1.01s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  10% 9/86 [00:09\u003c01:14,  1.04it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  12% 10/86 [00:10\u003c01:12,  1.05it/s]\u001b[A\n","epoch 019:  26% 57/219 [00:16\u003c00:12, 13.18it/s, loss=5.776, nll_loss=4.697, ppl=25.94, wps=40814.9, ups=12.52, wpb=3259.5, bsz=99.4, num_updates=4000, lr=0.00015, gnorm=0.991, train_wall=7, gb_free=14.1, wall=450]\n","epoch 019 | valid on 'valid' subset:  14% 12/86 [00:12\u003c01:09,  1.07it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  15% 13/86 [00:12\u003c01:06,  1.10it/s]\u001b[A2024-10-24 08:46:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:46:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:46:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 019 | valid on 'valid' subset:  16% 14/86 [00:13\u003c01:08,  1.06it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  17% 15/86 [00:14\u003c01:04,  1.10it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  19% 16/86 [00:15\u003c01:05,  1.07it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  20% 17/86 [00:16\u003c01:02,  1.10it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  21% 18/86 [00:17\u003c01:08,  1.01s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  22% 19/86 [00:18\u003c01:07,  1.01s/it]\u001b[A2024-10-24 08:46:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:46:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:46:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 019 | valid on 'valid' subset:  23% 20/86 [00:20\u003c01:16,  1.16s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  24% 21/86 [00:21\u003c01:16,  1.18s/it]\u001b[A2024-10-24 08:46:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:46:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:46:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 019 | valid on 'valid' subset:  26% 22/86 [00:23\u003c01:21,  1.28s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  27% 23/86 [00:23\u003c01:13,  1.16s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  28% 24/86 [00:24\u003c01:04,  1.03s/it]\u001b[A2024-10-24 08:46:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:46:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:46:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 019 | valid on 'valid' subset:  29% 25/86 [00:25\u003c01:01,  1.01s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  30% 26/86 [00:26\u003c00:55,  1.09it/s]\u001b[A2024-10-24 08:46:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:46:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:46:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 019 | valid on 'valid' subset:  31% 27/86 [00:27\u003c00:55,  1.06it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  33% 28/86 [00:28\u003c00:54,  1.07it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  34% 29/86 [00:29\u003c00:52,  1.09it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  35% 30/86 [00:30\u003c00:52,  1.06it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  36% 31/86 [00:31\u003c00:50,  1.09it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  37% 32/86 [00:31\u003c00:46,  1.16it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  38% 33/86 [00:32\u003c00:47,  1.12it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  40% 34/86 [00:33\u003c00:48,  1.07it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  41% 35/86 [00:35\u003c00:54,  1.06s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  42% 36/86 [00:36\u003c00:55,  1.11s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  43% 37/86 [00:37\u003c00:59,  1.22s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  44% 38/86 [00:39\u003c00:59,  1.25s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  45% 39/86 [00:39\u003c00:52,  1.12s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  47% 40/86 [00:40\u003c00:49,  1.08s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  48% 41/86 [00:41\u003c00:41,  1.09it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  49% 42/86 [00:42\u003c00:41,  1.05it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  50% 43/86 [00:43\u003c00:40,  1.07it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  51% 44/86 [00:44\u003c00:40,  1.05it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  52% 45/86 [00:45\u003c00:38,  1.05it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  53% 46/86 [00:46\u003c00:38,  1.04it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  55% 47/86 [00:47\u003c00:36,  1.06it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  56% 48/86 [00:48\u003c00:36,  1.04it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  57% 49/86 [00:49\u003c00:36,  1.02it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  58% 50/86 [00:50\u003c00:38,  1.08s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  59% 51/86 [00:51\u003c00:39,  1.13s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  60% 52/86 [00:53\u003c00:41,  1.21s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  62% 53/86 [00:54\u003c00:40,  1.22s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  63% 54/86 [00:55\u003c00:38,  1.21s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  64% 55/86 [00:56\u003c00:34,  1.10s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  65% 56/86 [00:57\u003c00:31,  1.05s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  66% 57/86 [00:58\u003c00:28,  1.02it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  67% 58/86 [00:59\u003c00:27,  1.02it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  69% 59/86 [01:00\u003c00:25,  1.04it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  70% 60/86 [01:01\u003c00:25,  1.03it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  71% 61/86 [01:02\u003c00:23,  1.06it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  72% 62/86 [01:02\u003c00:22,  1.07it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  73% 63/86 [01:03\u003c00:21,  1.07it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  74% 64/86 [01:04\u003c00:20,  1.05it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  76% 65/86 [01:06\u003c00:21,  1.04s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  77% 66/86 [01:07\u003c00:21,  1.09s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  78% 67/86 [01:08\u003c00:22,  1.16s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  79% 68/86 [01:09\u003c00:21,  1.18s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  80% 69/86 [01:11\u003c00:20,  1.21s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  81% 70/86 [01:12\u003c00:18,  1.17s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  83% 71/86 [01:13\u003c00:16,  1.09s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  84% 72/86 [01:14\u003c00:14,  1.02s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  85% 73/86 [01:14\u003c00:12,  1.02it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  86% 74/86 [01:15\u003c00:11,  1.03it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  87% 75/86 [01:16\u003c00:10,  1.05it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  88% 76/86 [01:17\u003c00:09,  1.08it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  90% 77/86 [01:18\u003c00:08,  1.09it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  91% 78/86 [01:19\u003c00:07,  1.09it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  92% 79/86 [01:20\u003c00:06,  1.09it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  93% 80/86 [01:21\u003c00:05,  1.09it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  94% 81/86 [01:22\u003c00:04,  1.01it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  95% 82/86 [01:23\u003c00:04,  1.05s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  97% 83/86 [01:24\u003c00:03,  1.10s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  98% 84/86 [01:26\u003c00:02,  1.19s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  99% 85/86 [01:27\u003c00:01,  1.21s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset: 100% 86/86 [01:28\u003c00:00,  1.17s/it]\u001b[A\n","                                                                        \u001b[A2024-10-24 08:47:50 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 5.654 | nll_loss 4.453 | ppl 21.9 | bleu 7.84 | wps 2321.1 | wpb 2386.9 | bsz 73.7 | num_updates 4000 | best_bleu 7.84\n","2024-10-24 08:47:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 4000 updates\n","2024-10-24 08:47:50 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_19_4000.pt\n","2024-10-24 08:47:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_19_4000.pt\n","2024-10-24 08:47:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulm/checkpoint_19_4000.pt (epoch 19 @ 4000 updates, score 7.84) (writing took 1.2965686509996885 seconds)\n","2024-10-24 08:48:06 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n","2024-10-24 08:48:06 | INFO | train | epoch 019 | loss 5.754 | nll_loss 4.671 | ppl 25.47 | wps 6387.5 | ups 2 | wpb 3195.3 | bsz 95.9 | num_updates 4161 | lr 0.000147069 | gnorm 1.028 | train_wall 18 | gb_free 14.2 | wall 555\n","2024-10-24 08:48:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:48:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 020:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:48:06 | INFO | fairseq.trainer | begin training epoch 20\n","2024-10-24 08:48:06 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:48:25 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n","2024-10-24 08:48:25 | INFO | train | epoch 020 | loss 5.7 | nll_loss 4.606 | ppl 24.36 | wps 37632.8 | ups 11.78 | wpb 3195.3 | bsz 95.9 | num_updates 4380 | lr 0.000143346 | gnorm 1.022 | train_wall 17 | gb_free 14.2 | wall 574\n","2024-10-24 08:48:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:48:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 021:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:48:25 | INFO | fairseq.trainer | begin training epoch 21\n","2024-10-24 08:48:25 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:48:43 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n","2024-10-24 08:48:43 | INFO | train | epoch 021 | loss 5.655 | nll_loss 4.554 | ppl 23.49 | wps 38176.3 | ups 11.95 | wpb 3195.3 | bsz 95.9 | num_updates 4599 | lr 0.000139891 | gnorm 1.042 | train_wall 17 | gb_free 14.1 | wall 592\n","2024-10-24 08:48:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:48:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 022:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:48:43 | INFO | fairseq.trainer | begin training epoch 22\n","2024-10-24 08:48:43 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:49:01 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n","2024-10-24 08:49:01 | INFO | train | epoch 022 | loss 5.609 | nll_loss 4.499 | ppl 22.61 | wps 38530.8 | ups 12.06 | wpb 3195.3 | bsz 95.9 | num_updates 4818 | lr 0.000136675 | gnorm 1.036 | train_wall 17 | gb_free 14.1 | wall 610\n","2024-10-24 08:49:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:49:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 023:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:49:01 | INFO | fairseq.trainer | begin training epoch 23\n","2024-10-24 08:49:01 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:49:20 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n","2024-10-24 08:49:20 | INFO | train | epoch 023 | loss 5.566 | nll_loss 4.448 | ppl 21.82 | wps 37457.1 | ups 11.72 | wpb 3195.3 | bsz 95.9 | num_updates 5037 | lr 0.00013367 | gnorm 1.068 | train_wall 17 | gb_free 14.1 | wall 629\n","2024-10-24 08:49:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:49:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 024:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:49:20 | INFO | fairseq.trainer | begin training epoch 24\n","2024-10-24 08:49:20 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:49:39 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n","2024-10-24 08:49:39 | INFO | train | epoch 024 | loss 5.525 | nll_loss 4.401 | ppl 21.12 | wps 36768.2 | ups 11.51 | wpb 3195.3 | bsz 95.9 | num_updates 5256 | lr 0.000130856 | gnorm 1.056 | train_wall 17 | gb_free 14 | wall 648\n","2024-10-24 08:49:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:49:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 025:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:49:39 | INFO | fairseq.trainer | begin training epoch 25\n","2024-10-24 08:49:39 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 025:  99% 217/219 [00:18\u003c00:00, 11.85it/s, loss=5.501, nll_loss=4.372, ppl=20.7, wps=41049.9, ups=12.99, wpb=3160.2, bsz=94.8, num_updates=5400, lr=0.000129099, gnorm=1.091, train_wall=7, gb_free=14.1, wall=660]2024-10-24 08:49:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 08:49:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 025 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:   1% 1/86 [00:00\u003c00:49,  1.73it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:   2% 2/86 [00:00\u003c00:40,  2.06it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:   3% 3/86 [00:01\u003c00:43,  1.93it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:   5% 4/86 [00:01\u003c00:37,  2.18it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:   6% 5/86 [00:02\u003c00:36,  2.25it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:   7% 6/86 [00:03\u003c00:48,  1.66it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:   8% 7/86 [00:03\u003c00:42,  1.85it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:   9% 8/86 [00:04\u003c00:40,  1.91it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  10% 9/86 [00:04\u003c00:39,  1.97it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  12% 10/86 [00:05\u003c00:38,  2.00it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  13% 11/86 [00:05\u003c00:37,  2.01it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  14% 12/86 [00:06\u003c00:38,  1.92it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  15% 13/86 [00:07\u003c00:44,  1.63it/s]\u001b[A2024-10-24 08:50:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:50:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:50:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 025 | valid on 'valid' subset:  16% 14/86 [00:07\u003c00:42,  1.70it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  17% 15/86 [00:07\u003c00:37,  1.90it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  19% 16/86 [00:08\u003c00:38,  1.82it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  20% 17/86 [00:08\u003c00:35,  1.94it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  21% 18/86 [00:09\u003c00:35,  1.93it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  22% 19/86 [00:10\u003c00:36,  1.85it/s]\u001b[A2024-10-24 08:50:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:50:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:50:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 025 | valid on 'valid' subset:  23% 20/86 [00:11\u003c00:46,  1.42it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  24% 21/86 [00:11\u003c00:44,  1.45it/s]\u001b[A2024-10-24 08:50:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:50:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:50:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 025 | valid on 'valid' subset:  26% 22/86 [00:12\u003c00:50,  1.27it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  27% 23/86 [00:13\u003c00:51,  1.23it/s]\u001b[A\n","epoch 025:  99% 217/219 [00:33\u003c00:00, 11.85it/s, loss=5.501, nll_loss=4.372, ppl=20.7, wps=41049.9, ups=12.99, wpb=3160.2, bsz=94.8, num_updates=5400, lr=0.000129099, gnorm=1.091, train_wall=7, gb_free=14.1, wall=660]2024-10-24 08:50:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:50:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:50:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 025 | valid on 'valid' subset:  29% 25/86 [00:15\u003c00:53,  1.15it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  30% 26/86 [00:16\u003c00:48,  1.25it/s]\u001b[A2024-10-24 08:50:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:50:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:50:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 025 | valid on 'valid' subset:  31% 27/86 [00:16\u003c00:43,  1.34it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  33% 28/86 [00:17\u003c00:45,  1.28it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  34% 29/86 [00:18\u003c00:39,  1.46it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  35% 30/86 [00:18\u003c00:36,  1.55it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  36% 31/86 [00:19\u003c00:32,  1.68it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  37% 32/86 [00:19\u003c00:29,  1.85it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  38% 33/86 [00:20\u003c00:30,  1.75it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  40% 34/86 [00:20\u003c00:30,  1.70it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  41% 35/86 [00:21\u003c00:29,  1.72it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  42% 36/86 [00:21\u003c00:28,  1.77it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  43% 37/86 [00:22\u003c00:30,  1.60it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  44% 38/86 [00:23\u003c00:30,  1.59it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  45% 39/86 [00:23\u003c00:28,  1.66it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  47% 40/86 [00:24\u003c00:32,  1.41it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  48% 41/86 [00:25\u003c00:28,  1.59it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  49% 42/86 [00:26\u003c00:33,  1.32it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  50% 43/86 [00:27\u003c00:32,  1.31it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  51% 44/86 [00:28\u003c00:34,  1.22it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  52% 45/86 [00:28\u003c00:32,  1.25it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  53% 46/86 [00:29\u003c00:33,  1.20it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  55% 47/86 [00:31\u003c00:37,  1.03it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  56% 48/86 [00:32\u003c00:38,  1.01s/it]\u001b[A\n","epoch 025 | valid on 'valid' subset:  57% 49/86 [00:32\u003c00:31,  1.17it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  58% 50/86 [00:33\u003c00:29,  1.23it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  59% 51/86 [00:33\u003c00:27,  1.29it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  60% 52/86 [00:34\u003c00:25,  1.35it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  62% 53/86 [00:35\u003c00:24,  1.36it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  63% 54/86 [00:36\u003c00:23,  1.38it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  64% 55/86 [00:36\u003c00:23,  1.31it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  65% 56/86 [00:37\u003c00:24,  1.25it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  66% 57/86 [00:38\u003c00:20,  1.41it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  67% 58/86 [00:39\u003c00:19,  1.42it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  69% 59/86 [00:39\u003c00:18,  1.46it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  70% 60/86 [00:40\u003c00:20,  1.29it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  71% 61/86 [00:41\u003c00:19,  1.26it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  72% 62/86 [00:42\u003c00:21,  1.10it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  73% 63/86 [00:43\u003c00:20,  1.13it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  74% 64/86 [00:44\u003c00:20,  1.06it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  76% 65/86 [00:45\u003c00:22,  1.05s/it]\u001b[A\n","epoch 025 | valid on 'valid' subset:  77% 66/86 [00:47\u003c00:21,  1.10s/it]\u001b[A\n","epoch 025 | valid on 'valid' subset:  78% 67/86 [00:47\u003c00:19,  1.00s/it]\u001b[A\n","epoch 025 | valid on 'valid' subset:  79% 68/86 [00:48\u003c00:16,  1.10it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  80% 69/86 [00:49\u003c00:13,  1.23it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  81% 70/86 [00:50\u003c00:13,  1.20it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  83% 71/86 [00:50\u003c00:12,  1.20it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  84% 72/86 [00:51\u003c00:11,  1.25it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  85% 73/86 [00:52\u003c00:10,  1.25it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  86% 74/86 [00:53\u003c00:09,  1.31it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  87% 75/86 [00:53\u003c00:08,  1.27it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  88% 76/86 [00:54\u003c00:07,  1.32it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  90% 77/86 [00:55\u003c00:07,  1.26it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  91% 78/86 [00:56\u003c00:06,  1.20it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  92% 79/86 [00:57\u003c00:05,  1.20it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  93% 80/86 [00:58\u003c00:05,  1.06it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  94% 81/86 [00:59\u003c00:05,  1.02s/it]\u001b[A\n","epoch 025 | valid on 'valid' subset:  95% 82/86 [01:00\u003c00:04,  1.07s/it]\u001b[A\n","epoch 025 | valid on 'valid' subset:  97% 83/86 [01:02\u003c00:03,  1.13s/it]\u001b[A\n","epoch 025 | valid on 'valid' subset:  98% 84/86 [01:03\u003c00:02,  1.11s/it]\u001b[A\n","epoch 025 | valid on 'valid' subset:  99% 85/86 [01:04\u003c00:01,  1.07s/it]\u001b[A\n","epoch 025 | valid on 'valid' subset: 100% 86/86 [01:04\u003c00:00,  1.02it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 08:51:02 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 5.404 | nll_loss 4.159 | ppl 17.87 | bleu 11.55 | wps 3176.5 | wpb 2386.9 | bsz 73.7 | num_updates 5475 | best_bleu 11.55\n","2024-10-24 08:51:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 5475 updates\n","2024-10-24 08:51:03 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_best.pt\n","2024-10-24 08:51:03 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_best.pt\n","2024-10-24 08:51:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulm/checkpoint_best.pt (epoch 25 @ 5475 updates, score 11.55) (writing took 1.1817645820001417 seconds)\n","2024-10-24 08:51:04 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n","2024-10-24 08:51:04 | INFO | train | epoch 025 | loss 5.49 | nll_loss 4.358 | ppl 20.51 | wps 8239.2 | ups 2.58 | wpb 3195.3 | bsz 95.9 | num_updates 5475 | lr 0.000128212 | gnorm 1.08 | train_wall 17 | gb_free 14.1 | wall 733\n","2024-10-24 08:51:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:51:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 026:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:51:04 | INFO | fairseq.trainer | begin training epoch 26\n","2024-10-24 08:51:04 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:51:25 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n","2024-10-24 08:51:25 | INFO | train | epoch 026 | loss 5.445 | nll_loss 4.306 | ppl 19.79 | wps 32200 | ups 10.08 | wpb 3195.3 | bsz 95.9 | num_updates 5694 | lr 0.000125722 | gnorm 1.068 | train_wall 19 | gb_free 14.1 | wall 755\n","2024-10-24 08:51:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:51:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 027:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:51:26 | INFO | fairseq.trainer | begin training epoch 27\n","2024-10-24 08:51:26 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:51:44 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n","2024-10-24 08:51:44 | INFO | train | epoch 027 | loss 5.417 | nll_loss 4.273 | ppl 19.33 | wps 38486 | ups 12.04 | wpb 3195.3 | bsz 95.9 | num_updates 5913 | lr 0.000123372 | gnorm 1.082 | train_wall 17 | gb_free 14.2 | wall 773\n","2024-10-24 08:51:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:51:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 028:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:51:44 | INFO | fairseq.trainer | begin training epoch 28\n","2024-10-24 08:51:44 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 028:  39% 85/219 [00:07\u003c00:10, 12.94it/s]2024-10-24 08:51:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 08:51:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 028 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:   1% 1/86 [00:00\u003c00:48,  1.77it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:   2% 2/86 [00:00\u003c00:40,  2.06it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:   3% 3/86 [00:01\u003c00:40,  2.06it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:   5% 4/86 [00:01\u003c00:35,  2.31it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:   6% 5/86 [00:02\u003c00:33,  2.39it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:   7% 6/86 [00:02\u003c00:32,  2.43it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:   8% 7/86 [00:03\u003c00:31,  2.49it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:   9% 8/86 [00:03\u003c00:31,  2.46it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  10% 9/86 [00:03\u003c00:33,  2.33it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  12% 10/86 [00:04\u003c00:32,  2.32it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  13% 11/86 [00:05\u003c00:40,  1.87it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  14% 12/86 [00:05\u003c00:38,  1.93it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  15% 13/86 [00:06\u003c00:43,  1.66it/s]\u001b[A2024-10-24 08:51:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:51:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:51:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 028 | valid on 'valid' subset:  16% 14/86 [00:06\u003c00:42,  1.69it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  17% 15/86 [00:07\u003c00:38,  1.86it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  19% 16/86 [00:08\u003c00:40,  1.72it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  20% 17/86 [00:09\u003c00:49,  1.39it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  21% 18/86 [00:09\u003c00:50,  1.36it/s]\u001b[A\n","epoch 028:  39% 85/219 [00:18\u003c00:10, 12.94it/s, loss=5.377, nll_loss=4.226, ppl=18.71, wps=36169.2, ups=11.1, wpb=3258, bsz=97.6, num_updates=6000, lr=0.000122474, gnorm=1.122, train_wall=8, gb_free=14.1, wall=781]2024-10-24 08:52:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:52:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:52:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 028 | valid on 'valid' subset:  23% 20/86 [00:11\u003c00:58,  1.14it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  24% 21/86 [00:12\u003c00:54,  1.20it/s]\u001b[A2024-10-24 08:52:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:52:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:52:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 028 | valid on 'valid' subset:  26% 22/86 [00:13\u003c00:54,  1.18it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  27% 23/86 [00:14\u003c00:50,  1.24it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  28% 24/86 [00:14\u003c00:42,  1.45it/s]\u001b[A2024-10-24 08:52:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:52:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:52:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 028 | valid on 'valid' subset:  29% 25/86 [00:15\u003c00:40,  1.49it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  30% 26/86 [00:15\u003c00:38,  1.57it/s]\u001b[A2024-10-24 08:52:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:52:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:52:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 028 | valid on 'valid' subset:  31% 27/86 [00:16\u003c00:38,  1.54it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  33% 28/86 [00:17\u003c00:39,  1.46it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  34% 29/86 [00:17\u003c00:35,  1.62it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  35% 30/86 [00:18\u003c00:38,  1.47it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  36% 31/86 [00:19\u003c00:39,  1.41it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  37% 32/86 [00:19\u003c00:33,  1.62it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  38% 33/86 [00:20\u003c00:33,  1.60it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  40% 34/86 [00:20\u003c00:32,  1.61it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  41% 35/86 [00:21\u003c00:31,  1.62it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  42% 36/86 [00:22\u003c00:30,  1.66it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  43% 37/86 [00:22\u003c00:30,  1.60it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  44% 38/86 [00:23\u003c00:28,  1.66it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  45% 39/86 [00:23\u003c00:27,  1.70it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  47% 40/86 [00:25\u003c00:36,  1.24it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  48% 41/86 [00:25\u003c00:33,  1.33it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  49% 42/86 [00:27\u003c00:40,  1.09it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  50% 43/86 [00:27\u003c00:37,  1.14it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  51% 44/86 [00:28\u003c00:38,  1.10it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  52% 45/86 [00:29\u003c00:36,  1.13it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  53% 46/86 [00:30\u003c00:32,  1.21it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  55% 47/86 [00:30\u003c00:29,  1.33it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  56% 48/86 [00:31\u003c00:28,  1.33it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  57% 49/86 [00:32\u003c00:26,  1.40it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  58% 50/86 [00:32\u003c00:25,  1.43it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  59% 51/86 [00:33\u003c00:23,  1.49it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  60% 52/86 [00:34\u003c00:22,  1.51it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  62% 53/86 [00:35\u003c00:23,  1.40it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  63% 54/86 [00:35\u003c00:22,  1.45it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  64% 55/86 [00:36\u003c00:20,  1.50it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  65% 56/86 [00:36\u003c00:19,  1.55it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  66% 57/86 [00:37\u003c00:17,  1.65it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  67% 58/86 [00:38\u003c00:17,  1.64it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  69% 59/86 [00:38\u003c00:17,  1.53it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  70% 60/86 [00:39\u003c00:19,  1.35it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  71% 61/86 [00:40\u003c00:21,  1.17it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  72% 62/86 [00:41\u003c00:20,  1.16it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  73% 63/86 [00:42\u003c00:19,  1.18it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  74% 64/86 [00:43\u003c00:21,  1.02it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  76% 65/86 [00:45\u003c00:23,  1.11s/it]\u001b[A\n","epoch 028 | valid on 'valid' subset:  77% 66/86 [00:46\u003c00:20,  1.03s/it]\u001b[A\n","epoch 028 | valid on 'valid' subset:  78% 67/86 [00:46\u003c00:17,  1.10it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  79% 68/86 [00:47\u003c00:15,  1.19it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  80% 69/86 [00:48\u003c00:13,  1.26it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  81% 70/86 [00:48\u003c00:12,  1.32it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  83% 71/86 [00:49\u003c00:11,  1.27it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  84% 72/86 [00:50\u003c00:11,  1.23it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  85% 73/86 [00:51\u003c00:10,  1.28it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  86% 74/86 [00:51\u003c00:09,  1.33it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  87% 75/86 [00:52\u003c00:08,  1.28it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  88% 76/86 [00:53\u003c00:07,  1.36it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  90% 77/86 [00:54\u003c00:06,  1.30it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  91% 78/86 [00:55\u003c00:06,  1.25it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  92% 79/86 [00:56\u003c00:06,  1.15it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  93% 80/86 [00:57\u003c00:05,  1.04it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  94% 81/86 [00:58\u003c00:05,  1.03s/it]\u001b[A\n","epoch 028 | valid on 'valid' subset:  95% 82/86 [00:59\u003c00:04,  1.13s/it]\u001b[A\n","epoch 028 | valid on 'valid' subset:  97% 83/86 [01:01\u003c00:03,  1.17s/it]\u001b[A\n","epoch 028 | valid on 'valid' subset:  98% 84/86 [01:02\u003c00:02,  1.14s/it]\u001b[A\n","epoch 028 | valid on 'valid' subset:  99% 85/86 [01:03\u003c00:01,  1.08s/it]\u001b[A\n","epoch 028 | valid on 'valid' subset: 100% 86/86 [01:03\u003c00:00,  1.00s/it]\u001b[A\n","                                                                        \u001b[A2024-10-24 08:52:56 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 5.361 | nll_loss 4.096 | ppl 17.1 | bleu 12.12 | wps 3222.7 | wpb 2386.9 | bsz 73.7 | num_updates 6000 | best_bleu 12.12\n","2024-10-24 08:52:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 6000 updates\n","2024-10-24 08:52:56 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_28_6000.pt\n","2024-10-24 08:52:56 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_28_6000.pt\n","2024-10-24 08:52:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulm/checkpoint_28_6000.pt (epoch 28 @ 6000 updates, score 12.12) (writing took 1.2190104040000733 seconds)\n","2024-10-24 08:53:10 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)\n","2024-10-24 08:53:10 | INFO | train | epoch 028 | loss 5.387 | nll_loss 4.238 | ppl 18.87 | wps 8135.9 | ups 2.55 | wpb 3195.3 | bsz 95.9 | num_updates 6132 | lr 0.000121149 | gnorm 1.111 | train_wall 19 | gb_free 14.1 | wall 859\n","2024-10-24 08:53:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:53:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 029:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:53:10 | INFO | fairseq.trainer | begin training epoch 29\n","2024-10-24 08:53:10 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:53:28 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)\n","2024-10-24 08:53:28 | INFO | train | epoch 029 | loss 5.356 | nll_loss 4.2 | ppl 18.38 | wps 37175.1 | ups 11.63 | wpb 3195.3 | bsz 95.9 | num_updates 6351 | lr 0.000119042 | gnorm 1.106 | train_wall 17 | gb_free 14.1 | wall 878\n","2024-10-24 08:53:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:53:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 030:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:53:29 | INFO | fairseq.trainer | begin training epoch 30\n","2024-10-24 08:53:29 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:53:47 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)\n","2024-10-24 08:53:47 | INFO | train | epoch 030 | loss 5.322 | nll_loss 4.16 | ppl 17.88 | wps 37985.2 | ups 11.89 | wpb 3195.3 | bsz 95.9 | num_updates 6570 | lr 0.000117041 | gnorm 1.091 | train_wall 17 | gb_free 14.1 | wall 896\n","2024-10-24 08:53:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:53:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 031:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:53:47 | INFO | fairseq.trainer | begin training epoch 31\n","2024-10-24 08:53:47 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:54:05 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)\n","2024-10-24 08:54:05 | INFO | train | epoch 031 | loss 5.293 | nll_loss 4.127 | ppl 17.47 | wps 38253 | ups 11.97 | wpb 3195.3 | bsz 95.9 | num_updates 6789 | lr 0.000115138 | gnorm 1.113 | train_wall 17 | gb_free 14.1 | wall 914\n","2024-10-24 08:54:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:54:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 032:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:54:05 | INFO | fairseq.trainer | begin training epoch 32\n","2024-10-24 08:54:05 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:54:23 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)\n","2024-10-24 08:54:23 | INFO | train | epoch 032 | loss 5.266 | nll_loss 4.095 | ppl 17.08 | wps 38496.7 | ups 12.05 | wpb 3195.3 | bsz 95.9 | num_updates 7008 | lr 0.000113325 | gnorm 1.118 | train_wall 17 | gb_free 14.1 | wall 932\n","2024-10-24 08:54:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:54:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 033:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:54:24 | INFO | fairseq.trainer | begin training epoch 33\n","2024-10-24 08:54:24 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:54:42 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)\n","2024-10-24 08:54:42 | INFO | train | epoch 033 | loss 5.237 | nll_loss 4.06 | ppl 16.68 | wps 36771.8 | ups 11.51 | wpb 3195.3 | bsz 95.9 | num_updates 7227 | lr 0.000111594 | gnorm 1.116 | train_wall 17 | gb_free 14 | wall 952\n","2024-10-24 08:54:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:54:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 034:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:54:43 | INFO | fairseq.trainer | begin training epoch 34\n","2024-10-24 08:54:43 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:55:01 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)\n","2024-10-24 08:55:01 | INFO | train | epoch 034 | loss 5.214 | nll_loss 4.034 | ppl 16.38 | wps 37113.8 | ups 11.61 | wpb 3195.3 | bsz 95.9 | num_updates 7446 | lr 0.000109941 | gnorm 1.133 | train_wall 17 | gb_free 14 | wall 970\n","2024-10-24 08:55:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:55:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 035:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:55:01 | INFO | fairseq.trainer | begin training epoch 35\n","2024-10-24 08:55:01 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:55:20 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)\n","2024-10-24 08:55:20 | INFO | train | epoch 035 | loss 5.188 | nll_loss 4.002 | ppl 16.02 | wps 38292.2 | ups 11.98 | wpb 3195.3 | bsz 95.9 | num_updates 7665 | lr 0.000108359 | gnorm 1.127 | train_wall 17 | gb_free 14.1 | wall 989\n","2024-10-24 08:55:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:55:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 036:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:55:20 | INFO | fairseq.trainer | begin training epoch 36\n","2024-10-24 08:55:20 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:55:38 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)\n","2024-10-24 08:55:38 | INFO | train | epoch 036 | loss 5.166 | nll_loss 3.977 | ppl 15.75 | wps 38656.6 | ups 12.1 | wpb 3195.3 | bsz 95.9 | num_updates 7884 | lr 0.000106843 | gnorm 1.157 | train_wall 17 | gb_free 14.2 | wall 1007\n","2024-10-24 08:55:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:55:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 037:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:55:38 | INFO | fairseq.trainer | begin training epoch 37\n","2024-10-24 08:55:38 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 037:  53% 115/219 [00:10\u003c00:08, 12.90it/s, loss=5.122, nll_loss=3.927, ppl=15.21, wps=39610.1, ups=12.55, wpb=3157, bsz=101.7, num_updates=7900, lr=0.000106735, gnorm=1.164, train_wall=7, gb_free=14.1, wall=1008]2024-10-24 08:55:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 08:55:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 037 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:   1% 1/86 [00:00\u003c00:45,  1.88it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:   2% 2/86 [00:00\u003c00:40,  2.10it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:   3% 3/86 [00:01\u003c00:38,  2.17it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:   5% 4/86 [00:01\u003c00:34,  2.40it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:   6% 5/86 [00:02\u003c00:33,  2.40it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:   7% 6/86 [00:03\u003c00:45,  1.74it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:   8% 7/86 [00:03\u003c00:46,  1.69it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:   9% 8/86 [00:04\u003c00:48,  1.62it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  10% 9/86 [00:04\u003c00:44,  1.75it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  12% 10/86 [00:05\u003c00:41,  1.83it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  13% 11/86 [00:06\u003c00:49,  1.51it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  14% 12/86 [00:06\u003c00:46,  1.59it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  15% 13/86 [00:07\u003c00:42,  1.71it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  16% 14/86 [00:07\u003c00:41,  1.74it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  17% 15/86 [00:08\u003c00:47,  1.50it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  19% 16/86 [00:09\u003c00:51,  1.37it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  20% 17/86 [00:10\u003c00:49,  1.39it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  21% 18/86 [00:11\u003c00:50,  1.34it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  22% 19/86 [00:11\u003c00:48,  1.37it/s]\u001b[A2024-10-24 08:56:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:56:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:56:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 037 | valid on 'valid' subset:  23% 20/86 [00:12\u003c00:56,  1.16it/s]\u001b[A\n","epoch 037:  53% 115/219 [00:24\u003c00:08, 12.90it/s, loss=5.126, nll_loss=3.93, ppl=15.24, wps=37749.3, ups=11.26, wpb=3351.1, bsz=100.9, num_updates=8000, lr=0.000106066, gnorm=1.15, train_wall=8, gb_free=14.1, wall=1017]2024-10-24 08:56:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:56:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:56:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 037 | valid on 'valid' subset:  26% 22/86 [00:14\u003c00:54,  1.17it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  27% 23/86 [00:15\u003c00:50,  1.25it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  28% 24/86 [00:16\u003c00:50,  1.24it/s]\u001b[A2024-10-24 08:56:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:56:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:56:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 037 | valid on 'valid' subset:  29% 25/86 [00:16\u003c00:46,  1.32it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  30% 26/86 [00:17\u003c00:42,  1.43it/s]\u001b[A2024-10-24 08:56:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:56:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:56:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 037 | valid on 'valid' subset:  31% 27/86 [00:18\u003c00:42,  1.40it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  33% 28/86 [00:18\u003c00:40,  1.43it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  34% 29/86 [00:19\u003c00:37,  1.51it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  35% 30/86 [00:20\u003c00:41,  1.36it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  36% 31/86 [00:20\u003c00:36,  1.49it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  37% 32/86 [00:21\u003c00:32,  1.68it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  38% 33/86 [00:21\u003c00:34,  1.54it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  40% 34/86 [00:22\u003c00:36,  1.43it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  41% 35/86 [00:23\u003c00:35,  1.46it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  42% 36/86 [00:24\u003c00:33,  1.47it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  43% 37/86 [00:25\u003c00:37,  1.30it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  44% 38/86 [00:26\u003c00:39,  1.22it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  45% 39/86 [00:26\u003c00:37,  1.26it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  47% 40/86 [00:27\u003c00:39,  1.17it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  48% 41/86 [00:28\u003c00:36,  1.24it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  49% 42/86 [00:29\u003c00:42,  1.03it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  50% 43/86 [00:30\u003c00:41,  1.03it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  51% 44/86 [00:31\u003c00:39,  1.07it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  52% 45/86 [00:32\u003c00:34,  1.20it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  53% 46/86 [00:32\u003c00:31,  1.28it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  55% 47/86 [00:33\u003c00:31,  1.25it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  56% 48/86 [00:34\u003c00:31,  1.21it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  57% 49/86 [00:35\u003c00:27,  1.36it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  58% 50/86 [00:35\u003c00:26,  1.38it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  59% 51/86 [00:36\u003c00:24,  1.41it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  60% 52/86 [00:37\u003c00:23,  1.43it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  62% 53/86 [00:38\u003c00:24,  1.34it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  63% 54/86 [00:38\u003c00:22,  1.39it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  64% 55/86 [00:39\u003c00:21,  1.43it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  65% 56/86 [00:40\u003c00:21,  1.40it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  66% 57/86 [00:40\u003c00:22,  1.30it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  67% 58/86 [00:41\u003c00:23,  1.20it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  69% 59/86 [00:42\u003c00:22,  1.21it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  70% 60/86 [00:43\u003c00:23,  1.10it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  71% 61/86 [00:45\u003c00:24,  1.01it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  72% 62/86 [00:46\u003c00:25,  1.08s/it]\u001b[A\n","epoch 037 | valid on 'valid' subset:  73% 63/86 [00:47\u003c00:23,  1.04s/it]\u001b[A\n","epoch 037 | valid on 'valid' subset:  74% 64/86 [00:48\u003c00:23,  1.08s/it]\u001b[A\n","epoch 037 | valid on 'valid' subset:  76% 65/86 [00:49\u003c00:22,  1.07s/it]\u001b[A\n","epoch 037 | valid on 'valid' subset:  77% 66/86 [00:50\u003c00:21,  1.10s/it]\u001b[A\n","epoch 037 | valid on 'valid' subset:  78% 67/86 [00:51\u003c00:19,  1.00s/it]\u001b[A\n","epoch 037 | valid on 'valid' subset:  79% 68/86 [00:52\u003c00:16,  1.12it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  80% 69/86 [00:52\u003c00:13,  1.22it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  81% 70/86 [00:53\u003c00:13,  1.20it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  83% 71/86 [00:54\u003c00:11,  1.30it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  84% 72/86 [00:54\u003c00:10,  1.35it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  85% 73/86 [00:55\u003c00:09,  1.33it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  86% 74/86 [00:56\u003c00:08,  1.34it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  87% 75/86 [00:57\u003c00:07,  1.41it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  88% 76/86 [00:57\u003c00:06,  1.51it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  90% 77/86 [00:58\u003c00:06,  1.50it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  91% 78/86 [00:59\u003c00:05,  1.37it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  92% 79/86 [01:00\u003c00:05,  1.30it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  93% 80/86 [01:01\u003c00:05,  1.15it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  94% 81/86 [01:02\u003c00:04,  1.04it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  95% 82/86 [01:03\u003c00:03,  1.01it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  97% 83/86 [01:04\u003c00:03,  1.07s/it]\u001b[A\n","epoch 037 | valid on 'valid' subset:  98% 84/86 [01:05\u003c00:02,  1.10s/it]\u001b[A\n","epoch 037 | valid on 'valid' subset:  99% 85/86 [01:06\u003c00:01,  1.03s/it]\u001b[A\n","epoch 037 | valid on 'valid' subset: 100% 86/86 [01:07\u003c00:00,  1.04it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 08:56:56 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 5.201 | nll_loss 3.891 | ppl 14.84 | bleu 14.35 | wps 3051.5 | wpb 2386.9 | bsz 73.7 | num_updates 8000 | best_bleu 14.35\n","2024-10-24 08:56:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 8000 updates\n","2024-10-24 08:56:56 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_37_8000.pt\n","2024-10-24 08:56:56 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_37_8000.pt\n","2024-10-24 08:56:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulm/checkpoint_37_8000.pt (epoch 37 @ 8000 updates, score 14.35) (writing took 1.228793979999864 seconds)\n","2024-10-24 08:57:06 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)\n","2024-10-24 08:57:06 | INFO | train | epoch 037 | loss 5.143 | nll_loss 3.95 | ppl 15.46 | wps 7891.6 | ups 2.47 | wpb 3195.3 | bsz 95.9 | num_updates 8103 | lr 0.00010539 | gnorm 1.162 | train_wall 18 | gb_free 14.1 | wall 1095\n","2024-10-24 08:57:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:57:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 038:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:57:07 | INFO | fairseq.trainer | begin training epoch 38\n","2024-10-24 08:57:07 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:57:26 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)\n","2024-10-24 08:57:26 | INFO | train | epoch 038 | loss 5.118 | nll_loss 3.92 | ppl 15.14 | wps 36219.7 | ups 11.34 | wpb 3195.3 | bsz 95.9 | num_updates 8322 | lr 0.000103994 | gnorm 1.139 | train_wall 17 | gb_free 14.1 | wall 1115\n","2024-10-24 08:57:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:57:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 039:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:57:26 | INFO | fairseq.trainer | begin training epoch 39\n","2024-10-24 08:57:26 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:57:44 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)\n","2024-10-24 08:57:44 | INFO | train | epoch 039 | loss 5.1 | nll_loss 3.899 | ppl 14.91 | wps 38113.4 | ups 11.93 | wpb 3195.3 | bsz 95.9 | num_updates 8541 | lr 0.000102652 | gnorm 1.166 | train_wall 17 | gb_free 14.2 | wall 1133\n","2024-10-24 08:57:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:57:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 040:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:57:44 | INFO | fairseq.trainer | begin training epoch 40\n","2024-10-24 08:57:44 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:58:02 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)\n","2024-10-24 08:58:02 | INFO | train | epoch 040 | loss 5.075 | nll_loss 3.869 | ppl 14.61 | wps 38469.8 | ups 12.04 | wpb 3195.3 | bsz 95.9 | num_updates 8760 | lr 0.000101361 | gnorm 1.148 | train_wall 17 | gb_free 14 | wall 1151\n","2024-10-24 08:58:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:58:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 041:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:58:02 | INFO | fairseq.trainer | begin training epoch 41\n","2024-10-24 08:58:02 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:58:21 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)\n","2024-10-24 08:58:21 | INFO | train | epoch 041 | loss 5.055 | nll_loss 3.846 | ppl 14.38 | wps 38147.1 | ups 11.94 | wpb 3195.3 | bsz 95.9 | num_updates 8979 | lr 0.000100117 | gnorm 1.156 | train_wall 17 | gb_free 14.1 | wall 1170\n","2024-10-24 08:58:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:58:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 042:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:58:21 | INFO | fairseq.trainer | begin training epoch 42\n","2024-10-24 08:58:21 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:58:40 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)\n","2024-10-24 08:58:40 | INFO | train | epoch 042 | loss 5.039 | nll_loss 3.827 | ppl 14.19 | wps 36836.5 | ups 11.53 | wpb 3195.3 | bsz 95.9 | num_updates 9198 | lr 9.89178e-05 | gnorm 1.185 | train_wall 17 | gb_free 14.1 | wall 1189\n","2024-10-24 08:58:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:58:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 043:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:58:40 | INFO | fairseq.trainer | begin training epoch 43\n","2024-10-24 08:58:40 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:58:59 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)\n","2024-10-24 08:58:59 | INFO | train | epoch 043 | loss 5.019 | nll_loss 3.804 | ppl 13.97 | wps 36881.5 | ups 11.54 | wpb 3195.3 | bsz 95.9 | num_updates 9417 | lr 9.77609e-05 | gnorm 1.166 | train_wall 17 | gb_free 14.1 | wall 1208\n","2024-10-24 08:58:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:58:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 044:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:58:59 | INFO | fairseq.trainer | begin training epoch 44\n","2024-10-24 08:58:59 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:59:17 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)\n","2024-10-24 08:59:17 | INFO | train | epoch 044 | loss 4.999 | nll_loss 3.781 | ppl 13.74 | wps 37692.8 | ups 11.8 | wpb 3195.3 | bsz 95.9 | num_updates 9636 | lr 9.66435e-05 | gnorm 1.175 | train_wall 17 | gb_free 14 | wall 1226\n","2024-10-24 08:59:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:59:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 045:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:59:17 | INFO | fairseq.trainer | begin training epoch 45\n","2024-10-24 08:59:17 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 08:59:35 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)\n","2024-10-24 08:59:35 | INFO | train | epoch 045 | loss 4.98 | nll_loss 3.757 | ppl 13.52 | wps 38361.6 | ups 12.01 | wpb 3195.3 | bsz 95.9 | num_updates 9855 | lr 9.55637e-05 | gnorm 1.187 | train_wall 17 | gb_free 14.1 | wall 1244\n","2024-10-24 08:59:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 08:59:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 046:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 08:59:36 | INFO | fairseq.trainer | begin training epoch 46\n","2024-10-24 08:59:36 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 046:  65% 143/219 [00:12\u003c00:06, 11.97it/s, loss=5.024, nll_loss=3.807, ppl=13.99, wps=39916.6, ups=12.49, wpb=3194.7, bsz=87.5, num_updates=9900, lr=9.53463e-05, gnorm=1.165, train_wall=7, gb_free=14.1, wall=1248]2024-10-24 08:59:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 08:59:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 046 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:   1% 1/86 [00:00\u003c00:51,  1.66it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:   2% 2/86 [00:01\u003c00:41,  2.01it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:   3% 3/86 [00:01\u003c00:40,  2.06it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:   5% 4/86 [00:01\u003c00:37,  2.16it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:   6% 5/86 [00:02\u003c00:34,  2.37it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:   7% 6/86 [00:02\u003c00:35,  2.28it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:   8% 7/86 [00:03\u003c00:35,  2.23it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:   9% 8/86 [00:03\u003c00:39,  1.98it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  10% 9/86 [00:04\u003c00:35,  2.19it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  12% 10/86 [00:04\u003c00:33,  2.27it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  13% 11/86 [00:05\u003c00:42,  1.76it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  14% 12/86 [00:06\u003c00:45,  1.64it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  15% 13/86 [00:06\u003c00:41,  1.77it/s]\u001b[A2024-10-24 08:59:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:59:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:59:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 046 | valid on 'valid' subset:  16% 14/86 [00:07\u003c00:40,  1.78it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  17% 15/86 [00:07\u003c00:35,  1.97it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  19% 16/86 [00:08\u003c00:36,  1.91it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  20% 17/86 [00:08\u003c00:33,  2.03it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  21% 18/86 [00:09\u003c00:39,  1.74it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  22% 19/86 [00:09\u003c00:37,  1.81it/s]\u001b[A2024-10-24 08:59:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 08:59:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 08:59:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 046 | valid on 'valid' subset:  23% 20/86 [00:10\u003c00:43,  1.52it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  24% 21/86 [00:11\u003c00:44,  1.45it/s]\u001b[A2024-10-24 09:00:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:00:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:00:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 046 | valid on 'valid' subset:  26% 22/86 [00:12\u003c00:48,  1.31it/s]\u001b[A\n","epoch 046:  65% 143/219 [00:27\u003c00:06, 11.97it/s, loss=4.962, nll_loss=3.737, ppl=13.33, wps=35039.5, ups=10.99, wpb=3188.8, bsz=94.4, num_updates=10000, lr=9.48683e-05, gnorm=1.186, train_wall=8, gb_free=14.1, wall=1257]\n","epoch 046 | valid on 'valid' subset:  28% 24/86 [00:13\u003c00:46,  1.35it/s]\u001b[A2024-10-24 09:00:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:00:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:00:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 046 | valid on 'valid' subset:  29% 25/86 [00:14\u003c00:48,  1.26it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  30% 26/86 [00:15\u003c00:45,  1.32it/s]\u001b[A2024-10-24 09:00:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:00:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:00:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 046 | valid on 'valid' subset:  31% 27/86 [00:16\u003c00:42,  1.37it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  33% 28/86 [00:16\u003c00:42,  1.38it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  34% 29/86 [00:17\u003c00:38,  1.47it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  35% 30/86 [00:18\u003c00:38,  1.44it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  36% 31/86 [00:18\u003c00:35,  1.56it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  37% 32/86 [00:19\u003c00:30,  1.76it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  38% 33/86 [00:19\u003c00:32,  1.64it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  40% 34/86 [00:20\u003c00:35,  1.48it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  41% 35/86 [00:21\u003c00:34,  1.48it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  42% 36/86 [00:21\u003c00:31,  1.57it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  43% 37/86 [00:22\u003c00:32,  1.49it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  44% 38/86 [00:23\u003c00:36,  1.32it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  45% 39/86 [00:24\u003c00:32,  1.44it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  47% 40/86 [00:24\u003c00:31,  1.47it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  48% 41/86 [00:25\u003c00:27,  1.65it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  49% 42/86 [00:26\u003c00:35,  1.26it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  50% 43/86 [00:27\u003c00:34,  1.25it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  51% 44/86 [00:28\u003c00:35,  1.19it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  52% 45/86 [00:29\u003c00:35,  1.16it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  53% 46/86 [00:30\u003c00:36,  1.10it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  55% 47/86 [00:31\u003c00:38,  1.01it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  56% 48/86 [00:31\u003c00:34,  1.10it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  57% 49/86 [00:32\u003c00:29,  1.25it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  58% 50/86 [00:33\u003c00:27,  1.33it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  59% 51/86 [00:33\u003c00:24,  1.42it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  60% 52/86 [00:34\u003c00:24,  1.41it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  62% 53/86 [00:35\u003c00:22,  1.44it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  63% 54/86 [00:35\u003c00:22,  1.45it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  64% 55/86 [00:36\u003c00:20,  1.50it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  65% 56/86 [00:37\u003c00:20,  1.48it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  66% 57/86 [00:37\u003c00:18,  1.55it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  67% 58/86 [00:38\u003c00:18,  1.54it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  69% 59/86 [00:38\u003c00:17,  1.56it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  70% 60/86 [00:39\u003c00:17,  1.45it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  71% 61/86 [00:40\u003c00:18,  1.35it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  72% 62/86 [00:41\u003c00:17,  1.38it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  73% 63/86 [00:42\u003c00:17,  1.31it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  74% 64/86 [00:43\u003c00:18,  1.22it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  76% 65/86 [00:43\u003c00:17,  1.19it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  77% 66/86 [00:44\u003c00:17,  1.15it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  78% 67/86 [00:45\u003c00:17,  1.12it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  79% 68/86 [00:46\u003c00:15,  1.13it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  80% 69/86 [00:47\u003c00:14,  1.14it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  81% 70/86 [00:48\u003c00:14,  1.14it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  83% 71/86 [00:49\u003c00:12,  1.23it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  84% 72/86 [00:49\u003c00:11,  1.26it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  85% 73/86 [00:50\u003c00:09,  1.31it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  86% 74/86 [00:51\u003c00:09,  1.33it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  87% 75/86 [00:51\u003c00:07,  1.38it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  88% 76/86 [00:52\u003c00:06,  1.45it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  90% 77/86 [00:53\u003c00:06,  1.44it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  91% 78/86 [00:53\u003c00:05,  1.44it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  92% 79/86 [00:54\u003c00:05,  1.28it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  93% 80/86 [00:55\u003c00:04,  1.20it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  94% 81/86 [00:56\u003c00:04,  1.19it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  95% 82/86 [00:57\u003c00:03,  1.10it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  97% 83/86 [00:59\u003c00:02,  1.02it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  98% 84/86 [01:00\u003c00:02,  1.05s/it]\u001b[A\n","epoch 046 | valid on 'valid' subset:  99% 85/86 [01:01\u003c00:01,  1.13s/it]\u001b[A\n","epoch 046 | valid on 'valid' subset: 100% 86/86 [01:02\u003c00:00,  1.11s/it]\u001b[A\n","                                                                        \u001b[A2024-10-24 09:00:51 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 5.07 | nll_loss 3.729 | ppl 13.26 | bleu 15.39 | wps 3294 | wpb 2386.9 | bsz 73.7 | num_updates 10000 | best_bleu 15.39\n","2024-10-24 09:00:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 10000 updates\n","2024-10-24 09:00:51 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_46_10000.pt\n","2024-10-24 09:00:52 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_46_10000.pt\n","2024-10-24 09:00:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulm/checkpoint_46_10000.pt (epoch 46 @ 10000 updates, score 15.39) (writing took 1.9998963039997761 seconds)\n","2024-10-24 09:01:00 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)\n","2024-10-24 09:01:00 | INFO | train | epoch 046 | loss 4.962 | nll_loss 3.737 | ppl 13.33 | wps 8280.2 | ups 2.59 | wpb 3195.3 | bsz 95.9 | num_updates 10074 | lr 9.45193e-05 | gnorm 1.17 | train_wall 18 | gb_free 14.1 | wall 1329\n","2024-10-24 09:01:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:01:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 047:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:01:00 | INFO | fairseq.trainer | begin training epoch 47\n","2024-10-24 09:01:00 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:01:19 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)\n","2024-10-24 09:01:19 | INFO | train | epoch 047 | loss 4.947 | nll_loss 3.718 | ppl 13.16 | wps 36287.7 | ups 11.36 | wpb 3195.3 | bsz 95.9 | num_updates 10293 | lr 9.35083e-05 | gnorm 1.192 | train_wall 18 | gb_free 14.1 | wall 1348\n","2024-10-24 09:01:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:01:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 048:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:01:19 | INFO | fairseq.trainer | begin training epoch 48\n","2024-10-24 09:01:19 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:01:38 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)\n","2024-10-24 09:01:38 | INFO | train | epoch 048 | loss 4.931 | nll_loss 3.7 | ppl 12.99 | wps 36849 | ups 11.53 | wpb 3195.3 | bsz 95.9 | num_updates 10512 | lr 9.25292e-05 | gnorm 1.204 | train_wall 17 | gb_free 14.1 | wall 1367\n","2024-10-24 09:01:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:01:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 049:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:01:38 | INFO | fairseq.trainer | begin training epoch 49\n","2024-10-24 09:01:38 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:01:58 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)\n","2024-10-24 09:01:58 | INFO | train | epoch 049 | loss 4.909 | nll_loss 3.675 | ppl 12.77 | wps 35256.9 | ups 11.03 | wpb 3195.3 | bsz 95.9 | num_updates 10731 | lr 9.15801e-05 | gnorm 1.191 | train_wall 18 | gb_free 14.1 | wall 1387\n","2024-10-24 09:01:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:01:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 050:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:01:58 | INFO | fairseq.trainer | begin training epoch 50\n","2024-10-24 09:01:58 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 050:  99% 217/219 [00:18\u003c00:00, 13.82it/s, loss=4.934, nll_loss=3.702, ppl=13.02, wps=36782.2, ups=11.25, wpb=3268.7, bsz=96, num_updates=10900, lr=9.08674e-05, gnorm=1.209, train_wall=8, gb_free=14, wall=1402]2024-10-24 09:02:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 09:02:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 050 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:   1% 1/86 [00:00\u003c00:49,  1.72it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:   2% 2/86 [00:01\u003c00:41,  2.04it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:   3% 3/86 [00:01\u003c00:39,  2.11it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:   5% 4/86 [00:01\u003c00:34,  2.36it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:   6% 5/86 [00:02\u003c00:31,  2.55it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:   7% 6/86 [00:02\u003c00:30,  2.61it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:   8% 7/86 [00:03\u003c00:33,  2.38it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:   9% 8/86 [00:03\u003c00:39,  1.99it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  10% 9/86 [00:04\u003c00:34,  2.21it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  12% 10/86 [00:04\u003c00:32,  2.32it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  13% 11/86 [00:05\u003c00:47,  1.59it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  14% 12/86 [00:06\u003c00:53,  1.38it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  15% 13/86 [00:07\u003c00:50,  1.44it/s]\u001b[A2024-10-24 09:02:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:02:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:02:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 050 | valid on 'valid' subset:  16% 14/86 [00:07\u003c00:51,  1.40it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  17% 15/86 [00:08\u003c00:59,  1.19it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  19% 16/86 [00:09\u003c00:58,  1.20it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  20% 17/86 [00:10\u003c00:54,  1.27it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  21% 18/86 [00:11\u003c00:51,  1.33it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  22% 19/86 [00:11\u003c00:43,  1.54it/s]\u001b[A2024-10-24 09:02:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:02:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:02:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 050 | valid on 'valid' subset:  23% 20/86 [00:12\u003c00:44,  1.50it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  24% 21/86 [00:12\u003c00:40,  1.62it/s]\u001b[A2024-10-24 09:02:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:02:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:02:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 050 | valid on 'valid' subset:  26% 22/86 [00:13\u003c00:40,  1.60it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  27% 23/86 [00:13\u003c00:37,  1.69it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  28% 24/86 [00:14\u003c00:34,  1.78it/s]\u001b[A2024-10-24 09:02:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:02:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:02:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 050 | valid on 'valid' subset:  29% 25/86 [00:15\u003c00:39,  1.55it/s]\u001b[A\n","epoch 050:  99% 217/219 [00:34\u003c00:00, 13.82it/s, loss=4.934, nll_loss=3.702, ppl=13.02, wps=36782.2, ups=11.25, wpb=3268.7, bsz=96, num_updates=10900, lr=9.08674e-05, gnorm=1.209, train_wall=8, gb_free=14, wall=1402]2024-10-24 09:02:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:02:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:02:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 050 | valid on 'valid' subset:  31% 27/86 [00:16\u003c00:38,  1.54it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  33% 28/86 [00:17\u003c00:36,  1.60it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  34% 29/86 [00:17\u003c00:33,  1.69it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  35% 30/86 [00:18\u003c00:38,  1.44it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  36% 31/86 [00:19\u003c00:36,  1.52it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  37% 32/86 [00:19\u003c00:31,  1.72it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  38% 33/86 [00:20\u003c00:33,  1.59it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  40% 34/86 [00:21\u003c00:36,  1.44it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  41% 35/86 [00:21\u003c00:38,  1.33it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  42% 36/86 [00:22\u003c00:39,  1.27it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  43% 37/86 [00:23\u003c00:40,  1.22it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  44% 38/86 [00:24\u003c00:45,  1.06it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  45% 39/86 [00:25\u003c00:44,  1.05it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  47% 40/86 [00:26\u003c00:44,  1.04it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  48% 41/86 [00:27\u003c00:37,  1.21it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  49% 42/86 [00:28\u003c00:37,  1.17it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  50% 43/86 [00:28\u003c00:34,  1.26it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  51% 44/86 [00:29\u003c00:32,  1.29it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  52% 45/86 [00:30\u003c00:30,  1.36it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  53% 46/86 [00:31\u003c00:32,  1.25it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  55% 47/86 [00:32\u003c00:31,  1.22it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  56% 48/86 [00:32\u003c00:28,  1.31it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  57% 49/86 [00:33\u003c00:25,  1.45it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  58% 50/86 [00:33\u003c00:24,  1.47it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  59% 51/86 [00:34\u003c00:22,  1.53it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  60% 52/86 [00:35\u003c00:22,  1.54it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  62% 53/86 [00:36\u003c00:23,  1.41it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  63% 54/86 [00:36\u003c00:23,  1.33it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  64% 55/86 [00:37\u003c00:25,  1.24it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  65% 56/86 [00:38\u003c00:25,  1.18it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  66% 57/86 [00:39\u003c00:25,  1.14it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  67% 58/86 [00:40\u003c00:26,  1.08it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  69% 59/86 [00:42\u003c00:27,  1.02s/it]\u001b[A\n","epoch 050 | valid on 'valid' subset:  70% 60/86 [00:42\u003c00:25,  1.01it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  71% 61/86 [00:43\u003c00:23,  1.06it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  72% 62/86 [00:44\u003c00:21,  1.12it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  73% 63/86 [00:45\u003c00:18,  1.22it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  74% 64/86 [00:45\u003c00:17,  1.27it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  76% 65/86 [00:46\u003c00:15,  1.34it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  77% 66/86 [00:47\u003c00:14,  1.42it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  78% 67/86 [00:47\u003c00:13,  1.39it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  79% 68/86 [00:48\u003c00:12,  1.47it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  80% 69/86 [00:49\u003c00:11,  1.48it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  81% 70/86 [00:50\u003c00:11,  1.35it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  83% 71/86 [00:50\u003c00:11,  1.34it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  84% 72/86 [00:51\u003c00:10,  1.36it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  85% 73/86 [00:52\u003c00:10,  1.29it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  86% 74/86 [00:53\u003c00:10,  1.15it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  87% 75/86 [00:54\u003c00:09,  1.11it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  88% 76/86 [00:55\u003c00:09,  1.11it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  90% 77/86 [00:56\u003c00:08,  1.07it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  91% 78/86 [00:57\u003c00:08,  1.01s/it]\u001b[A\n","epoch 050 | valid on 'valid' subset:  92% 79/86 [00:58\u003c00:07,  1.08s/it]\u001b[A\n","epoch 050 | valid on 'valid' subset:  93% 80/86 [00:59\u003c00:06,  1.07s/it]\u001b[A\n","epoch 050 | valid on 'valid' subset:  94% 81/86 [01:00\u003c00:05,  1.01s/it]\u001b[A\n","epoch 050 | valid on 'valid' subset:  95% 82/86 [01:01\u003c00:03,  1.04it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  97% 83/86 [01:02\u003c00:02,  1.07it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  98% 84/86 [01:03\u003c00:01,  1.10it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  99% 85/86 [01:04\u003c00:00,  1.09it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset: 100% 86/86 [01:05\u003c00:00,  1.12it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 09:03:21 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 5.023 | nll_loss 3.673 | ppl 12.75 | bleu 15.6 | wps 3166.5 | wpb 2386.9 | bsz 73.7 | num_updates 10950 | best_bleu 15.6\n","2024-10-24 09:03:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 10950 updates\n","2024-10-24 09:03:21 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_best.pt\n","2024-10-24 09:03:22 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_best.pt\n","2024-10-24 09:03:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulm/checkpoint_best.pt (epoch 50 @ 10950 updates, score 15.6) (writing took 0.869555852999838 seconds)\n","2024-10-24 09:03:22 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)\n","2024-10-24 09:03:22 | INFO | train | epoch 050 | loss 4.898 | nll_loss 3.661 | ppl 12.65 | wps 8302.5 | ups 2.6 | wpb 3195.3 | bsz 95.9 | num_updates 10950 | lr 9.06597e-05 | gnorm 1.195 | train_wall 17 | gb_free 14.1 | wall 1471\n","2024-10-24 09:03:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:03:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 051:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:03:22 | INFO | fairseq.trainer | begin training epoch 51\n","2024-10-24 09:03:22 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:03:42 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)\n","2024-10-24 09:03:42 | INFO | train | epoch 051 | loss 4.883 | nll_loss 3.643 | ppl 12.49 | wps 35268.7 | ups 11.04 | wpb 3195.3 | bsz 95.9 | num_updates 11169 | lr 8.97665e-05 | gnorm 1.194 | train_wall 18 | gb_free 14.2 | wall 1491\n","2024-10-24 09:03:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:03:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 052:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:03:42 | INFO | fairseq.trainer | begin training epoch 52\n","2024-10-24 09:03:42 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:04:01 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)\n","2024-10-24 09:04:01 | INFO | train | epoch 052 | loss 4.862 | nll_loss 3.619 | ppl 12.29 | wps 36501.7 | ups 11.42 | wpb 3195.3 | bsz 95.9 | num_updates 11388 | lr 8.88991e-05 | gnorm 1.18 | train_wall 17 | gb_free 14.2 | wall 1510\n","2024-10-24 09:04:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:04:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 053:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:04:02 | INFO | fairseq.trainer | begin training epoch 53\n","2024-10-24 09:04:02 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:04:20 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)\n","2024-10-24 09:04:20 | INFO | train | epoch 053 | loss 4.85 | nll_loss 3.604 | ppl 12.16 | wps 36972.7 | ups 11.57 | wpb 3195.3 | bsz 95.9 | num_updates 11607 | lr 8.80565e-05 | gnorm 1.201 | train_wall 17 | gb_free 14.1 | wall 1529\n","2024-10-24 09:04:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:04:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 054:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:04:20 | INFO | fairseq.trainer | begin training epoch 54\n","2024-10-24 09:04:20 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:04:39 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)\n","2024-10-24 09:04:39 | INFO | train | epoch 054 | loss 4.839 | nll_loss 3.592 | ppl 12.06 | wps 37639.2 | ups 11.78 | wpb 3195.3 | bsz 95.9 | num_updates 11826 | lr 8.72373e-05 | gnorm 1.223 | train_wall 17 | gb_free 14.2 | wall 1548\n","2024-10-24 09:04:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:04:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 055:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:04:39 | INFO | fairseq.trainer | begin training epoch 55\n","2024-10-24 09:04:39 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 055:  79% 173/219 [00:14\u003c00:03, 12.70it/s, loss=4.848, nll_loss=3.601, ppl=12.13, wps=40244.4, ups=12.33, wpb=3263.1, bsz=95.6, num_updates=11900, lr=8.69657e-05, gnorm=1.22, train_wall=7, gb_free=14.2, wall=1554]2024-10-24 09:04:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 09:04:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 055 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:   1% 1/86 [00:00\u003c00:49,  1.73it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:   2% 2/86 [00:01\u003c00:40,  2.05it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:   3% 3/86 [00:01\u003c00:42,  1.94it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:   5% 4/86 [00:01\u003c00:39,  2.09it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:   6% 5/86 [00:02\u003c00:35,  2.27it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:   7% 6/86 [00:02\u003c00:34,  2.34it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:   8% 7/86 [00:03\u003c00:33,  2.38it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:   9% 8/86 [00:03\u003c00:39,  1.99it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  10% 9/86 [00:04\u003c00:35,  2.20it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  12% 10/86 [00:04\u003c00:33,  2.30it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  13% 11/86 [00:05\u003c00:37,  1.98it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  14% 12/86 [00:05\u003c00:37,  1.97it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  15% 13/86 [00:06\u003c00:36,  2.01it/s]\u001b[A2024-10-24 09:05:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:05:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:05:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 055 | valid on 'valid' subset:  16% 14/86 [00:06\u003c00:36,  1.96it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  17% 15/86 [00:07\u003c00:42,  1.67it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  19% 16/86 [00:08\u003c00:41,  1.71it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  20% 17/86 [00:08\u003c00:39,  1.76it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  21% 18/86 [00:09\u003c00:44,  1.52it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  22% 19/86 [00:10\u003c00:40,  1.64it/s]\u001b[A2024-10-24 09:05:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:05:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:05:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 055 | valid on 'valid' subset:  23% 20/86 [00:10\u003c00:45,  1.46it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  24% 21/86 [00:11\u003c00:43,  1.51it/s]\u001b[A2024-10-24 09:05:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:05:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:05:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 055 | valid on 'valid' subset:  26% 22/86 [00:12\u003c00:47,  1.34it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  27% 23/86 [00:13\u003c00:47,  1.31it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  28% 24/86 [00:13\u003c00:44,  1.40it/s]\u001b[A2024-10-24 09:05:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:05:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:05:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 055 | valid on 'valid' subset:  29% 25/86 [00:14\u003c00:47,  1.29it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  30% 26/86 [00:15\u003c00:40,  1.48it/s]\u001b[A2024-10-24 09:05:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:05:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:05:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 055 | valid on 'valid' subset:  31% 27/86 [00:15\u003c00:40,  1.47it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  33% 28/86 [00:16\u003c00:38,  1.52it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  34% 29/86 [00:16\u003c00:33,  1.69it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  35% 30/86 [00:17\u003c00:34,  1.60it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  36% 31/86 [00:18\u003c00:32,  1.67it/s]\u001b[A\n","epoch 055:  79% 173/219 [00:33\u003c00:03, 12.70it/s, loss=4.808, nll_loss=3.556, ppl=11.76, wps=35611, ups=11.37, wpb=3132.9, bsz=96, num_updates=12000, lr=8.66025e-05, gnorm=1.219, train_wall=8, gb_free=14.1, wall=1563]   \n","epoch 055 | valid on 'valid' subset:  38% 33/86 [00:19\u003c00:35,  1.50it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  40% 34/86 [00:20\u003c00:34,  1.52it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  41% 35/86 [00:20\u003c00:33,  1.54it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  42% 36/86 [00:21\u003c00:31,  1.57it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  43% 37/86 [00:22\u003c00:30,  1.59it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  44% 38/86 [00:22\u003c00:30,  1.60it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  45% 39/86 [00:23\u003c00:32,  1.46it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  47% 40/86 [00:24\u003c00:33,  1.39it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  48% 41/86 [00:24\u003c00:29,  1.55it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  49% 42/86 [00:25\u003c00:36,  1.21it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  50% 43/86 [00:27\u003c00:40,  1.05it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  51% 44/86 [00:28\u003c00:39,  1.05it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  52% 45/86 [00:29\u003c00:39,  1.05it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  53% 46/86 [00:30\u003c00:37,  1.07it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  55% 47/86 [00:30\u003c00:36,  1.07it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  56% 48/86 [00:31\u003c00:31,  1.19it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  57% 49/86 [00:32\u003c00:29,  1.27it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  58% 50/86 [00:32\u003c00:27,  1.33it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  59% 51/86 [00:33\u003c00:24,  1.43it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  60% 52/86 [00:34\u003c00:23,  1.48it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  62% 53/86 [00:34\u003c00:23,  1.38it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  63% 54/86 [00:35\u003c00:22,  1.43it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  64% 55/86 [00:36\u003c00:21,  1.48it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  65% 56/86 [00:36\u003c00:20,  1.46it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  66% 57/86 [00:37\u003c00:19,  1.50it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  67% 58/86 [00:38\u003c00:19,  1.43it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  69% 59/86 [00:38\u003c00:17,  1.52it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  70% 60/86 [00:39\u003c00:17,  1.46it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  71% 61/86 [00:40\u003c00:18,  1.36it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  72% 62/86 [00:41\u003c00:20,  1.16it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  73% 63/86 [00:42\u003c00:20,  1.14it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  74% 64/86 [00:43\u003c00:19,  1.14it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  76% 65/86 [00:44\u003c00:20,  1.02it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  77% 66/86 [00:45\u003c00:19,  1.05it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  78% 67/86 [00:46\u003c00:19,  1.00s/it]\u001b[A\n","epoch 055 | valid on 'valid' subset:  79% 68/86 [00:47\u003c00:16,  1.10it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  80% 69/86 [00:47\u003c00:14,  1.20it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  81% 70/86 [00:48\u003c00:13,  1.17it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  83% 71/86 [00:49\u003c00:12,  1.21it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  84% 72/86 [00:50\u003c00:10,  1.28it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  85% 73/86 [00:50\u003c00:09,  1.34it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  86% 74/86 [00:51\u003c00:08,  1.37it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  87% 75/86 [00:52\u003c00:07,  1.40it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  88% 76/86 [00:52\u003c00:06,  1.45it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  90% 77/86 [00:53\u003c00:06,  1.42it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  91% 78/86 [00:54\u003c00:05,  1.34it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  92% 79/86 [00:55\u003c00:05,  1.27it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  93% 80/86 [00:56\u003c00:04,  1.22it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  94% 81/86 [00:57\u003c00:04,  1.15it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  95% 82/86 [00:58\u003c00:03,  1.04it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  97% 83/86 [00:59\u003c00:03,  1.03s/it]\u001b[A\n","epoch 055 | valid on 'valid' subset:  98% 84/86 [01:00\u003c00:02,  1.11s/it]\u001b[A\n","epoch 055 | valid on 'valid' subset:  99% 85/86 [01:02\u003c00:01,  1.16s/it]\u001b[A\n","epoch 055 | valid on 'valid' subset: 100% 86/86 [01:03\u003c00:00,  1.15s/it]\u001b[A\n","                                                                        \u001b[A2024-10-24 09:05:57 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 4.988 | nll_loss 3.627 | ppl 12.35 | bleu 16.5 | wps 3251.2 | wpb 2386.9 | bsz 73.7 | num_updates 12000 | best_bleu 16.5\n","2024-10-24 09:05:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 12000 updates\n","2024-10-24 09:05:57 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_55_12000.pt\n","2024-10-24 09:05:58 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_55_12000.pt\n","2024-10-24 09:05:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulm/checkpoint_55_12000.pt (epoch 55 @ 12000 updates, score 16.5) (writing took 1.2902290440006254 seconds)\n","2024-10-24 09:06:02 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)\n","2024-10-24 09:06:02 | INFO | train | epoch 055 | loss 4.826 | nll_loss 3.576 | ppl 11.93 | wps 8399.8 | ups 2.63 | wpb 3195.3 | bsz 95.9 | num_updates 12045 | lr 8.64406e-05 | gnorm 1.224 | train_wall 17 | gb_free 14.1 | wall 1631\n","2024-10-24 09:06:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:06:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 056:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:06:02 | INFO | fairseq.trainer | begin training epoch 56\n","2024-10-24 09:06:02 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:06:22 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)\n","2024-10-24 09:06:22 | INFO | train | epoch 056 | loss 4.806 | nll_loss 3.553 | ppl 11.74 | wps 35803.2 | ups 11.2 | wpb 3195.3 | bsz 95.9 | num_updates 12264 | lr 8.56653e-05 | gnorm 1.199 | train_wall 18 | gb_free 14.1 | wall 1651\n","2024-10-24 09:06:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:06:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 057:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:06:22 | INFO | fairseq.trainer | begin training epoch 57\n","2024-10-24 09:06:22 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:06:40 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)\n","2024-10-24 09:06:40 | INFO | train | epoch 057 | loss 4.792 | nll_loss 3.536 | ppl 11.6 | wps 37817 | ups 11.84 | wpb 3195.3 | bsz 95.9 | num_updates 12483 | lr 8.49106e-05 | gnorm 1.203 | train_wall 17 | gb_free 14.2 | wall 1669\n","2024-10-24 09:06:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:06:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 058:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:06:40 | INFO | fairseq.trainer | begin training epoch 58\n","2024-10-24 09:06:40 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:06:59 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)\n","2024-10-24 09:06:59 | INFO | train | epoch 058 | loss 4.78 | nll_loss 3.522 | ppl 11.49 | wps 37285.8 | ups 11.67 | wpb 3195.3 | bsz 95.9 | num_updates 12702 | lr 8.41754e-05 | gnorm 1.203 | train_wall 17 | gb_free 14.1 | wall 1688\n","2024-10-24 09:06:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:06:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 059:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:06:59 | INFO | fairseq.trainer | begin training epoch 59\n","2024-10-24 09:06:59 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:07:18 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)\n","2024-10-24 09:07:18 | INFO | train | epoch 059 | loss 4.766 | nll_loss 3.505 | ppl 11.35 | wps 37251.4 | ups 11.66 | wpb 3195.3 | bsz 95.9 | num_updates 12921 | lr 8.3459e-05 | gnorm 1.208 | train_wall 17 | gb_free 14.1 | wall 1707\n","2024-10-24 09:07:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:07:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 060:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:07:18 | INFO | fairseq.trainer | begin training epoch 60\n","2024-10-24 09:07:18 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:07:37 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)\n","2024-10-24 09:07:37 | INFO | train | epoch 060 | loss 4.753 | nll_loss 3.491 | ppl 11.24 | wps 36122.2 | ups 11.3 | wpb 3195.3 | bsz 95.9 | num_updates 13140 | lr 8.27606e-05 | gnorm 1.212 | train_wall 17 | gb_free 14.1 | wall 1726\n","2024-10-24 09:07:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:07:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 061:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:07:37 | INFO | fairseq.trainer | begin training epoch 61\n","2024-10-24 09:07:37 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:07:56 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)\n","2024-10-24 09:07:56 | INFO | train | epoch 061 | loss 4.742 | nll_loss 3.477 | ppl 11.13 | wps 38110.1 | ups 11.93 | wpb 3195.3 | bsz 95.9 | num_updates 13359 | lr 8.20794e-05 | gnorm 1.227 | train_wall 17 | gb_free 14.2 | wall 1745\n","2024-10-24 09:07:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:07:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 062:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:07:56 | INFO | fairseq.trainer | begin training epoch 62\n","2024-10-24 09:07:56 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:08:14 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)\n","2024-10-24 09:08:14 | INFO | train | epoch 062 | loss 4.732 | nll_loss 3.465 | ppl 11.04 | wps 37932.2 | ups 11.87 | wpb 3195.3 | bsz 95.9 | num_updates 13578 | lr 8.14148e-05 | gnorm 1.243 | train_wall 17 | gb_free 14.1 | wall 1763\n","2024-10-24 09:08:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:08:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 063:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:08:14 | INFO | fairseq.trainer | begin training epoch 63\n","2024-10-24 09:08:14 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:08:32 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)\n","2024-10-24 09:08:32 | INFO | train | epoch 063 | loss 4.717 | nll_loss 3.447 | ppl 10.91 | wps 38360.2 | ups 12.01 | wpb 3195.3 | bsz 95.9 | num_updates 13797 | lr 8.07661e-05 | gnorm 1.213 | train_wall 17 | gb_free 14.1 | wall 1781\n","2024-10-24 09:08:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:08:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 064:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:08:32 | INFO | fairseq.trainer | begin training epoch 64\n","2024-10-24 09:08:32 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 064:  92% 202/219 [00:17\u003c00:01, 10.76it/s, loss=4.712, nll_loss=3.442, ppl=10.87, wps=38370, ups=11.6, wpb=3309.1, bsz=97.1, num_updates=13900, lr=8.04663e-05, gnorm=1.205, train_wall=8, gb_free=14, wall=1791]2024-10-24 09:08:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 09:08:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 064 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:   1% 1/86 [00:00\u003c01:07,  1.26it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:   2% 2/86 [00:01\u003c00:56,  1.48it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:   3% 3/86 [00:02\u003c00:55,  1.49it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:   5% 4/86 [00:02\u003c00:51,  1.60it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:   6% 5/86 [00:03\u003c00:47,  1.70it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:   7% 6/86 [00:03\u003c00:46,  1.72it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:   8% 7/86 [00:04\u003c00:47,  1.67it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:   9% 8/86 [00:04\u003c00:42,  1.82it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  10% 9/86 [00:05\u003c00:37,  2.03it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  12% 10/86 [00:05\u003c00:34,  2.18it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  13% 11/86 [00:06\u003c00:40,  1.87it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  14% 12/86 [00:06\u003c00:37,  1.98it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  15% 13/86 [00:07\u003c00:34,  2.13it/s]\u001b[A2024-10-24 09:08:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:08:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:08:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 064 | valid on 'valid' subset:  16% 14/86 [00:07\u003c00:35,  2.03it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  17% 15/86 [00:07\u003c00:32,  2.22it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  19% 16/86 [00:08\u003c00:35,  1.99it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  20% 17/86 [00:08\u003c00:32,  2.10it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  21% 18/86 [00:09\u003c00:34,  1.99it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  22% 19/86 [00:09\u003c00:31,  2.15it/s]\u001b[A2024-10-24 09:09:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:09:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:09:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 064 | valid on 'valid' subset:  23% 20/86 [00:10\u003c00:32,  2.02it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  24% 21/86 [00:10\u003c00:31,  2.09it/s]\u001b[A2024-10-24 09:09:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:09:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:09:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 064 | valid on 'valid' subset:  26% 22/86 [00:11\u003c00:32,  1.98it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  27% 23/86 [00:12\u003c00:31,  1.97it/s]\u001b[A\n","epoch 064:  92% 202/219 [00:30\u003c00:01, 10.76it/s, loss=4.705, nll_loss=3.436, ppl=10.82, wps=37559.9, ups=12.23, wpb=3071.8, bsz=93.7, num_updates=14000, lr=8.01784e-05, gnorm=1.238, train_wall=8, gb_free=14.1, wall=1799]2024-10-24 09:09:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:09:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:09:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 064 | valid on 'valid' subset:  29% 25/86 [00:13\u003c00:31,  1.92it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  30% 26/86 [00:13\u003c00:30,  1.99it/s]\u001b[A2024-10-24 09:09:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:09:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:09:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 064 | valid on 'valid' subset:  31% 27/86 [00:14\u003c00:33,  1.78it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  33% 28/86 [00:14\u003c00:34,  1.67it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  34% 29/86 [00:15\u003c00:35,  1.61it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  35% 30/86 [00:16\u003c00:37,  1.50it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  36% 31/86 [00:16\u003c00:35,  1.57it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  37% 32/86 [00:17\u003c00:32,  1.67it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  38% 33/86 [00:18\u003c00:36,  1.44it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  40% 34/86 [00:19\u003c00:37,  1.40it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  41% 35/86 [00:19\u003c00:39,  1.31it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  42% 36/86 [00:20\u003c00:35,  1.39it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  43% 37/86 [00:21\u003c00:32,  1.49it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  44% 38/86 [00:21\u003c00:30,  1.59it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  45% 39/86 [00:22\u003c00:28,  1.66it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  47% 40/86 [00:22\u003c00:26,  1.73it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  48% 41/86 [00:23\u003c00:24,  1.81it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  49% 42/86 [00:23\u003c00:25,  1.74it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  50% 43/86 [00:24\u003c00:24,  1.75it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  51% 44/86 [00:25\u003c00:23,  1.76it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  52% 45/86 [00:25\u003c00:23,  1.75it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  53% 46/86 [00:26\u003c00:24,  1.61it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  55% 47/86 [00:26\u003c00:24,  1.61it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  56% 48/86 [00:27\u003c00:23,  1.63it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  57% 49/86 [00:28\u003c00:22,  1.67it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  58% 50/86 [00:28\u003c00:22,  1.63it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  59% 51/86 [00:29\u003c00:21,  1.66it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  60% 52/86 [00:29\u003c00:20,  1.69it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  62% 53/86 [00:30\u003c00:22,  1.48it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  63% 54/86 [00:31\u003c00:22,  1.42it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  64% 55/86 [00:32\u003c00:22,  1.39it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  65% 56/86 [00:33\u003c00:22,  1.31it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  66% 57/86 [00:34\u003c00:22,  1.26it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  67% 58/86 [00:34\u003c00:23,  1.19it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  69% 59/86 [00:35\u003c00:23,  1.16it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  70% 60/86 [00:36\u003c00:21,  1.22it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  71% 61/86 [00:37\u003c00:18,  1.34it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  72% 62/86 [00:37\u003c00:17,  1.36it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  73% 63/86 [00:38\u003c00:15,  1.46it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  74% 64/86 [00:39\u003c00:14,  1.49it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  76% 65/86 [00:39\u003c00:13,  1.53it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  77% 66/86 [00:40\u003c00:12,  1.58it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  78% 67/86 [00:40\u003c00:11,  1.61it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  79% 68/86 [00:41\u003c00:10,  1.70it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  80% 69/86 [00:41\u003c00:09,  1.73it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  81% 70/86 [00:42\u003c00:10,  1.58it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  83% 71/86 [00:43\u003c00:09,  1.56it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  84% 72/86 [00:44\u003c00:09,  1.54it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  85% 73/86 [00:44\u003c00:08,  1.56it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  86% 74/86 [00:45\u003c00:07,  1.57it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  87% 75/86 [00:45\u003c00:07,  1.54it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  88% 76/86 [00:46\u003c00:06,  1.48it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  90% 77/86 [00:47\u003c00:06,  1.37it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  91% 78/86 [00:48\u003c00:06,  1.27it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  92% 79/86 [00:49\u003c00:06,  1.08it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  93% 80/86 [00:50\u003c00:05,  1.09it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  94% 81/86 [00:51\u003c00:04,  1.05it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  95% 82/86 [00:52\u003c00:03,  1.01it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  97% 83/86 [00:53\u003c00:02,  1.05it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  98% 84/86 [00:54\u003c00:01,  1.14it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  99% 85/86 [00:55\u003c00:00,  1.16it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset: 100% 86/86 [00:55\u003c00:00,  1.18it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 09:09:46 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 4.935 | nll_loss 3.554 | ppl 11.74 | bleu 16.9 | wps 3703.2 | wpb 2386.9 | bsz 73.7 | num_updates 14000 | best_bleu 16.9\n","2024-10-24 09:09:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 14000 updates\n","2024-10-24 09:09:46 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_64_14000.pt\n","2024-10-24 09:09:46 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_64_14000.pt\n","2024-10-24 09:09:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulm/checkpoint_64_14000.pt (epoch 64 @ 14000 updates, score 16.9) (writing took 1.3231873559998348 seconds)\n","2024-10-24 09:09:48 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)\n","2024-10-24 09:09:48 | INFO | train | epoch 064 | loss 4.704 | nll_loss 3.433 | ppl 10.8 | wps 9183.4 | ups 2.87 | wpb 3195.3 | bsz 95.9 | num_updates 14016 | lr 8.01326e-05 | gnorm 1.219 | train_wall 17 | gb_free 14.1 | wall 1858\n","2024-10-24 09:09:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:09:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 065:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:09:49 | INFO | fairseq.trainer | begin training epoch 65\n","2024-10-24 09:09:49 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:10:09 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)\n","2024-10-24 09:10:09 | INFO | train | epoch 065 | loss 4.696 | nll_loss 3.423 | ppl 10.72 | wps 33973.6 | ups 10.63 | wpb 3195.3 | bsz 95.9 | num_updates 14235 | lr 7.95138e-05 | gnorm 1.235 | train_wall 19 | gb_free 14.1 | wall 1878\n","2024-10-24 09:10:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:10:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 066:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:10:09 | INFO | fairseq.trainer | begin training epoch 66\n","2024-10-24 09:10:09 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:10:28 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)\n","2024-10-24 09:10:28 | INFO | train | epoch 066 | loss 4.684 | nll_loss 3.409 | ppl 10.62 | wps 36972.4 | ups 11.57 | wpb 3195.3 | bsz 95.9 | num_updates 14454 | lr 7.89091e-05 | gnorm 1.244 | train_wall 17 | gb_free 14.1 | wall 1897\n","2024-10-24 09:10:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:10:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 067:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:10:28 | INFO | fairseq.trainer | begin training epoch 67\n","2024-10-24 09:10:28 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:10:47 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)\n","2024-10-24 09:10:47 | INFO | train | epoch 067 | loss 4.674 | nll_loss 3.397 | ppl 10.53 | wps 37195.1 | ups 11.64 | wpb 3195.3 | bsz 95.9 | num_updates 14673 | lr 7.8318e-05 | gnorm 1.246 | train_wall 17 | gb_free 14.2 | wall 1916\n","2024-10-24 09:10:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:10:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 068:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:10:47 | INFO | fairseq.trainer | begin training epoch 68\n","2024-10-24 09:10:47 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:11:05 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)\n","2024-10-24 09:11:05 | INFO | train | epoch 068 | loss 4.663 | nll_loss 3.384 | ppl 10.44 | wps 38278.3 | ups 11.98 | wpb 3195.3 | bsz 95.9 | num_updates 14892 | lr 7.774e-05 | gnorm 1.242 | train_wall 17 | gb_free 14.1 | wall 1934\n","2024-10-24 09:11:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:11:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 069:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:11:05 | INFO | fairseq.trainer | begin training epoch 69\n","2024-10-24 09:11:05 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:11:23 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)\n","2024-10-24 09:11:23 | INFO | train | epoch 069 | loss 4.651 | nll_loss 3.37 | ppl 10.34 | wps 38313.8 | ups 11.99 | wpb 3195.3 | bsz 95.9 | num_updates 15111 | lr 7.71746e-05 | gnorm 1.246 | train_wall 17 | gb_free 14.1 | wall 1952\n","2024-10-24 09:11:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:11:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 070:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:11:24 | INFO | fairseq.trainer | begin training epoch 70\n","2024-10-24 09:11:24 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:11:42 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)\n","2024-10-24 09:11:42 | INFO | train | epoch 070 | loss 4.64 | nll_loss 3.357 | ppl 10.24 | wps 38368.3 | ups 12.01 | wpb 3195.3 | bsz 95.9 | num_updates 15330 | lr 7.66214e-05 | gnorm 1.248 | train_wall 17 | gb_free 14.1 | wall 1971\n","2024-10-24 09:11:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:11:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 071:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:11:42 | INFO | fairseq.trainer | begin training epoch 71\n","2024-10-24 09:11:42 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:12:00 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)\n","2024-10-24 09:12:00 | INFO | train | epoch 071 | loss 4.627 | nll_loss 3.342 | ppl 10.14 | wps 37786.9 | ups 11.83 | wpb 3195.3 | bsz 95.9 | num_updates 15549 | lr 7.60799e-05 | gnorm 1.243 | train_wall 17 | gb_free 14.1 | wall 1989\n","2024-10-24 09:12:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:12:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 072:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:12:00 | INFO | fairseq.trainer | begin training epoch 72\n","2024-10-24 09:12:00 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:12:19 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)\n","2024-10-24 09:12:19 | INFO | train | epoch 072 | loss 4.619 | nll_loss 3.333 | ppl 10.07 | wps 36933.1 | ups 11.56 | wpb 3195.3 | bsz 95.9 | num_updates 15768 | lr 7.55497e-05 | gnorm 1.244 | train_wall 17 | gb_free 14.1 | wall 2008\n","2024-10-24 09:12:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:12:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 073:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:12:19 | INFO | fairseq.trainer | begin training epoch 73\n","2024-10-24 09:12:19 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:12:37 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)\n","2024-10-24 09:12:37 | INFO | train | epoch 073 | loss 4.609 | nll_loss 3.321 | ppl 9.99 | wps 38332.1 | ups 12 | wpb 3195.3 | bsz 95.9 | num_updates 15987 | lr 7.50305e-05 | gnorm 1.243 | train_wall 17 | gb_free 14.1 | wall 2026\n","2024-10-24 09:12:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:12:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 074:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:12:38 | INFO | fairseq.trainer | begin training epoch 74\n","2024-10-24 09:12:38 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 074:   5% 11/219 [00:01\u003c00:17, 11.62it/s]2024-10-24 09:12:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 09:12:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 074 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:   1% 1/86 [00:00\u003c00:50,  1.69it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:   2% 2/86 [00:00\u003c00:40,  2.10it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:   3% 3/86 [00:01\u003c00:40,  2.05it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:   5% 4/86 [00:01\u003c00:34,  2.37it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:   6% 5/86 [00:02\u003c00:31,  2.56it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:   7% 6/86 [00:02\u003c00:31,  2.58it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:   8% 7/86 [00:02\u003c00:30,  2.56it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:   9% 8/86 [00:03\u003c00:33,  2.35it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  10% 9/86 [00:03\u003c00:30,  2.51it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  12% 10/86 [00:04\u003c00:29,  2.60it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  13% 11/86 [00:04\u003c00:31,  2.42it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  14% 12/86 [00:05\u003c00:31,  2.36it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  15% 13/86 [00:05\u003c00:29,  2.44it/s]\u001b[A2024-10-24 09:12:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:12:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:12:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 074 | valid on 'valid' subset:  16% 14/86 [00:05\u003c00:31,  2.32it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  17% 15/86 [00:06\u003c00:29,  2.39it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  19% 16/86 [00:07\u003c00:35,  1.96it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  20% 17/86 [00:07\u003c00:36,  1.91it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  21% 18/86 [00:08\u003c00:41,  1.63it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  22% 19/86 [00:08\u003c00:38,  1.76it/s]\u001b[A2024-10-24 09:12:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:12:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:12:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 074 | valid on 'valid' subset:  23% 20/86 [00:09\u003c00:42,  1.57it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  24% 21/86 [00:10\u003c00:41,  1.55it/s]\u001b[A2024-10-24 09:12:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:12:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:12:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 074 | valid on 'valid' subset:  26% 22/86 [00:11\u003c00:43,  1.47it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  27% 23/86 [00:11\u003c00:44,  1.40it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  28% 24/86 [00:12\u003c00:40,  1.52it/s]\u001b[A2024-10-24 09:12:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:12:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:12:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 074 | valid on 'valid' subset:  29% 25/86 [00:12\u003c00:38,  1.57it/s]\u001b[A\n","epoch 074:   5% 11/219 [00:15\u003c00:17, 11.62it/s, loss=4.617, nll_loss=3.33, ppl=10.05, wps=35557.8, ups=11.23, wpb=3167, bsz=94, num_updates=16000, lr=7.5e-05, gnorm=1.252, train_wall=8, gb_free=14.2, wall=2028]2024-10-24 09:12:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:12:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:12:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 074 | valid on 'valid' subset:  31% 27/86 [00:14\u003c00:36,  1.61it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  33% 28/86 [00:14\u003c00:36,  1.59it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  34% 29/86 [00:15\u003c00:33,  1.71it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  35% 30/86 [00:15\u003c00:33,  1.65it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  36% 31/86 [00:16\u003c00:34,  1.59it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  37% 32/86 [00:17\u003c00:30,  1.78it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  38% 33/86 [00:17\u003c00:30,  1.73it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  40% 34/86 [00:18\u003c00:28,  1.82it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  41% 35/86 [00:18\u003c00:28,  1.76it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  42% 36/86 [00:19\u003c00:31,  1.59it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  43% 37/86 [00:20\u003c00:32,  1.49it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  44% 38/86 [00:21\u003c00:33,  1.44it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  45% 39/86 [00:21\u003c00:33,  1.42it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  47% 40/86 [00:22\u003c00:35,  1.31it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  48% 41/86 [00:23\u003c00:33,  1.34it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  49% 42/86 [00:24\u003c00:39,  1.12it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  50% 43/86 [00:25\u003c00:39,  1.10it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  51% 44/86 [00:26\u003c00:37,  1.12it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  52% 45/86 [00:27\u003c00:41,  1.01s/it]\u001b[A\n","epoch 074 | valid on 'valid' subset:  53% 46/86 [00:28\u003c00:41,  1.04s/it]\u001b[A\n","epoch 074 | valid on 'valid' subset:  55% 47/86 [00:29\u003c00:38,  1.02it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  56% 48/86 [00:30\u003c00:36,  1.05it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  57% 49/86 [00:31\u003c00:35,  1.05it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  58% 50/86 [00:32\u003c00:30,  1.18it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  59% 51/86 [00:32\u003c00:26,  1.30it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  60% 52/86 [00:33\u003c00:24,  1.39it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  62% 53/86 [00:33\u003c00:23,  1.42it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  63% 54/86 [00:34\u003c00:21,  1.50it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  64% 55/86 [00:35\u003c00:19,  1.55it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  65% 56/86 [00:35\u003c00:18,  1.60it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  66% 57/86 [00:36\u003c00:17,  1.64it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  67% 58/86 [00:36\u003c00:17,  1.58it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  69% 59/86 [00:37\u003c00:16,  1.62it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  70% 60/86 [00:38\u003c00:16,  1.61it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  71% 61/86 [00:38\u003c00:17,  1.45it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  72% 62/86 [00:39\u003c00:16,  1.49it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  73% 63/86 [00:40\u003c00:14,  1.55it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  74% 64/86 [00:40\u003c00:14,  1.51it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  76% 65/86 [00:41\u003c00:15,  1.36it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  77% 66/86 [00:42\u003c00:15,  1.32it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  78% 67/86 [00:43\u003c00:14,  1.28it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  79% 68/86 [00:44\u003c00:14,  1.27it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  80% 69/86 [00:45\u003c00:13,  1.23it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  81% 70/86 [00:46\u003c00:15,  1.05it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  83% 71/86 [00:47\u003c00:14,  1.05it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  84% 72/86 [00:48\u003c00:12,  1.16it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  85% 73/86 [00:48\u003c00:10,  1.24it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  86% 74/86 [00:49\u003c00:08,  1.35it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  87% 75/86 [00:49\u003c00:07,  1.39it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  88% 76/86 [00:50\u003c00:06,  1.47it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  90% 77/86 [00:51\u003c00:06,  1.47it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  91% 78/86 [00:51\u003c00:05,  1.44it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  92% 79/86 [00:52\u003c00:05,  1.35it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  93% 80/86 [00:53\u003c00:04,  1.36it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  94% 81/86 [00:54\u003c00:03,  1.34it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  95% 82/86 [00:55\u003c00:02,  1.35it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  97% 83/86 [00:55\u003c00:02,  1.26it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  98% 84/86 [00:56\u003c00:01,  1.21it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  99% 85/86 [00:57\u003c00:00,  1.09it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset: 100% 86/86 [00:59\u003c00:00,  1.04it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 09:13:38 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 4.874 | nll_loss 3.474 | ppl 11.11 | bleu 17.41 | wps 3493.9 | wpb 2386.9 | bsz 73.7 | num_updates 16000 | best_bleu 17.41\n","2024-10-24 09:13:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 16000 updates\n","2024-10-24 09:13:38 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_74_16000.pt\n","2024-10-24 09:13:38 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_74_16000.pt\n","2024-10-24 09:13:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulm/checkpoint_74_16000.pt (epoch 74 @ 16000 updates, score 17.41) (writing took 1.8191289039996263 seconds)\n","2024-10-24 09:13:58 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)\n","2024-10-24 09:13:58 | INFO | train | epoch 074 | loss 4.602 | nll_loss 3.313 | ppl 9.94 | wps 8645.5 | ups 2.71 | wpb 3195.3 | bsz 95.9 | num_updates 16206 | lr 7.45218e-05 | gnorm 1.265 | train_wall 18 | gb_free 14.1 | wall 2107\n","2024-10-24 09:13:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:13:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 075:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:13:59 | INFO | fairseq.trainer | begin training epoch 75\n","2024-10-24 09:13:59 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 075: 100% 218/219 [00:18\u003c00:00, 13.77it/s, loss=4.589, nll_loss=3.298, ppl=9.83, wps=35175.9, ups=11.29, wpb=3115.5, bsz=92.6, num_updates=16400, lr=7.40797e-05, gnorm=1.285, train_wall=8, gb_free=14.1, wall=2124]2024-10-24 09:14:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 09:14:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 075 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:   1% 1/86 [00:00\u003c00:52,  1.63it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:   2% 2/86 [00:01\u003c00:43,  1.94it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:   3% 3/86 [00:01\u003c00:42,  1.96it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:   5% 4/86 [00:02\u003c00:39,  2.06it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:   6% 5/86 [00:02\u003c00:48,  1.68it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:   7% 6/86 [00:03\u003c00:42,  1.90it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:   8% 7/86 [00:03\u003c00:38,  2.03it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:   9% 8/86 [00:04\u003c00:38,  2.05it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  10% 9/86 [00:04\u003c00:34,  2.22it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  12% 10/86 [00:04\u003c00:32,  2.36it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  13% 11/86 [00:05\u003c00:39,  1.90it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  14% 12/86 [00:06\u003c00:41,  1.79it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  15% 13/86 [00:06\u003c00:41,  1.76it/s]\u001b[A2024-10-24 09:14:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:14:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:14:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 075 | valid on 'valid' subset:  16% 14/86 [00:07\u003c00:43,  1.66it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  17% 15/86 [00:08\u003c00:42,  1.67it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  19% 16/86 [00:08\u003c00:47,  1.49it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  20% 17/86 [00:09\u003c00:44,  1.54it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  21% 18/86 [00:10\u003c00:53,  1.27it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  22% 19/86 [00:11\u003c00:47,  1.41it/s]\u001b[A2024-10-24 09:14:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:14:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:14:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 075 | valid on 'valid' subset:  23% 20/86 [00:11\u003c00:44,  1.49it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  24% 21/86 [00:12\u003c00:40,  1.61it/s]\u001b[A2024-10-24 09:14:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:14:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:14:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 075 | valid on 'valid' subset:  26% 22/86 [00:12\u003c00:38,  1.64it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  27% 23/86 [00:13\u003c00:37,  1.67it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  28% 24/86 [00:13\u003c00:34,  1.82it/s]\u001b[A2024-10-24 09:14:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:14:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:14:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 075 | valid on 'valid' subset:  29% 25/86 [00:14\u003c00:37,  1.64it/s]\u001b[A\n","epoch 075: 100% 218/219 [00:34\u003c00:00, 13.77it/s, loss=4.589, nll_loss=3.298, ppl=9.83, wps=35175.9, ups=11.29, wpb=3115.5, bsz=92.6, num_updates=16400, lr=7.40797e-05, gnorm=1.285, train_wall=8, gb_free=14.1, wall=2124]2024-10-24 09:14:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:14:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:14:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 075 | valid on 'valid' subset:  31% 27/86 [00:15\u003c00:35,  1.65it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  33% 28/86 [00:16\u003c00:34,  1.70it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  34% 29/86 [00:16\u003c00:31,  1.83it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  35% 30/86 [00:17\u003c00:36,  1.53it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  36% 31/86 [00:18\u003c00:32,  1.67it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  37% 32/86 [00:18\u003c00:29,  1.81it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  38% 33/86 [00:19\u003c00:30,  1.75it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  40% 34/86 [00:19\u003c00:28,  1.80it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  41% 35/86 [00:20\u003c00:31,  1.63it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  42% 36/86 [00:21\u003c00:37,  1.34it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  43% 37/86 [00:22\u003c00:38,  1.26it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  44% 38/86 [00:23\u003c00:37,  1.29it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  45% 39/86 [00:23\u003c00:35,  1.31it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  47% 40/86 [00:24\u003c00:36,  1.25it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  48% 41/86 [00:25\u003c00:34,  1.32it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  49% 42/86 [00:26\u003c00:37,  1.16it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  50% 43/86 [00:27\u003c00:35,  1.20it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  51% 44/86 [00:27\u003c00:31,  1.32it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  52% 45/86 [00:28\u003c00:29,  1.41it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  53% 46/86 [00:29\u003c00:28,  1.40it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  55% 47/86 [00:29\u003c00:26,  1.45it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  56% 48/86 [00:30\u003c00:25,  1.47it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  57% 49/86 [00:31\u003c00:25,  1.47it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  58% 50/86 [00:31\u003c00:23,  1.52it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  59% 51/86 [00:32\u003c00:22,  1.57it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  60% 52/86 [00:32\u003c00:21,  1.60it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  62% 53/86 [00:33\u003c00:21,  1.57it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  63% 54/86 [00:34\u003c00:20,  1.58it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  64% 55/86 [00:34\u003c00:18,  1.63it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  65% 56/86 [00:35\u003c00:18,  1.60it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  66% 57/86 [00:36\u003c00:17,  1.65it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  67% 58/86 [00:36\u003c00:18,  1.54it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  69% 59/86 [00:37\u003c00:18,  1.48it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  70% 60/86 [00:38\u003c00:19,  1.36it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  71% 61/86 [00:39\u003c00:19,  1.28it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  72% 62/86 [00:40\u003c00:19,  1.24it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  73% 63/86 [00:40\u003c00:18,  1.23it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  74% 64/86 [00:41\u003c00:18,  1.18it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  76% 65/86 [00:42\u003c00:18,  1.15it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  77% 66/86 [00:43\u003c00:16,  1.23it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  78% 67/86 [00:44\u003c00:14,  1.33it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  79% 68/86 [00:44\u003c00:12,  1.43it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  80% 69/86 [00:45\u003c00:11,  1.50it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  81% 70/86 [00:46\u003c00:11,  1.44it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  83% 71/86 [00:46\u003c00:10,  1.47it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  84% 72/86 [00:47\u003c00:09,  1.51it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  85% 73/86 [00:47\u003c00:08,  1.52it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  86% 74/86 [00:48\u003c00:07,  1.59it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  87% 75/86 [00:49\u003c00:06,  1.59it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  88% 76/86 [00:49\u003c00:06,  1.57it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  90% 77/86 [00:50\u003c00:05,  1.54it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  91% 78/86 [00:51\u003c00:05,  1.51it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  92% 79/86 [00:52\u003c00:05,  1.38it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  93% 80/86 [00:52\u003c00:04,  1.39it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  94% 81/86 [00:53\u003c00:04,  1.24it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  95% 82/86 [00:54\u003c00:03,  1.21it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  97% 83/86 [00:55\u003c00:02,  1.11it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  98% 84/86 [00:56\u003c00:01,  1.04it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  99% 85/86 [00:58\u003c00:01,  1.05s/it]\u001b[A\n","epoch 075 | valid on 'valid' subset: 100% 86/86 [00:58\u003c00:00,  1.00it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 09:15:16 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 4.854 | nll_loss 3.452 | ppl 10.94 | bleu 18.29 | wps 3501.3 | wpb 2386.9 | bsz 73.7 | num_updates 16425 | best_bleu 18.29\n","2024-10-24 09:15:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 16425 updates\n","2024-10-24 09:15:16 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_best.pt\n","2024-10-24 09:15:16 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_best.pt\n","2024-10-24 09:15:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulm/checkpoint_best.pt (epoch 75 @ 16425 updates, score 18.29) (writing took 0.9867996389994005 seconds)\n","2024-10-24 09:15:17 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)\n","2024-10-24 09:15:17 | INFO | train | epoch 075 | loss 4.591 | nll_loss 3.299 | ppl 9.85 | wps 8919.3 | ups 2.79 | wpb 3195.3 | bsz 95.9 | num_updates 16425 | lr 7.40233e-05 | gnorm 1.26 | train_wall 17 | gb_free 14.1 | wall 2186\n","2024-10-24 09:15:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:15:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 076:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:15:17 | INFO | fairseq.trainer | begin training epoch 76\n","2024-10-24 09:15:17 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:15:36 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)\n","2024-10-24 09:15:36 | INFO | train | epoch 076 | loss 4.583 | nll_loss 3.289 | ppl 9.78 | wps 35539.1 | ups 11.12 | wpb 3195.3 | bsz 95.9 | num_updates 16644 | lr 7.35347e-05 | gnorm 1.263 | train_wall 18 | gb_free 14.1 | wall 2206\n","2024-10-24 09:15:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:15:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 077:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:15:37 | INFO | fairseq.trainer | begin training epoch 77\n","2024-10-24 09:15:37 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:15:55 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)\n","2024-10-24 09:15:55 | INFO | train | epoch 077 | loss 4.574 | nll_loss 3.28 | ppl 9.71 | wps 37896.9 | ups 11.86 | wpb 3195.3 | bsz 95.9 | num_updates 16863 | lr 7.30557e-05 | gnorm 1.283 | train_wall 17 | gb_free 14.1 | wall 2224\n","2024-10-24 09:15:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:15:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 078:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:15:55 | INFO | fairseq.trainer | begin training epoch 78\n","2024-10-24 09:15:55 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:16:13 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)\n","2024-10-24 09:16:13 | INFO | train | epoch 078 | loss 4.563 | nll_loss 3.267 | ppl 9.63 | wps 37892.1 | ups 11.86 | wpb 3195.3 | bsz 95.9 | num_updates 17082 | lr 7.25858e-05 | gnorm 1.266 | train_wall 17 | gb_free 14.1 | wall 2243\n","2024-10-24 09:16:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:16:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 079:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:16:14 | INFO | fairseq.trainer | begin training epoch 79\n","2024-10-24 09:16:14 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:16:32 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)\n","2024-10-24 09:16:32 | INFO | train | epoch 079 | loss 4.554 | nll_loss 3.256 | ppl 9.55 | wps 37073.9 | ups 11.6 | wpb 3195.3 | bsz 95.9 | num_updates 17301 | lr 7.2125e-05 | gnorm 1.275 | train_wall 17 | gb_free 14.1 | wall 2261\n","2024-10-24 09:16:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:16:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 080:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:16:33 | INFO | fairseq.trainer | begin training epoch 80\n","2024-10-24 09:16:33 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:16:51 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)\n","2024-10-24 09:16:51 | INFO | train | epoch 080 | loss 4.543 | nll_loss 3.242 | ppl 9.46 | wps 37348.5 | ups 11.69 | wpb 3195.3 | bsz 95.9 | num_updates 17520 | lr 7.16728e-05 | gnorm 1.258 | train_wall 17 | gb_free 14.1 | wall 2280\n","2024-10-24 09:16:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:16:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 081:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:16:51 | INFO | fairseq.trainer | begin training epoch 81\n","2024-10-24 09:16:51 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:17:10 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)\n","2024-10-24 09:17:10 | INFO | train | epoch 081 | loss 4.532 | nll_loss 3.23 | ppl 9.38 | wps 37618.5 | ups 11.77 | wpb 3195.3 | bsz 95.9 | num_updates 17739 | lr 7.1229e-05 | gnorm 1.262 | train_wall 17 | gb_free 14.1 | wall 2299\n","2024-10-24 09:17:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:17:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 082:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:17:10 | INFO | fairseq.trainer | begin training epoch 82\n","2024-10-24 09:17:10 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:17:28 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)\n","2024-10-24 09:17:28 | INFO | train | epoch 082 | loss 4.526 | nll_loss 3.223 | ppl 9.34 | wps 37863.7 | ups 11.85 | wpb 3195.3 | bsz 95.9 | num_updates 17958 | lr 7.07933e-05 | gnorm 1.269 | train_wall 17 | gb_free 14.1 | wall 2317\n","2024-10-24 09:17:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:17:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 083:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:17:28 | INFO | fairseq.trainer | begin training epoch 83\n","2024-10-24 09:17:28 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 083:  18% 40/219 [00:03\u003c00:13, 13.44it/s]2024-10-24 09:17:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 09:17:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 083 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:   1% 1/86 [00:00\u003c00:53,  1.58it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:   2% 2/86 [00:01\u003c00:42,  1.98it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:   3% 3/86 [00:01\u003c00:39,  2.08it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:   5% 4/86 [00:01\u003c00:39,  2.10it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:   6% 5/86 [00:02\u003c00:39,  2.06it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:   7% 6/86 [00:02\u003c00:38,  2.06it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:   8% 7/86 [00:03\u003c00:41,  1.90it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:   9% 8/86 [00:04\u003c00:41,  1.86it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  10% 9/86 [00:04\u003c00:42,  1.80it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  12% 10/86 [00:05\u003c00:42,  1.79it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  13% 11/86 [00:06\u003c00:47,  1.58it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  14% 12/86 [00:06\u003c00:48,  1.53it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  15% 13/86 [00:07\u003c00:46,  1.56it/s]\u001b[A2024-10-24 09:17:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:17:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:17:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 083 | valid on 'valid' subset:  16% 14/86 [00:08\u003c00:46,  1.55it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  17% 15/86 [00:08\u003c00:41,  1.71it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  19% 16/86 [00:09\u003c00:41,  1.71it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  20% 17/86 [00:09\u003c00:36,  1.87it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  21% 18/86 [00:10\u003c00:37,  1.81it/s]\u001b[A\n","epoch 083:  18% 40/219 [00:14\u003c00:13, 13.44it/s, loss=4.545, nll_loss=3.244, ppl=9.48, wps=40258.1, ups=12.48, wpb=3226.4, bsz=92, num_updates=18000, lr=7.07107e-05, gnorm=1.229, train_wall=7, gb_free=14.1, wall=2321]2024-10-24 09:17:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:17:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:17:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 083 | valid on 'valid' subset:  23% 20/86 [00:11\u003c00:34,  1.90it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  24% 21/86 [00:11\u003c00:34,  1.91it/s]\u001b[A2024-10-24 09:17:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:17:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:17:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 083 | valid on 'valid' subset:  26% 22/86 [00:12\u003c00:35,  1.81it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  27% 23/86 [00:12\u003c00:34,  1.80it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  28% 24/86 [00:13\u003c00:32,  1.90it/s]\u001b[A2024-10-24 09:17:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:17:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:17:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 083 | valid on 'valid' subset:  29% 25/86 [00:13\u003c00:34,  1.79it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  30% 26/86 [00:14\u003c00:31,  1.93it/s]\u001b[A2024-10-24 09:17:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:17:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:17:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 083 | valid on 'valid' subset:  31% 27/86 [00:14\u003c00:33,  1.76it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  33% 28/86 [00:15\u003c00:32,  1.79it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  34% 29/86 [00:15\u003c00:30,  1.89it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  35% 30/86 [00:16\u003c00:32,  1.74it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  36% 31/86 [00:17\u003c00:31,  1.75it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  37% 32/86 [00:17\u003c00:28,  1.90it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  38% 33/86 [00:18\u003c00:33,  1.58it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  40% 34/86 [00:19\u003c00:34,  1.49it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  41% 35/86 [00:20\u003c00:40,  1.25it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  42% 36/86 [00:21\u003c00:40,  1.23it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  43% 37/86 [00:22\u003c00:47,  1.02it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  44% 38/86 [00:23\u003c00:45,  1.06it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  45% 39/86 [00:23\u003c00:38,  1.23it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  47% 40/86 [00:24\u003c00:34,  1.32it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  48% 41/86 [00:25\u003c00:30,  1.47it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  49% 42/86 [00:25\u003c00:32,  1.34it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  50% 43/86 [00:26\u003c00:29,  1.45it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  51% 44/86 [00:27\u003c00:27,  1.52it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  52% 45/86 [00:27\u003c00:26,  1.56it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  53% 46/86 [00:28\u003c00:25,  1.56it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  55% 47/86 [00:28\u003c00:24,  1.59it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  56% 48/86 [00:29\u003c00:23,  1.60it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  57% 49/86 [00:30\u003c00:22,  1.63it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  58% 50/86 [00:30\u003c00:22,  1.61it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  59% 51/86 [00:31\u003c00:21,  1.65it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  60% 52/86 [00:32\u003c00:21,  1.56it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  62% 53/86 [00:32\u003c00:21,  1.54it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  63% 54/86 [00:33\u003c00:20,  1.58it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  64% 55/86 [00:34\u003c00:20,  1.51it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  65% 56/86 [00:34\u003c00:21,  1.39it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  66% 57/86 [00:35\u003c00:21,  1.38it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  67% 58/86 [00:36\u003c00:22,  1.23it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  69% 59/86 [00:37\u003c00:22,  1.20it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  70% 60/86 [00:38\u003c00:22,  1.15it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  71% 61/86 [00:39\u003c00:24,  1.04it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  72% 62/86 [00:40\u003c00:20,  1.15it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  73% 63/86 [00:40\u003c00:18,  1.27it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  74% 64/86 [00:41\u003c00:16,  1.33it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  76% 65/86 [00:42\u003c00:16,  1.25it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  77% 66/86 [00:43\u003c00:14,  1.37it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  78% 67/86 [00:43\u003c00:13,  1.45it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  79% 68/86 [00:44\u003c00:11,  1.54it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  80% 69/86 [00:44\u003c00:10,  1.60it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  81% 70/86 [00:45\u003c00:10,  1.47it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  83% 71/86 [00:46\u003c00:10,  1.48it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  84% 72/86 [00:46\u003c00:09,  1.50it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  85% 73/86 [00:47\u003c00:08,  1.49it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  86% 74/86 [00:48\u003c00:08,  1.49it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  87% 75/86 [00:48\u003c00:07,  1.49it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  88% 76/86 [00:49\u003c00:06,  1.54it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  90% 77/86 [00:50\u003c00:06,  1.43it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  91% 78/86 [00:51\u003c00:06,  1.31it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  92% 79/86 [00:52\u003c00:06,  1.12it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  93% 80/86 [00:53\u003c00:05,  1.06it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  94% 81/86 [00:54\u003c00:04,  1.00it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  95% 82/86 [00:55\u003c00:04,  1.02s/it]\u001b[A\n","epoch 083 | valid on 'valid' subset:  97% 83/86 [00:56\u003c00:02,  1.01it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  98% 84/86 [00:57\u003c00:01,  1.07it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  99% 85/86 [00:58\u003c00:00,  1.11it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset: 100% 86/86 [00:59\u003c00:00,  1.15it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 09:18:31 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 4.831 | nll_loss 3.42 | ppl 10.71 | bleu 18.86 | wps 3492 | wpb 2386.9 | bsz 73.7 | num_updates 18000 | best_bleu 18.86\n","2024-10-24 09:18:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 18000 updates\n","2024-10-24 09:18:31 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_83_18000.pt\n","2024-10-24 09:18:31 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_83_18000.pt\n","2024-10-24 09:18:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulm/checkpoint_83_18000.pt (epoch 83 @ 18000 updates, score 18.86) (writing took 1.2120059120006772 seconds)\n","2024-10-24 09:18:49 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)\n","2024-10-24 09:18:49 | INFO | train | epoch 083 | loss 4.518 | nll_loss 3.213 | ppl 9.27 | wps 8683.1 | ups 2.72 | wpb 3195.3 | bsz 95.9 | num_updates 18177 | lr 7.03656e-05 | gnorm 1.281 | train_wall 19 | gb_free 14 | wall 2398\n","2024-10-24 09:18:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:18:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 084:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:18:49 | INFO | fairseq.trainer | begin training epoch 84\n","2024-10-24 09:18:49 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:19:09 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)\n","2024-10-24 09:19:09 | INFO | train | epoch 084 | loss 4.511 | nll_loss 3.205 | ppl 9.22 | wps 34126.1 | ups 10.68 | wpb 3195.3 | bsz 95.9 | num_updates 18396 | lr 6.99455e-05 | gnorm 1.282 | train_wall 18 | gb_free 14.2 | wall 2418\n","2024-10-24 09:19:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:19:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 085:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:19:09 | INFO | fairseq.trainer | begin training epoch 85\n","2024-10-24 09:19:09 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:19:28 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)\n","2024-10-24 09:19:28 | INFO | train | epoch 085 | loss 4.501 | nll_loss 3.193 | ppl 9.15 | wps 38081.9 | ups 11.92 | wpb 3195.3 | bsz 95.9 | num_updates 18615 | lr 6.95328e-05 | gnorm 1.29 | train_wall 17 | gb_free 14 | wall 2437\n","2024-10-24 09:19:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:19:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 086:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:19:28 | INFO | fairseq.trainer | begin training epoch 86\n","2024-10-24 09:19:28 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:19:46 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)\n","2024-10-24 09:19:46 | INFO | train | epoch 086 | loss 4.495 | nll_loss 3.186 | ppl 9.1 | wps 37495.7 | ups 11.73 | wpb 3195.3 | bsz 95.9 | num_updates 18834 | lr 6.91274e-05 | gnorm 1.285 | train_wall 17 | gb_free 14.1 | wall 2455\n","2024-10-24 09:19:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:19:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 087:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:19:46 | INFO | fairseq.trainer | begin training epoch 87\n","2024-10-24 09:19:47 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:20:05 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)\n","2024-10-24 09:20:05 | INFO | train | epoch 087 | loss 4.488 | nll_loss 3.178 | ppl 9.05 | wps 36948.5 | ups 11.56 | wpb 3195.3 | bsz 95.9 | num_updates 19053 | lr 6.87289e-05 | gnorm 1.279 | train_wall 17 | gb_free 14.1 | wall 2474\n","2024-10-24 09:20:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:20:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 088:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:20:05 | INFO | fairseq.trainer | begin training epoch 88\n","2024-10-24 09:20:05 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:20:24 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)\n","2024-10-24 09:20:24 | INFO | train | epoch 088 | loss 4.476 | nll_loss 3.163 | ppl 8.96 | wps 37268.4 | ups 11.66 | wpb 3195.3 | bsz 95.9 | num_updates 19272 | lr 6.83373e-05 | gnorm 1.267 | train_wall 17 | gb_free 14 | wall 2493\n","2024-10-24 09:20:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:20:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 089:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:20:24 | INFO | fairseq.trainer | begin training epoch 89\n","2024-10-24 09:20:24 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:20:42 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)\n","2024-10-24 09:20:42 | INFO | train | epoch 089 | loss 4.472 | nll_loss 3.158 | ppl 8.93 | wps 38098.5 | ups 11.92 | wpb 3195.3 | bsz 95.9 | num_updates 19491 | lr 6.79523e-05 | gnorm 1.297 | train_wall 17 | gb_free 14.1 | wall 2511\n","2024-10-24 09:20:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:20:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 090:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:20:43 | INFO | fairseq.trainer | begin training epoch 90\n","2024-10-24 09:20:43 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:21:01 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)\n","2024-10-24 09:21:01 | INFO | train | epoch 090 | loss 4.462 | nll_loss 3.147 | ppl 8.86 | wps 37637.5 | ups 11.78 | wpb 3195.3 | bsz 95.9 | num_updates 19710 | lr 6.75737e-05 | gnorm 1.277 | train_wall 17 | gb_free 14.2 | wall 2530\n","2024-10-24 09:21:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:21:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 091:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:21:01 | INFO | fairseq.trainer | begin training epoch 91\n","2024-10-24 09:21:01 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:21:19 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)\n","2024-10-24 09:21:19 | INFO | train | epoch 091 | loss 4.454 | nll_loss 3.138 | ppl 8.8 | wps 37854.2 | ups 11.85 | wpb 3195.3 | bsz 95.9 | num_updates 19929 | lr 6.72014e-05 | gnorm 1.281 | train_wall 17 | gb_free 14.1 | wall 2549\n","2024-10-24 09:21:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:21:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 092:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:21:20 | INFO | fairseq.trainer | begin training epoch 92\n","2024-10-24 09:21:20 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 092:  32% 70/219 [00:06\u003c00:14, 10.30it/s]2024-10-24 09:21:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 09:21:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 092 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:   1% 1/86 [00:00\u003c01:12,  1.17it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:   2% 2/86 [00:01\u003c00:52,  1.60it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:   3% 3/86 [00:01\u003c00:46,  1.78it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:   5% 4/86 [00:02\u003c00:42,  1.94it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:   6% 5/86 [00:02\u003c00:37,  2.15it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:   7% 6/86 [00:03\u003c00:35,  2.24it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:   8% 7/86 [00:03\u003c00:36,  2.19it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:   9% 8/86 [00:03\u003c00:35,  2.18it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  10% 9/86 [00:04\u003c00:34,  2.23it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  12% 10/86 [00:04\u003c00:32,  2.35it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  13% 11/86 [00:05\u003c00:33,  2.24it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  14% 12/86 [00:05\u003c00:33,  2.22it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  15% 13/86 [00:06\u003c00:32,  2.25it/s]\u001b[A2024-10-24 09:21:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:21:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:21:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 092 | valid on 'valid' subset:  16% 14/86 [00:06\u003c00:33,  2.16it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  17% 15/86 [00:07\u003c00:31,  2.22it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  19% 16/86 [00:07\u003c00:34,  2.03it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  20% 17/86 [00:08\u003c00:33,  2.06it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  21% 18/86 [00:08\u003c00:36,  1.86it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  22% 19/86 [00:09\u003c00:33,  1.99it/s]\u001b[A2024-10-24 09:21:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:21:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:21:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 092 | valid on 'valid' subset:  23% 20/86 [00:09\u003c00:34,  1.92it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  24% 21/86 [00:10\u003c00:32,  1.99it/s]\u001b[A2024-10-24 09:21:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:21:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:21:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 092 | valid on 'valid' subset:  26% 22/86 [00:10\u003c00:33,  1.89it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  27% 23/86 [00:11\u003c00:36,  1.72it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  28% 24/86 [00:12\u003c00:36,  1.70it/s]\u001b[A2024-10-24 09:21:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:21:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:21:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 092 | valid on 'valid' subset:  29% 25/86 [00:13\u003c00:41,  1.47it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  30% 26/86 [00:13\u003c00:39,  1.51it/s]\u001b[A2024-10-24 09:21:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:21:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:21:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 092 | valid on 'valid' subset:  31% 27/86 [00:14\u003c00:45,  1.28it/s]\u001b[A\n","epoch 092:  32% 70/219 [00:23\u003c00:14, 10.30it/s, loss=4.415, nll_loss=3.093, ppl=8.53, wps=35538.1, ups=11.1, wpb=3200.3, bsz=102.9, num_updates=20000, lr=6.7082e-05, gnorm=1.293, train_wall=8, gb_free=14.1, wall=2555]\n","epoch 092 | valid on 'valid' subset:  34% 29/86 [00:16\u003c00:43,  1.31it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  35% 30/86 [00:17\u003c00:49,  1.14it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  36% 31/86 [00:17\u003c00:43,  1.27it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  37% 32/86 [00:18\u003c00:35,  1.50it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  38% 33/86 [00:18\u003c00:35,  1.51it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  40% 34/86 [00:19\u003c00:33,  1.57it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  41% 35/86 [00:20\u003c00:34,  1.48it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  42% 36/86 [00:21\u003c00:34,  1.43it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  43% 37/86 [00:21\u003c00:33,  1.48it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  44% 38/86 [00:22\u003c00:30,  1.57it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  45% 39/86 [00:22\u003c00:28,  1.66it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  47% 40/86 [00:23\u003c00:27,  1.66it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  48% 41/86 [00:23\u003c00:25,  1.76it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  49% 42/86 [00:24\u003c00:30,  1.44it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  50% 43/86 [00:25\u003c00:28,  1.49it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  51% 44/86 [00:26\u003c00:27,  1.51it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  52% 45/86 [00:26\u003c00:26,  1.56it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  53% 46/86 [00:27\u003c00:26,  1.53it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  55% 47/86 [00:28\u003c00:33,  1.16it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  56% 48/86 [00:29\u003c00:32,  1.15it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  57% 49/86 [00:30\u003c00:32,  1.14it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  58% 50/86 [00:31\u003c00:32,  1.11it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  59% 51/86 [00:32\u003c00:30,  1.14it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  60% 52/86 [00:33\u003c00:30,  1.11it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  62% 53/86 [00:33\u003c00:28,  1.18it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  63% 54/86 [00:34\u003c00:25,  1.28it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  64% 55/86 [00:35\u003c00:22,  1.36it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  65% 56/86 [00:35\u003c00:21,  1.42it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  66% 57/86 [00:36\u003c00:19,  1.51it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  67% 58/86 [00:37\u003c00:18,  1.51it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  69% 59/86 [00:37\u003c00:17,  1.57it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  70% 60/86 [00:38\u003c00:16,  1.55it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  71% 61/86 [00:39\u003c00:17,  1.44it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  72% 62/86 [00:39\u003c00:16,  1.47it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  73% 63/86 [00:40\u003c00:14,  1.54it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  74% 64/86 [00:41\u003c00:14,  1.51it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  76% 65/86 [00:41\u003c00:14,  1.43it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  77% 66/86 [00:42\u003c00:13,  1.48it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  78% 67/86 [00:43\u003c00:12,  1.49it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  79% 68/86 [00:43\u003c00:12,  1.48it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  80% 69/86 [00:44\u003c00:12,  1.40it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  81% 70/86 [00:45\u003c00:13,  1.20it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  83% 71/86 [00:46\u003c00:13,  1.13it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  84% 72/86 [00:47\u003c00:12,  1.09it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  85% 73/86 [00:48\u003c00:12,  1.06it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  86% 74/86 [00:49\u003c00:11,  1.06it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  87% 75/86 [00:50\u003c00:09,  1.14it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  88% 76/86 [00:50\u003c00:07,  1.28it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  90% 77/86 [00:51\u003c00:06,  1.33it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  91% 78/86 [00:52\u003c00:05,  1.38it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  92% 79/86 [00:53\u003c00:05,  1.31it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  93% 80/86 [00:53\u003c00:04,  1.35it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  94% 81/86 [00:54\u003c00:03,  1.30it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  95% 82/86 [00:55\u003c00:03,  1.33it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  97% 83/86 [00:56\u003c00:02,  1.29it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  98% 84/86 [00:56\u003c00:01,  1.31it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  99% 85/86 [00:57\u003c00:00,  1.24it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset: 100% 86/86 [00:58\u003c00:00,  1.25it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 09:22:25 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 4.779 | nll_loss 3.355 | ppl 10.23 | bleu 19.63 | wps 3534.6 | wpb 2386.9 | bsz 73.7 | num_updates 20000 | best_bleu 19.63\n","2024-10-24 09:22:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 20000 updates\n","2024-10-24 09:22:25 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_92_20000.pt\n","2024-10-24 09:22:25 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_92_20000.pt\n","2024-10-24 09:22:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulm/checkpoint_92_20000.pt (epoch 92 @ 20000 updates, score 19.63) (writing took 1.2175195999998323 seconds)\n","2024-10-24 09:22:40 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)\n","2024-10-24 09:22:40 | INFO | train | epoch 092 | loss 4.45 | nll_loss 3.133 | ppl 8.77 | wps 8693.3 | ups 2.72 | wpb 3195.3 | bsz 95.9 | num_updates 20148 | lr 6.68352e-05 | gnorm 1.31 | train_wall 18 | gb_free 14.1 | wall 2629\n","2024-10-24 09:22:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:22:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 093:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:22:40 | INFO | fairseq.trainer | begin training epoch 93\n","2024-10-24 09:22:40 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:22:59 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)\n","2024-10-24 09:22:59 | INFO | train | epoch 093 | loss 4.438 | nll_loss 3.119 | ppl 8.69 | wps 37334.9 | ups 11.68 | wpb 3195.3 | bsz 95.9 | num_updates 20367 | lr 6.64749e-05 | gnorm 1.281 | train_wall 17 | gb_free 14 | wall 2648\n","2024-10-24 09:22:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:22:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 094:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:22:59 | INFO | fairseq.trainer | begin training epoch 94\n","2024-10-24 09:22:59 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:23:18 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)\n","2024-10-24 09:23:18 | INFO | train | epoch 094 | loss 4.433 | nll_loss 3.113 | ppl 8.65 | wps 37080.2 | ups 11.6 | wpb 3195.3 | bsz 95.9 | num_updates 20586 | lr 6.61204e-05 | gnorm 1.288 | train_wall 17 | gb_free 14.1 | wall 2667\n","2024-10-24 09:23:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:23:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 095:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:23:18 | INFO | fairseq.trainer | begin training epoch 95\n","2024-10-24 09:23:18 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:23:37 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)\n","2024-10-24 09:23:37 | INFO | train | epoch 095 | loss 4.428 | nll_loss 3.107 | ppl 8.62 | wps 35688.8 | ups 11.17 | wpb 3195.3 | bsz 95.9 | num_updates 20805 | lr 6.57714e-05 | gnorm 1.301 | train_wall 18 | gb_free 14.1 | wall 2686\n","2024-10-24 09:23:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:23:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 096:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:23:37 | INFO | fairseq.trainer | begin training epoch 96\n","2024-10-24 09:23:37 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:23:56 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)\n","2024-10-24 09:23:56 | INFO | train | epoch 096 | loss 4.422 | nll_loss 3.1 | ppl 8.57 | wps 37137.6 | ups 11.62 | wpb 3195.3 | bsz 95.9 | num_updates 21024 | lr 6.5428e-05 | gnorm 1.314 | train_wall 17 | gb_free 14.1 | wall 2705\n","2024-10-24 09:23:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:23:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 097:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:23:56 | INFO | fairseq.trainer | begin training epoch 97\n","2024-10-24 09:23:56 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:24:15 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)\n","2024-10-24 09:24:15 | INFO | train | epoch 097 | loss 4.412 | nll_loss 3.087 | ppl 8.5 | wps 37484.7 | ups 11.73 | wpb 3195.3 | bsz 95.9 | num_updates 21243 | lr 6.50899e-05 | gnorm 1.315 | train_wall 17 | gb_free 14.1 | wall 2724\n","2024-10-24 09:24:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:24:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 098:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:24:15 | INFO | fairseq.trainer | begin training epoch 98\n","2024-10-24 09:24:15 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:24:34 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)\n","2024-10-24 09:24:34 | INFO | train | epoch 098 | loss 4.406 | nll_loss 3.081 | ppl 8.46 | wps 35460.9 | ups 11.1 | wpb 3195.3 | bsz 95.9 | num_updates 21462 | lr 6.47569e-05 | gnorm 1.299 | train_wall 17 | gb_free 14.1 | wall 2744\n","2024-10-24 09:24:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:24:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 099:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:24:35 | INFO | fairseq.trainer | begin training epoch 99\n","2024-10-24 09:24:35 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-10-24 09:24:53 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)\n","2024-10-24 09:24:53 | INFO | train | epoch 099 | loss 4.398 | nll_loss 3.072 | ppl 8.41 | wps 38117.8 | ups 11.93 | wpb 3195.3 | bsz 95.9 | num_updates 21681 | lr 6.4429e-05 | gnorm 1.293 | train_wall 17 | gb_free 14.1 | wall 2762\n","2024-10-24 09:24:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:24:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 100:   0% 0/219 [00:00\u003c?, ?it/s]2024-10-24 09:24:53 | INFO | fairseq.trainer | begin training epoch 100\n","2024-10-24 09:24:53 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 100: 100% 218/219 [00:18\u003c00:00, 12.33it/s, loss=4.392, nll_loss=3.064, ppl=8.36, wps=36801.3, ups=11.46, wpb=3212.1, bsz=94.9, num_updates=21800, lr=6.42529e-05, gnorm=1.276, train_wall=8, gb_free=14.1, wall=2773]2024-10-24 09:25:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 09:25:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 100 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:   1% 1/86 [00:00\u003c01:08,  1.24it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:   2% 2/86 [00:01\u003c00:58,  1.45it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:   3% 3/86 [00:02\u003c00:58,  1.43it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:   5% 4/86 [00:02\u003c00:57,  1.43it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:   6% 5/86 [00:03\u003c00:50,  1.61it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:   7% 6/86 [00:03\u003c00:46,  1.72it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:   8% 7/86 [00:04\u003c00:43,  1.83it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:   9% 8/86 [00:04\u003c00:38,  2.01it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  10% 9/86 [00:05\u003c00:34,  2.21it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  12% 10/86 [00:05\u003c00:32,  2.32it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  13% 11/86 [00:05\u003c00:33,  2.27it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  14% 12/86 [00:06\u003c00:31,  2.32it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  15% 13/86 [00:06\u003c00:30,  2.38it/s]\u001b[A2024-10-24 09:25:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:25:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:25:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 100 | valid on 'valid' subset:  16% 14/86 [00:07\u003c00:31,  2.27it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  17% 15/86 [00:07\u003c00:30,  2.35it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  19% 16/86 [00:08\u003c00:31,  2.25it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  20% 17/86 [00:08\u003c00:30,  2.28it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  21% 18/86 [00:09\u003c00:34,  1.99it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  22% 19/86 [00:09\u003c00:31,  2.14it/s]\u001b[A2024-10-24 09:25:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:25:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:25:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 100 | valid on 'valid' subset:  23% 20/86 [00:10\u003c00:32,  2.01it/s]\u001b[A\n","epoch 100: 100% 218/219 [00:29\u003c00:00, 12.33it/s, loss=4.38, nll_loss=3.051, ppl=8.29, wps=39925.6, ups=12.59, wpb=3171.2, bsz=97.7, num_updates=21900, lr=6.41061e-05, gnorm=1.298, train_wall=7, gb_free=14.1, wall=2781] 2024-10-24 09:25:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:25:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:25:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 100 | valid on 'valid' subset:  26% 22/86 [00:11\u003c00:33,  1.93it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  27% 23/86 [00:11\u003c00:33,  1.86it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  28% 24/86 [00:12\u003c00:30,  2.00it/s]\u001b[A2024-10-24 09:25:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:25:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:25:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 100 | valid on 'valid' subset:  29% 25/86 [00:12\u003c00:32,  1.89it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  30% 26/86 [00:13\u003c00:29,  2.02it/s]\u001b[A2024-10-24 09:25:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 09:25:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 09:25:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 100 | valid on 'valid' subset:  31% 27/86 [00:13\u003c00:32,  1.81it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  33% 28/86 [00:14\u003c00:35,  1.62it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  34% 29/86 [00:15\u003c00:35,  1.61it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  35% 30/86 [00:16\u003c00:40,  1.38it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  36% 31/86 [00:16\u003c00:40,  1.34it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  37% 32/86 [00:17\u003c00:38,  1.41it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  38% 33/86 [00:18\u003c00:41,  1.29it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  40% 34/86 [00:19\u003c00:41,  1.25it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  41% 35/86 [00:20\u003c00:42,  1.20it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  42% 36/86 [00:20\u003c00:38,  1.29it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  43% 37/86 [00:21\u003c00:39,  1.23it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  44% 38/86 [00:22\u003c00:35,  1.35it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  45% 39/86 [00:22\u003c00:31,  1.50it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  47% 40/86 [00:23\u003c00:29,  1.56it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  48% 41/86 [00:24\u003c00:27,  1.67it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  49% 42/86 [00:24\u003c00:28,  1.57it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  50% 43/86 [00:25\u003c00:26,  1.63it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  51% 44/86 [00:25\u003c00:25,  1.64it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  52% 45/86 [00:26\u003c00:24,  1.66it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  53% 46/86 [00:27\u003c00:24,  1.63it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  55% 47/86 [00:27\u003c00:23,  1.64it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  56% 48/86 [00:28\u003c00:23,  1.63it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  57% 49/86 [00:28\u003c00:22,  1.66it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  58% 50/86 [00:29\u003c00:21,  1.66it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  59% 51/86 [00:30\u003c00:21,  1.60it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  60% 52/86 [00:31\u003c00:23,  1.43it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  62% 53/86 [00:31\u003c00:25,  1.31it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  63% 54/86 [00:32\u003c00:25,  1.24it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  64% 55/86 [00:33\u003c00:24,  1.24it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  65% 56/86 [00:34\u003c00:24,  1.23it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  66% 57/86 [00:35\u003c00:23,  1.23it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  67% 58/86 [00:36\u003c00:21,  1.28it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  69% 59/86 [00:36\u003c00:19,  1.40it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  70% 60/86 [00:37\u003c00:17,  1.47it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  71% 61/86 [00:37\u003c00:16,  1.49it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  72% 62/86 [00:38\u003c00:16,  1.49it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  73% 63/86 [00:39\u003c00:14,  1.56it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  74% 64/86 [00:39\u003c00:14,  1.53it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  76% 65/86 [00:40\u003c00:13,  1.54it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  77% 66/86 [00:40\u003c00:12,  1.59it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  78% 67/86 [00:41\u003c00:12,  1.57it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  79% 68/86 [00:42\u003c00:10,  1.67it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  80% 69/86 [00:42\u003c00:10,  1.65it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  81% 70/86 [00:43\u003c00:11,  1.45it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  83% 71/86 [00:44\u003c00:09,  1.54it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  84% 72/86 [00:44\u003c00:08,  1.56it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  85% 73/86 [00:45\u003c00:08,  1.51it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  86% 74/86 [00:46\u003c00:08,  1.39it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  87% 75/86 [00:47\u003c00:09,  1.21it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  88% 76/86 [00:48\u003c00:08,  1.23it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  90% 77/86 [00:49\u003c00:07,  1.19it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  91% 78/86 [00:50\u003c00:06,  1.15it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  92% 79/86 [00:51\u003c00:06,  1.01it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  93% 80/86 [00:52\u003c00:05,  1.08it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  94% 81/86 [00:52\u003c00:04,  1.17it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  95% 82/86 [00:53\u003c00:03,  1.19it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  97% 83/86 [00:54\u003c00:02,  1.19it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  98% 84/86 [00:55\u003c00:01,  1.23it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  99% 85/86 [00:56\u003c00:00,  1.20it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset: 100% 86/86 [00:56\u003c00:00,  1.22it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 09:26:08 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 4.758 | nll_loss 3.324 | ppl 10.02 | bleu 19.84 | wps 3639.3 | wpb 2386.9 | bsz 73.7 | num_updates 21900 | best_bleu 19.84\n","2024-10-24 09:26:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 21900 updates\n","2024-10-24 09:26:08 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_best.pt\n","2024-10-24 09:26:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulm/checkpoints-ulm/checkpoint_best.pt\n","2024-10-24 09:26:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulm/checkpoint_best.pt (epoch 100 @ 21900 updates, score 19.84) (writing took 0.8367798490007772 seconds)\n","2024-10-24 09:26:09 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)\n","2024-10-24 09:26:09 | INFO | train | epoch 100 | loss 4.389 | nll_loss 3.061 | ppl 8.34 | wps 9147.1 | ups 2.86 | wpb 3195.3 | bsz 95.9 | num_updates 21900 | lr 6.41061e-05 | gnorm 1.289 | train_wall 17 | gb_free 14.1 | wall 2838\n","2024-10-24 09:26:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:26:09 | INFO | fairseq_cli.train | done training in 2836.9 seconds\n"]}],"source":["!fairseq-train data-bin \\\n","--arch transformer \\\n","--activation-fn relu \\\n","--share-decoder-input-output-embed \\\n","--share-all-embeddings \\\n","--encoder-layers 3 \\\n","--encoder-attention-heads 4 \\\n","--encoder-embed-dim 256 \\\n","--encoder-ffn-embed-dim 1024 \\\n","--decoder-layers 3 \\\n","--decoder-attention-heads 4 \\\n","--decoder-embed-dim 256 \\\n","--decoder-ffn-embed-dim 1024 \\\n","--dropout 0.25 \\\n","--seed 2024 \\\n","--optimizer 'adam' \\\n","--adam-betas '(0.9, 0.999)' \\\n","--lr-scheduler 'inverse_sqrt' \\\n","--patience 5 \\\n","--warmup-updates 1000 \\\n","--criterion 'label_smoothed_cross_entropy' \\\n","--label-smoothing 0.1 \\\n","--lr 0.0003 \\\n","--weight-decay 0.0 \\\n","--max-tokens 4096 \\\n","--max-tokens-valid 3600 \\\n","--required-batch-size-multiple 1 \\\n","--best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n","--max-epoch 100 \\\n","--validate-interval 25 \\\n","--save-interval 25 \\\n","--validate-interval-updates 2000 \\\n","--save-interval-updates 2000 \\\n","--log-interval 100 \\\n","--curriculum 0 \\\n","--no-epoch-checkpoints \\\n","--eval-bleu \\\n","--eval-bleu-args '{\"beam\": 5, \"max_len_a\": 0, \"max_len_b\": 100, \"len_pen\": 1}' \\\n","--eval-bleu-detok space \\\n","--eval-bleu-remove-bpe sentencepiece \\\n","--save-dir checkpoints-ulm \\\n","--ddp-backend=no_c10d \\\n","--wandb-project 'fairseq-standard-subword-tok-eng-to-nso'"]},{"cell_type":"markdown","metadata":{"id":"MUb4wZ_-lfRO"},"source":["## Training NMT with BPE-Dropout"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kYVppsHKlkoS"},"outputs":[],"source":["# change working directory\n","os.chdir(f'/content/drive/MyDrive/Research/eng-to-{target_code}/bpeDROP')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Q_c14yNHl1ij","outputId":"0862a630-4318-4596-f113-5cee2cddf755"},"outputs":[{"name":"stdout","output_type":"stream","text":["2024-10-24 10:42:03.129814: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-10-24 10:42:03.150781: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-10-24 10:42:03.157446: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-10-24 10:42:03.173710: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-10-24 10:42:04.264425: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-10-24 10:42:05 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n","2024-10-24 10:42:13 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': 'fairseq-standard-subword-tok-eng-to-nso', 'azureml_logging': False, 'seed': 2024, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 2000, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3600, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 4, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0003], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints-bpeDROP', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 5, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project='fairseq-standard-subword-tok-eng-to-nso', azureml_logging=False, seed=2024, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, batch_size=None, required_batch_size_multiple=1, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=2000, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid='3600', batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=4, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0003], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='checkpoints-bpeDROP', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='bleu', maximize_best_checkpoint_metric=True, patience=5, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='data-bin-25', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 0, \"max_len_b\": 100, \"len_pen\": 1}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe='@@ ', eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.999)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=1000, warmup_init_lr=-1, pad=1, eos=2, unk=3, activation_fn='relu', share_decoder_input_output_embed=True, share_all_embeddings=True, encoder_layers=3, encoder_attention_heads=4, encoder_embed_dim=256, encoder_ffn_embed_dim=1024, decoder_layers=3, decoder_attention_heads=4, decoder_embed_dim=256, decoder_ffn_embed_dim=1024, dropout=0.25, no_seed_provided=False, encoder_embed_path=None, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=256, decoder_input_dim=256, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': 'data-bin-25', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 0, \"max_len_b\": 100, \"len_pen\": 1}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0003]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 1000, 'warmup_init_lr': -1.0, 'lr': [0.0003]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n","2024-10-24 10:42:13 | INFO | fairseq.tasks.translation | [eng] dictionary: 4160 types\n","2024-10-24 10:42:13 | INFO | fairseq.tasks.translation | [nso] dictionary: 4160 types\n","2024-10-24 10:42:14 | INFO | fairseq_cli.train | TransformerModel(\n","  (encoder): TransformerEncoderBase(\n","    (dropout_module): FairseqDropout()\n","    (embed_tokens): Embedding(4160, 256, padding_idx=1)\n","    (embed_positions): SinusoidalPositionalEmbedding()\n","    (layers): ModuleList(\n","      (0-2): 3 x TransformerEncoderLayerBase(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (decoder): TransformerDecoderBase(\n","    (dropout_module): FairseqDropout()\n","    (embed_tokens): Embedding(4160, 256, padding_idx=1)\n","    (embed_positions): SinusoidalPositionalEmbedding()\n","    (layers): ModuleList(\n","      (0-2): 3 x TransformerDecoderLayerBase(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (output_projection): Linear(in_features=256, out_features=4160, bias=False)\n","  )\n",")\n","2024-10-24 10:42:14 | INFO | fairseq_cli.train | task: TranslationTask\n","2024-10-24 10:42:14 | INFO | fairseq_cli.train | model: TransformerModel\n","2024-10-24 10:42:14 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n","2024-10-24 10:42:14 | INFO | fairseq_cli.train | num. shared model params: 6,594,560 (num. trained: 6,594,560)\n","2024-10-24 10:42:14 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n","2024-10-24 10:42:14 | INFO | fairseq.data.data_utils | loaded 6,336 examples from: data-bin-25/valid.eng-nso.eng\n","2024-10-24 10:42:15 | INFO | fairseq.data.data_utils | loaded 6,336 examples from: data-bin-25/valid.eng-nso.nso\n","2024-10-24 10:42:15 | INFO | fairseq.tasks.translation | data-bin-25 valid eng-nso 6336 examples\n","2024-10-24 10:42:16 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight \u003c- decoder.embed_tokens.weight\n","2024-10-24 10:42:16 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight \u003c- decoder.output_projection.weight\n","2024-10-24 10:42:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2024-10-24 10:42:16 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n","2024-10-24 10:42:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2024-10-24 10:42:16 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n","2024-10-24 10:42:16 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n","2024-10-24 10:42:16 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints-bpeDROP/checkpoint_last.pt\n","2024-10-24 10:42:16 | INFO | fairseq.trainer | No existing checkpoint found checkpoints-bpeDROP/checkpoint_last.pt\n","2024-10-24 10:42:16 | INFO | fairseq.trainer | loading train data for epoch 1\n","2024-10-24 10:42:18 | INFO | fairseq.data.data_utils | loaded 524,850 examples from: data-bin-25/train.eng-nso.eng\n","2024-10-24 10:42:21 | INFO | fairseq.data.data_utils | loaded 524,850 examples from: data-bin-25/train.eng-nso.nso\n","2024-10-24 10:42:21 | INFO | fairseq.tasks.translation | data-bin-25 train eng-nso 524850 examples\n","2024-10-24 10:42:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 10:42:21 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2024-10-24 10:42:21 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2024-10-24 10:42:21 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2024-10-24 10:42:22 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n","2024-10-24 10:42:22 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n","2024-10-24 10:42:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 10:42:22 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2024-10-24 10:42:22 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2024-10-24 10:42:22 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2024-10-24 10:42:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5161\n","epoch 001:   0% 0/5161 [00:00\u003c?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtyobeka-mandisa\u001b[0m (\u001b[33mtyobeka-mandisa-university-of-cape-town\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.5\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/Research/eng-to-nso/bpeDROP/wandb/run-20241024_104224-wlkejy4p\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcheckpoints-bpeDROP\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 猸锔 View project at \u001b[34m\u001b[4mhttps://wandb.ai/tyobeka-mandisa-university-of-cape-town/fairseq-standard-subword-tok-eng-to-nso\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/tyobeka-mandisa-university-of-cape-town/fairseq-standard-subword-tok-eng-to-nso/runs/wlkejy4p\u001b[0m\n","2024-10-24 10:42:25 | INFO | fairseq.trainer | begin training epoch 1\n","2024-10-24 10:42:25 | INFO | fairseq_cli.train | Start iterating over samples\n","/content/drive/MyDrive/Research/eng-to-nso/fairseq/fairseq/tasks/fairseq_task.py:531: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","/content/drive/MyDrive/Research/eng-to-nso/fairseq/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n","  warnings.warn(\n","epoch 001:  39% 1999/5161 [02:56\u003c04:16, 12.35it/s, loss=7.212, nll_loss=6.359, ppl=82.08, wps=41310.8, ups=10.78, wpb=3833.6, bsz=104.9, num_updates=1900, lr=0.000217643, gnorm=0.841, train_wall=8, gb_free=14.1, wall=177]2024-10-24 10:45:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 10:45:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 001 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   1% 1/84 [00:01\u003c01:52,  1.36s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   2% 2/84 [00:02\u003c01:52,  1.37s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   4% 3/84 [00:04\u003c01:51,  1.37s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   5% 4/84 [00:05\u003c01:52,  1.40s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   6% 5/84 [00:06\u003c01:40,  1.27s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   7% 6/84 [00:07\u003c01:35,  1.23s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   8% 7/84 [00:08\u003c01:27,  1.14s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  10% 8/84 [00:09\u003c01:23,  1.10s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  11% 9/84 [00:10\u003c01:17,  1.03s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  12% 10/84 [00:11\u003c01:13,  1.01it/s]\u001b[A\n","epoch 001:  39% 1999/5161 [03:10\u003c04:16, 12.35it/s, loss=7.171, nll_loss=6.31, ppl=79.37, wps=47533.6, ups=12.53, wpb=3792.6, bsz=95.9, num_updates=2000, lr=0.000212132, gnorm=0.801, train_wall=8, gb_free=14, wall=185]    \n","epoch 001 | valid on 'valid' subset:  14% 12/84 [00:13\u003c01:07,  1.07it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  15% 13/84 [00:14\u003c01:06,  1.07it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  17% 14/84 [00:15\u003c01:03,  1.11it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  18% 15/84 [00:16\u003c01:04,  1.07it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  19% 16/84 [00:16\u003c01:01,  1.10it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  20% 17/84 [00:18\u003c01:06,  1.01it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  21% 18/84 [00:19\u003c01:09,  1.06s/it]\u001b[A2024-10-24 10:45:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:45:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:45:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 001 | valid on 'valid' subset:  23% 19/84 [00:20\u003c01:18,  1.20s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  24% 20/84 [00:22\u003c01:18,  1.23s/it]\u001b[A2024-10-24 10:45:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:45:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:45:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 001 | valid on 'valid' subset:  25% 21/84 [00:23\u003c01:22,  1.32s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  26% 22/84 [00:24\u003c01:16,  1.23s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  27% 23/84 [00:25\u003c01:08,  1.12s/it]\u001b[A2024-10-24 10:45:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:45:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:45:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 001 | valid on 'valid' subset:  29% 24/84 [00:26\u003c01:06,  1.11s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  30% 25/84 [00:27\u003c01:00,  1.03s/it]\u001b[A2024-10-24 10:45:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:45:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:45:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 001 | valid on 'valid' subset:  31% 26/84 [00:28\u003c01:00,  1.04s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  32% 27/84 [00:29\u003c00:57,  1.02s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  33% 28/84 [00:30\u003c00:53,  1.05it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  35% 29/84 [00:31\u003c00:53,  1.03it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  36% 30/84 [00:32\u003c00:51,  1.05it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  37% 31/84 [00:33\u003c00:51,  1.03it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  38% 32/84 [00:34\u003c00:48,  1.07it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  39% 33/84 [00:35\u003c00:55,  1.09s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  40% 34/84 [00:36\u003c00:57,  1.14s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  42% 35/84 [00:38\u003c00:59,  1.21s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  43% 36/84 [00:39\u003c01:01,  1.29s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  44% 37/84 [00:40\u003c00:59,  1.27s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  45% 38/84 [00:41\u003c00:54,  1.19s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  46% 39/84 [00:42\u003c00:48,  1.08s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  48% 40/84 [00:43\u003c00:47,  1.07s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  49% 41/84 [00:44\u003c00:44,  1.04s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  50% 42/84 [00:45\u003c00:44,  1.05s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  51% 43/84 [00:46\u003c00:41,  1.00s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  52% 44/84 [00:47\u003c00:40,  1.02s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  54% 45/84 [00:48\u003c00:37,  1.04it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  55% 46/84 [00:49\u003c00:35,  1.07it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  56% 47/84 [00:50\u003c00:35,  1.05it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  57% 48/84 [00:51\u003c00:35,  1.02it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  58% 49/84 [00:52\u003c00:38,  1.09s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  60% 50/84 [00:54\u003c00:39,  1.16s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  61% 51/84 [00:55\u003c00:40,  1.24s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  62% 52/84 [00:56\u003c00:40,  1.25s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  63% 53/84 [00:57\u003c00:36,  1.17s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  64% 54/84 [00:58\u003c00:32,  1.08s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  65% 55/84 [00:59\u003c00:30,  1.05s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  67% 56/84 [01:00\u003c00:28,  1.01s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  68% 57/84 [01:01\u003c00:26,  1.01it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  69% 58/84 [01:02\u003c00:25,  1.03it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  70% 59/84 [01:03\u003c00:24,  1.04it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  71% 60/84 [01:04\u003c00:22,  1.07it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  73% 61/84 [01:05\u003c00:21,  1.07it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  74% 62/84 [01:06\u003c00:20,  1.09it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  75% 63/84 [01:07\u003c00:20,  1.03it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  76% 64/84 [01:08\u003c00:21,  1.06s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  77% 65/84 [01:09\u003c00:21,  1.12s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  79% 66/84 [01:11\u003c00:21,  1.20s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  80% 67/84 [01:12\u003c00:20,  1.22s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  81% 68/84 [01:13\u003c00:18,  1.19s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  82% 69/84 [01:14\u003c00:16,  1.10s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  83% 70/84 [01:15\u003c00:14,  1.04s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  85% 71/84 [01:16\u003c00:12,  1.01it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  86% 72/84 [01:16\u003c00:11,  1.05it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  87% 73/84 [01:17\u003c00:10,  1.09it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  88% 74/84 [01:18\u003c00:09,  1.09it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  89% 75/84 [01:19\u003c00:08,  1.10it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  90% 76/84 [01:20\u003c00:07,  1.12it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  92% 77/84 [01:21\u003c00:06,  1.12it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  93% 78/84 [01:22\u003c00:05,  1.11it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  94% 79/84 [01:23\u003c00:04,  1.03it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  95% 80/84 [01:24\u003c00:04,  1.06s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  96% 81/84 [01:25\u003c00:03,  1.09s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  98% 82/84 [01:27\u003c00:02,  1.17s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  99% 83/84 [01:28\u003c00:01,  1.20s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset: 100% 84/84 [01:29\u003c00:00,  1.13s/it]\u001b[A\n","                                                                        \u001b[A2024-10-24 10:46:50 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 6.884 | nll_loss 5.94 | ppl 61.38 | bleu 1.22 | wps 2285.8 | wpb 2417.9 | bsz 75.4 | num_updates 2000\n","2024-10-24 10:46:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 2000 updates\n","2024-10-24 10:46:50 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_1_2000.pt\n","2024-10-24 10:46:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_1_2000.pt\n","2024-10-24 10:46:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpeDROP/checkpoint_1_2000.pt (epoch 1 @ 2000 updates, score 1.22) (writing took 1.6839100110009895 seconds)\n","epoch 001:  77% 3999/5161 [07:24\u003c01:31, 12.75it/s, loss=6.514, nll_loss=5.544, ppl=46.64, wps=47863.6, ups=12.32, wpb=3884.2, bsz=102.4, num_updates=3900, lr=0.000151911, gnorm=0.863, train_wall=8, gb_free=14.1, wall=444]2024-10-24 10:49:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 10:49:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 001 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A2024-10-24 10:49:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:49:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:49:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 001 | valid on 'valid' subset:   1% 1/84 [00:01\u003c02:10,  1.57s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   2% 2/84 [00:03\u003c02:09,  1.58s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   4% 3/84 [00:04\u003c01:47,  1.32s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   5% 4/84 [00:05\u003c01:35,  1.19s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   6% 5/84 [00:06\u003c01:27,  1.10s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   7% 6/84 [00:07\u003c01:23,  1.07s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   8% 7/84 [00:08\u003c01:27,  1.13s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  10% 8/84 [00:09\u003c01:31,  1.21s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  11% 9/84 [00:11\u003c01:32,  1.23s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  12% 10/84 [00:12\u003c01:32,  1.24s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  13% 11/84 [00:13\u003c01:32,  1.27s/it]\u001b[A\n","epoch 001:  77% 3999/5161 [07:40\u003c01:31, 12.75it/s, loss=6.48, nll_loss=5.505, ppl=45.42, wps=42756.5, ups=10.95, wpb=3904.5, bsz=107.3, num_updates=4000, lr=0.00015, gnorm=0.872, train_wall=8, gb_free=14.1, wall=453]     \n","epoch 001 | valid on 'valid' subset:  15% 13/84 [00:15\u003c01:17,  1.09s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  17% 14/84 [00:16\u003c01:12,  1.04s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  18% 15/84 [00:17\u003c01:10,  1.02s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  19% 16/84 [00:18\u003c01:05,  1.03it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  20% 17/84 [00:19\u003c01:05,  1.03it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  21% 18/84 [00:20\u003c01:01,  1.08it/s]\u001b[A2024-10-24 10:50:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:50:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:50:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 001 | valid on 'valid' subset:  23% 19/84 [00:21\u003c01:02,  1.04it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  24% 20/84 [00:21\u003c00:57,  1.12it/s]\u001b[A2024-10-24 10:50:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:50:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:50:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 001 | valid on 'valid' subset:  25% 21/84 [00:22\u003c00:59,  1.06it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  26% 22/84 [00:23\u003c01:00,  1.03it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  27% 23/84 [00:24\u003c00:59,  1.02it/s]\u001b[A2024-10-24 10:50:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:50:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:50:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 001 | valid on 'valid' subset:  29% 24/84 [00:26\u003c01:08,  1.14s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  30% 25/84 [00:27\u003c01:09,  1.18s/it]\u001b[A2024-10-24 10:50:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:50:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:50:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 001 | valid on 'valid' subset:  31% 26/84 [00:29\u003c01:14,  1.28s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  32% 27/84 [00:30\u003c01:07,  1.19s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  33% 28/84 [00:30\u003c01:00,  1.08s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  35% 29/84 [00:31\u003c00:57,  1.05s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  36% 30/84 [00:32\u003c00:53,  1.00it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  37% 31/84 [00:33\u003c00:52,  1.00it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  38% 32/84 [00:34\u003c00:49,  1.04it/s]\u001b[A2024-10-24 10:50:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:50:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:50:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 001 | valid on 'valid' subset:  39% 33/84 [00:35\u003c00:50,  1.02it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  40% 34/84 [00:36\u003c00:49,  1.00it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  42% 35/84 [00:37\u003c00:47,  1.04it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  43% 36/84 [00:38\u003c00:45,  1.05it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  44% 37/84 [00:39\u003c00:45,  1.04it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  45% 38/84 [00:40\u003c00:49,  1.08s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  46% 39/84 [00:42\u003c00:48,  1.08s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  48% 40/84 [00:43\u003c00:53,  1.22s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  49% 41/84 [00:44\u003c00:54,  1.27s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  50% 42/84 [00:46\u003c00:52,  1.24s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  51% 43/84 [00:46\u003c00:46,  1.13s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  52% 44/84 [00:48\u003c00:44,  1.12s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  54% 45/84 [00:48\u003c00:40,  1.05s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  55% 46/84 [00:49\u003c00:35,  1.08it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  56% 47/84 [00:50\u003c00:34,  1.08it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  57% 48/84 [00:51\u003c00:32,  1.11it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  58% 49/84 [00:52\u003c00:32,  1.08it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  60% 50/84 [00:53\u003c00:31,  1.09it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  61% 51/84 [00:54\u003c00:31,  1.05it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  62% 52/84 [00:55\u003c00:29,  1.08it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  63% 53/84 [00:56\u003c00:32,  1.03s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  64% 54/84 [00:57\u003c00:33,  1.11s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  65% 55/84 [00:59\u003c00:35,  1.21s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  67% 56/84 [01:00\u003c00:34,  1.24s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  68% 57/84 [01:01\u003c00:34,  1.28s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  69% 58/84 [01:02\u003c00:30,  1.16s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  70% 59/84 [01:03\u003c00:27,  1.09s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  71% 60/84 [01:04\u003c00:24,  1.02s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  73% 61/84 [01:05\u003c00:22,  1.01it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  74% 62/84 [01:06\u003c00:20,  1.07it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  75% 63/84 [01:07\u003c00:19,  1.06it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  76% 64/84 [01:08\u003c00:18,  1.08it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  77% 65/84 [01:09\u003c00:17,  1.08it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  79% 66/84 [01:09\u003c00:16,  1.08it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  80% 67/84 [01:10\u003c00:15,  1.07it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  81% 68/84 [01:11\u003c00:15,  1.06it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  82% 69/84 [01:13\u003c00:15,  1.01s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  83% 70/84 [01:14\u003c00:15,  1.08s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  85% 71/84 [01:15\u003c00:15,  1.16s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  86% 72/84 [01:16\u003c00:14,  1.22s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  87% 73/84 [01:18\u003c00:13,  1.23s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  88% 74/84 [01:19\u003c00:11,  1.13s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  89% 75/84 [01:20\u003c00:09,  1.07s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  90% 76/84 [01:20\u003c00:08,  1.01s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  92% 77/84 [01:21\u003c00:06,  1.03it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  93% 78/84 [01:22\u003c00:05,  1.05it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  94% 79/84 [01:23\u003c00:04,  1.06it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  95% 80/84 [01:24\u003c00:03,  1.07it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  96% 81/84 [01:25\u003c00:02,  1.09it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  98% 82/84 [01:26\u003c00:01,  1.08it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  99% 83/84 [01:27\u003c00:00,  1.12it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset: 100% 84/84 [01:27\u003c00:00,  1.16it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 10:51:16 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 6.153 | nll_loss 5.075 | ppl 33.7 | bleu 3.78 | wps 2330.7 | wpb 2417.9 | bsz 75.4 | num_updates 4000 | best_bleu 3.78\n","2024-10-24 10:51:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 4000 updates\n","2024-10-24 10:51:16 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_1_4000.pt\n","2024-10-24 10:51:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_1_4000.pt\n","2024-10-24 10:51:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpeDROP/checkpoint_1_4000.pt (epoch 1 @ 4000 updates, score 3.78) (writing took 1.5693673339992529 seconds)\n","epoch 001: 100% 5160/5161 [10:37\u003c00:00, 12.71it/s, loss=6.332, nll_loss=5.33, ppl=40.23, wps=46372.9, ups=11.84, wpb=3915.2, bsz=96.2, num_updates=5100, lr=0.000132842, gnorm=0.86, train_wall=8, gb_free=14.1, wall=640]2024-10-24 10:53:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 10:53:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 001 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   1% 1/84 [00:01\u003c01:40,  1.22s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   2% 2/84 [00:02\u003c01:30,  1.10s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   4% 3/84 [00:03\u003c01:23,  1.03s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   5% 4/84 [00:03\u003c01:14,  1.07it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   6% 5/84 [00:04\u003c01:01,  1.28it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   7% 6/84 [00:04\u003c00:52,  1.49it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   8% 7/84 [00:05\u003c00:53,  1.45it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  10% 8/84 [00:06\u003c00:48,  1.56it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  11% 9/84 [00:07\u003c00:53,  1.39it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  12% 10/84 [00:07\u003c00:49,  1.49it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  13% 11/84 [00:08\u003c00:48,  1.51it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  14% 12/84 [00:09\u003c00:53,  1.34it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  15% 13/84 [00:10\u003c00:56,  1.26it/s]\u001b[A\n","epoch 001: 100% 5160/5161 [10:50\u003c00:00, 12.71it/s, loss=6.332, nll_loss=5.33, ppl=40.23, wps=46372.9, ups=11.84, wpb=3915.2, bsz=96.2, num_updates=5100, lr=0.000132842, gnorm=0.86, train_wall=8, gb_free=14.1, wall=640]\n","epoch 001 | valid on 'valid' subset:  18% 15/84 [00:12\u003c01:10,  1.02s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  19% 16/84 [00:13\u003c01:05,  1.04it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  20% 17/84 [00:14\u003c01:10,  1.05s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  21% 18/84 [00:15\u003c01:04,  1.03it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  23% 19/84 [00:16\u003c01:00,  1.07it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  24% 20/84 [00:16\u003c00:51,  1.23it/s]\u001b[A2024-10-24 10:53:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:53:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:53:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 001 | valid on 'valid' subset:  25% 21/84 [00:17\u003c00:53,  1.17it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  26% 22/84 [00:18\u003c00:47,  1.30it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  27% 23/84 [00:19\u003c00:47,  1.27it/s]\u001b[A2024-10-24 10:53:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:53:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:53:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 001 | valid on 'valid' subset:  29% 24/84 [00:19\u003c00:47,  1.27it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  30% 25/84 [00:20\u003c00:42,  1.39it/s]\u001b[A2024-10-24 10:53:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:53:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:53:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 001 | valid on 'valid' subset:  31% 26/84 [00:21\u003c00:46,  1.24it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  32% 27/84 [00:22\u003c00:46,  1.22it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  33% 28/84 [00:23\u003c00:45,  1.24it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  35% 29/84 [00:23\u003c00:43,  1.26it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  36% 30/84 [00:24\u003c00:42,  1.27it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  37% 31/84 [00:26\u003c00:51,  1.03it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  38% 32/84 [00:26\u003c00:47,  1.10it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  39% 33/84 [00:28\u003c00:51,  1.02s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  40% 34/84 [00:29\u003c00:50,  1.01s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  42% 35/84 [00:30\u003c00:48,  1.00it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  43% 36/84 [00:30\u003c00:47,  1.02it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  44% 37/84 [00:31\u003c00:43,  1.08it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  45% 38/84 [00:32\u003c00:39,  1.16it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  46% 39/84 [00:33\u003c00:34,  1.30it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  48% 40/84 [00:33\u003c00:36,  1.22it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  49% 41/84 [00:34\u003c00:36,  1.17it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  50% 42/84 [00:35\u003c00:37,  1.12it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  51% 43/84 [00:36\u003c00:33,  1.23it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  52% 44/84 [00:37\u003c00:34,  1.16it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  54% 45/84 [00:38\u003c00:32,  1.22it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  55% 46/84 [00:38\u003c00:29,  1.28it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  56% 47/84 [00:39\u003c00:29,  1.27it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  57% 48/84 [00:40\u003c00:29,  1.21it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  58% 49/84 [00:41\u003c00:33,  1.05it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  60% 50/84 [00:43\u003c00:34,  1.03s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  61% 51/84 [00:44\u003c00:35,  1.09s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  62% 52/84 [00:45\u003c00:36,  1.16s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  63% 53/84 [00:46\u003c00:34,  1.13s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  64% 54/84 [00:47\u003c00:30,  1.00s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  65% 55/84 [00:48\u003c00:28,  1.00it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  67% 56/84 [00:49\u003c00:26,  1.05it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  68% 57/84 [00:50\u003c00:25,  1.05it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  69% 58/84 [00:51\u003c00:24,  1.07it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  70% 59/84 [00:52\u003c00:23,  1.06it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  71% 60/84 [00:52\u003c00:22,  1.08it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  73% 61/84 [00:53\u003c00:21,  1.09it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  74% 62/84 [00:54\u003c00:19,  1.14it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  75% 63/84 [00:55\u003c00:18,  1.12it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  76% 64/84 [00:56\u003c00:18,  1.08it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  77% 65/84 [00:57\u003c00:19,  1.01s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  79% 66/84 [00:58\u003c00:19,  1.07s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  80% 67/84 [01:00\u003c00:19,  1.12s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  81% 68/84 [01:01\u003c00:18,  1.18s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  82% 69/84 [01:02\u003c00:17,  1.19s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  83% 70/84 [01:03\u003c00:15,  1.10s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  85% 71/84 [01:04\u003c00:13,  1.05s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  86% 72/84 [01:05\u003c00:12,  1.01s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  87% 73/84 [01:06\u003c00:10,  1.06it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  88% 74/84 [01:07\u003c00:09,  1.08it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  89% 75/84 [01:07\u003c00:08,  1.10it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  90% 76/84 [01:08\u003c00:07,  1.10it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  92% 77/84 [01:09\u003c00:06,  1.06it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  93% 78/84 [01:11\u003c00:06,  1.04s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  94% 79/84 [01:12\u003c00:05,  1.13s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  95% 80/84 [01:14\u003c00:04,  1.24s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  96% 81/84 [01:15\u003c00:04,  1.34s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  98% 82/84 [01:16\u003c00:02,  1.34s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  99% 83/84 [01:18\u003c00:01,  1.36s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset: 100% 84/84 [01:19\u003c00:00,  1.31s/it]\u001b[A\n","                                                                        \u001b[A2024-10-24 10:54:21 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 5.913 | nll_loss 4.787 | ppl 27.61 | bleu 6.15 | wps 2571.5 | wpb 2417.9 | bsz 75.4 | num_updates 5161 | best_bleu 6.15\n","2024-10-24 10:54:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 5161 updates\n","2024-10-24 10:54:21 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_best.pt\n","2024-10-24 10:54:22 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_best.pt\n","2024-10-24 10:54:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpeDROP/checkpoint_best.pt (epoch 1 @ 5161 updates, score 6.15) (writing took 1.1956960690004053 seconds)\n","2024-10-24 10:54:22 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n","2024-10-24 10:54:22 | INFO | train | epoch 001 | loss 7.306 | nll_loss 6.472 | ppl 88.76 | wps 27861.1 | ups 7.22 | wpb 3860.3 | bsz 101.7 | num_updates 5161 | lr 0.000132055 | gnorm 0.887 | train_wall 418 | gb_free 14.1 | wall 727\n","2024-10-24 10:54:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 10:54:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5161\n","epoch 002:   0% 0/5161 [00:00\u003c?, ?it/s]2024-10-24 10:54:23 | INFO | fairseq.trainer | begin training epoch 2\n","2024-10-24 10:54:23 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 002:  16% 838/5161 [01:13\u003c05:53, 12.24it/s, loss=6.177, nll_loss=5.152, ppl=35.55, wps=41642, ups=10.89, wpb=3824.6, bsz=101, num_updates=5900, lr=0.000123508, gnorm=0.89, train_wall=8, gb_free=14.1, wall=792]2024-10-24 10:55:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 10:55:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 002 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   1% 1/84 [00:00\u003c01:13,  1.14it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   2% 2/84 [00:02\u003c01:51,  1.36s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:   4% 3/84 [00:03\u003c01:51,  1.38s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:   5% 4/84 [00:05\u003c01:43,  1.30s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:   6% 5/84 [00:05\u003c01:26,  1.09s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:   7% 6/84 [00:06\u003c01:11,  1.09it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   8% 7/84 [00:07\u003c01:11,  1.07it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  10% 8/84 [00:08\u003c01:04,  1.18it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  11% 9/84 [00:08\u003c00:59,  1.27it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  12% 10/84 [00:09\u003c00:51,  1.44it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  13% 11/84 [00:09\u003c00:49,  1.46it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  14% 12/84 [00:10\u003c00:43,  1.64it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  15% 13/84 [00:10\u003c00:44,  1.60it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  17% 14/84 [00:11\u003c00:39,  1.78it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  18% 15/84 [00:12\u003c00:45,  1.50it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  19% 16/84 [00:12\u003c00:44,  1.54it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  20% 17/84 [00:13\u003c00:44,  1.49it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  21% 18/84 [00:14\u003c00:41,  1.58it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  23% 19/84 [00:14\u003c00:43,  1.48it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  24% 20/84 [00:15\u003c00:38,  1.65it/s]\u001b[A2024-10-24 10:55:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:55:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:55:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 002 | valid on 'valid' subset:  25% 21/84 [00:16\u003c00:45,  1.39it/s]\u001b[A\n","epoch 002:  16% 838/5161 [01:31\u003c05:53, 12.24it/s, loss=6.162, nll_loss=5.133, ppl=35.08, wps=47030.3, ups=12.24, wpb=3841.7, bsz=110.6, num_updates=6000, lr=0.000122474, gnorm=0.967, train_wall=8, gb_free=14.1, wall=801]\n","epoch 002 | valid on 'valid' subset:  27% 23/84 [00:17\u003c00:46,  1.31it/s]\u001b[A2024-10-24 10:55:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:55:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:55:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 002 | valid on 'valid' subset:  29% 24/84 [00:19\u003c00:52,  1.14it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  30% 25/84 [00:19\u003c00:50,  1.18it/s]\u001b[A2024-10-24 10:55:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:55:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:55:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 002 | valid on 'valid' subset:  31% 26/84 [00:21\u003c00:55,  1.05it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  32% 27/84 [00:22\u003c00:54,  1.06it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  33% 28/84 [00:22\u003c00:51,  1.10it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  35% 29/84 [00:23\u003c00:50,  1.08it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  36% 30/84 [00:24\u003c00:43,  1.24it/s]\u001b[A2024-10-24 10:56:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:56:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:56:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 002 | valid on 'valid' subset:  37% 31/84 [00:25\u003c00:42,  1.26it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  38% 32/84 [00:25\u003c00:37,  1.39it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  39% 33/84 [00:26\u003c00:36,  1.40it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  40% 34/84 [00:27\u003c00:34,  1.44it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  42% 35/84 [00:27\u003c00:31,  1.55it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  43% 36/84 [00:28\u003c00:32,  1.49it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  44% 37/84 [00:28\u003c00:30,  1.54it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  45% 38/84 [00:29\u003c00:30,  1.49it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  46% 39/84 [00:30\u003c00:29,  1.54it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  48% 40/84 [00:30\u003c00:29,  1.50it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  49% 41/84 [00:31\u003c00:32,  1.32it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  50% 42/84 [00:32\u003c00:36,  1.16it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  51% 43/84 [00:33\u003c00:37,  1.10it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  52% 44/84 [00:35\u003c00:39,  1.02it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  54% 45/84 [00:36\u003c00:37,  1.03it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  55% 46/84 [00:37\u003c00:39,  1.03s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  56% 47/84 [00:38\u003c00:40,  1.09s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  57% 48/84 [00:39\u003c00:34,  1.05it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  58% 49/84 [00:40\u003c00:34,  1.01it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  60% 50/84 [00:40\u003c00:30,  1.10it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  61% 51/84 [00:41\u003c00:30,  1.09it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  62% 52/84 [00:42\u003c00:26,  1.19it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  63% 53/84 [00:43\u003c00:25,  1.23it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  64% 54/84 [00:43\u003c00:23,  1.30it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  65% 55/84 [00:44\u003c00:22,  1.30it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  67% 56/84 [00:45\u003c00:22,  1.25it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  68% 57/84 [00:46\u003c00:22,  1.19it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  69% 58/84 [00:47\u003c00:20,  1.27it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  70% 59/84 [00:47\u003c00:18,  1.33it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  71% 60/84 [00:48\u003c00:19,  1.21it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  73% 61/84 [00:49\u003c00:21,  1.09it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  74% 62/84 [00:50\u003c00:19,  1.13it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  75% 63/84 [00:51\u003c00:19,  1.05it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  76% 64/84 [00:53\u003c00:21,  1.05s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  77% 65/84 [00:54\u003c00:20,  1.06s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  79% 66/84 [00:55\u003c00:18,  1.01s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  80% 67/84 [00:56\u003c00:16,  1.04it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  81% 68/84 [00:56\u003c00:15,  1.06it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  82% 69/84 [00:57\u003c00:13,  1.10it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  83% 70/84 [00:58\u003c00:12,  1.09it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  85% 71/84 [00:59\u003c00:11,  1.12it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  86% 72/84 [01:00\u003c00:10,  1.12it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  87% 73/84 [01:01\u003c00:09,  1.14it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  88% 74/84 [01:02\u003c00:08,  1.18it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  89% 75/84 [01:02\u003c00:07,  1.24it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  90% 76/84 [01:03\u003c00:06,  1.22it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  92% 77/84 [01:04\u003c00:06,  1.15it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  93% 78/84 [01:05\u003c00:05,  1.04it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  94% 79/84 [01:06\u003c00:05,  1.05s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  95% 80/84 [01:08\u003c00:04,  1.14s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  96% 81/84 [01:09\u003c00:03,  1.17s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  98% 82/84 [01:10\u003c00:02,  1.15s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  99% 83/84 [01:11\u003c00:01,  1.07s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset: 100% 84/84 [01:12\u003c00:00,  1.02it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 10:56:49 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 5.813 | nll_loss 4.663 | ppl 25.34 | bleu 7.54 | wps 2817.1 | wpb 2417.9 | bsz 75.4 | num_updates 6000 | best_bleu 7.54\n","2024-10-24 10:56:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 6000 updates\n","2024-10-24 10:56:49 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_2_6000.pt\n","2024-10-24 10:56:49 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_2_6000.pt\n","2024-10-24 10:56:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpeDROP/checkpoint_2_6000.pt (epoch 2 @ 6000 updates, score 7.54) (writing took 1.7787323880002077 seconds)\n","epoch 002:  55% 2838/5161 [05:26\u003c03:41, 10.51it/s, loss=5.943, nll_loss=4.879, ppl=29.42, wps=47623.7, ups=12.27, wpb=3881.6, bsz=104.2, num_updates=7900, lr=0.000106735, gnorm=0.925, train_wall=8, gb_free=14.1, wall=1043]2024-10-24 10:59:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 10:59:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 002 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   1% 1/84 [00:00\u003c01:19,  1.04it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   2% 2/84 [00:01\u003c01:05,  1.26it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   4% 3/84 [00:02\u003c00:56,  1.44it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   5% 4/84 [00:02\u003c00:56,  1.41it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   6% 5/84 [00:03\u003c00:57,  1.39it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   7% 6/84 [00:04\u003c00:47,  1.63it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   8% 7/84 [00:04\u003c00:44,  1.72it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  10% 8/84 [00:05\u003c00:41,  1.84it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  11% 9/84 [00:05\u003c00:38,  1.96it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  12% 10/84 [00:05\u003c00:35,  2.07it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  13% 11/84 [00:06\u003c00:38,  1.89it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  14% 12/84 [00:07\u003c00:41,  1.72it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  15% 13/84 [00:08\u003c00:53,  1.32it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  17% 14/84 [00:09\u003c00:53,  1.31it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  18% 15/84 [00:10\u003c01:03,  1.08it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  19% 16/84 [00:11\u003c01:09,  1.03s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  20% 17/84 [00:12\u003c01:11,  1.07s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  21% 18/84 [00:13\u003c01:04,  1.02it/s]\u001b[A\n","epoch 002:  55% 2838/5161 [05:41\u003c03:41, 10.51it/s, loss=5.922, nll_loss=4.854, ppl=28.93, wps=38378.8, ups=9.9, wpb=3875.2, bsz=107, num_updates=8000, lr=0.000106066, gnorm=0.941, train_wall=9, gb_free=14, wall=1053]      \n","epoch 002 | valid on 'valid' subset:  24% 20/84 [00:14\u003c00:49,  1.28it/s]\u001b[A2024-10-24 11:00:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:00:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:00:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 002 | valid on 'valid' subset:  25% 21/84 [00:15\u003c00:51,  1.23it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  26% 22/84 [00:16\u003c00:44,  1.39it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  27% 23/84 [00:16\u003c00:41,  1.48it/s]\u001b[A2024-10-24 11:00:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:00:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:00:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 002 | valid on 'valid' subset:  29% 24/84 [00:17\u003c00:41,  1.45it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  30% 25/84 [00:18\u003c00:37,  1.58it/s]\u001b[A2024-10-24 11:00:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:00:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:00:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 002 | valid on 'valid' subset:  31% 26/84 [00:18\u003c00:39,  1.46it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  32% 27/84 [00:19\u003c00:40,  1.41it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  33% 28/84 [00:20\u003c00:38,  1.45it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  35% 29/84 [00:21\u003c00:41,  1.33it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  36% 30/84 [00:21\u003c00:37,  1.45it/s]\u001b[A2024-10-24 11:00:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:00:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:00:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 002 | valid on 'valid' subset:  37% 31/84 [00:22\u003c00:38,  1.36it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  38% 32/84 [00:23\u003c00:35,  1.45it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  39% 33/84 [00:24\u003c00:40,  1.26it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  40% 34/84 [00:25\u003c00:40,  1.23it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  42% 35/84 [00:25\u003c00:39,  1.24it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  43% 36/84 [00:27\u003c00:46,  1.03it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  44% 37/84 [00:28\u003c00:45,  1.03it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  45% 38/84 [00:29\u003c00:46,  1.00s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  46% 39/84 [00:30\u003c00:46,  1.04s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  48% 40/84 [00:31\u003c00:41,  1.07it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  49% 41/84 [00:32\u003c00:39,  1.08it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  50% 42/84 [00:32\u003c00:39,  1.07it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  51% 43/84 [00:33\u003c00:34,  1.18it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  52% 44/84 [00:34\u003c00:32,  1.22it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  54% 45/84 [00:35\u003c00:31,  1.25it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  55% 46/84 [00:35\u003c00:29,  1.30it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  56% 47/84 [00:36\u003c00:30,  1.22it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  57% 48/84 [00:37\u003c00:30,  1.19it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  58% 49/84 [00:38\u003c00:30,  1.14it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  60% 50/84 [00:39\u003c00:27,  1.25it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  61% 51/84 [00:40\u003c00:28,  1.17it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  62% 52/84 [00:41\u003c00:29,  1.09it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  63% 53/84 [00:42\u003c00:28,  1.08it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  64% 54/84 [00:43\u003c00:26,  1.13it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  65% 55/84 [00:44\u003c00:30,  1.05s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  67% 56/84 [00:45\u003c00:32,  1.16s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  68% 57/84 [00:46\u003c00:29,  1.10s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  69% 58/84 [00:47\u003c00:25,  1.01it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  70% 59/84 [00:48\u003c00:24,  1.03it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  71% 60/84 [00:49\u003c00:21,  1.13it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  73% 61/84 [00:50\u003c00:20,  1.12it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  74% 62/84 [00:50\u003c00:18,  1.16it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  75% 63/84 [00:51\u003c00:18,  1.13it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  76% 64/84 [00:52\u003c00:17,  1.14it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  77% 65/84 [00:53\u003c00:16,  1.14it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  79% 66/84 [00:54\u003c00:15,  1.14it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  80% 67/84 [00:55\u003c00:15,  1.13it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  81% 68/84 [00:56\u003c00:13,  1.20it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  82% 69/84 [00:57\u003c00:14,  1.07it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  83% 70/84 [00:58\u003c00:14,  1.04s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  85% 71/84 [00:59\u003c00:14,  1.10s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  86% 72/84 [01:01\u003c00:13,  1.16s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  87% 73/84 [01:02\u003c00:13,  1.20s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  88% 74/84 [01:03\u003c00:11,  1.16s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  89% 75/84 [01:04\u003c00:09,  1.03s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  90% 76/84 [01:05\u003c00:07,  1.01it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  92% 77/84 [01:05\u003c00:06,  1.05it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  93% 78/84 [01:06\u003c00:05,  1.05it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  94% 79/84 [01:07\u003c00:04,  1.07it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  95% 80/84 [01:08\u003c00:03,  1.08it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  96% 81/84 [01:09\u003c00:02,  1.10it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  98% 82/84 [01:10\u003c00:01,  1.09it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  99% 83/84 [01:11\u003c00:00,  1.11it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset: 100% 84/84 [01:12\u003c00:00,  1.12it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 11:01:01 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 5.568 | nll_loss 4.373 | ppl 20.72 | bleu 9.64 | wps 2827.4 | wpb 2417.9 | bsz 75.4 | num_updates 8000 | best_bleu 9.64\n","2024-10-24 11:01:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 8000 updates\n","2024-10-24 11:01:01 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_2_8000.pt\n","2024-10-24 11:01:01 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_2_8000.pt\n","2024-10-24 11:01:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpeDROP/checkpoint_2_8000.pt (epoch 2 @ 8000 updates, score 9.64) (writing took 1.4972361689997342 seconds)\n","epoch 002:  94% 4837/5161 [09:37\u003c00:29, 10.84it/s, loss=5.769, nll_loss=4.676, ppl=25.56, wps=41535, ups=10.82, wpb=3839.5, bsz=108.9, num_updates=9900, lr=9.53463e-05, gnorm=0.99, train_wall=8, gb_free=14.1, wall=1296]2024-10-24 11:04:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 11:04:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 002 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   1% 1/84 [00:00\u003c01:09,  1.19it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   2% 2/84 [00:02\u003c01:32,  1.13s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:   4% 3/84 [00:03\u003c01:42,  1.26s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:   5% 4/84 [00:04\u003c01:40,  1.26s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:   6% 5/84 [00:05\u003c01:27,  1.11s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:   7% 6/84 [00:06\u003c01:17,  1.00it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   8% 7/84 [00:07\u003c01:14,  1.03it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  10% 8/84 [00:07\u003c01:00,  1.25it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  11% 9/84 [00:08\u003c00:53,  1.40it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  12% 10/84 [00:08\u003c00:46,  1.60it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  13% 11/84 [00:09\u003c00:48,  1.50it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  14% 12/84 [00:10\u003c00:51,  1.41it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  15% 13/84 [00:10\u003c00:47,  1.49it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  17% 14/84 [00:11\u003c00:41,  1.70it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  18% 15/84 [00:11\u003c00:40,  1.71it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  19% 16/84 [00:12\u003c00:38,  1.75it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  20% 17/84 [00:13\u003c00:39,  1.70it/s]\u001b[A\n","epoch 002:  94% 4837/5161 [09:51\u003c00:29, 10.84it/s, loss=5.797, nll_loss=4.708, ppl=26.14, wps=47225.8, ups=12.06, wpb=3915.2, bsz=96.8, num_updates=10000, lr=9.48683e-05, gnorm=0.954, train_wall=8, gb_free=14, wall=1304]\n","epoch 002 | valid on 'valid' subset:  23% 19/84 [00:14\u003c00:36,  1.76it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  24% 20/84 [00:14\u003c00:33,  1.93it/s]\u001b[A2024-10-24 11:04:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:04:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:04:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 002 | valid on 'valid' subset:  25% 21/84 [00:15\u003c00:34,  1.83it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  26% 22/84 [00:15\u003c00:35,  1.76it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  27% 23/84 [00:16\u003c00:35,  1.72it/s]\u001b[A2024-10-24 11:04:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:04:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:04:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 002 | valid on 'valid' subset:  29% 24/84 [00:17\u003c00:40,  1.49it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  30% 25/84 [00:17\u003c00:38,  1.52it/s]\u001b[A2024-10-24 11:04:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:04:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:04:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 002 | valid on 'valid' subset:  31% 26/84 [00:19\u003c00:51,  1.13it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  32% 27/84 [00:20\u003c00:50,  1.13it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  33% 28/84 [00:21\u003c00:49,  1.14it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  35% 29/84 [00:21\u003c00:49,  1.11it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  36% 30/84 [00:22\u003c00:42,  1.27it/s]\u001b[A2024-10-24 11:04:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:04:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:04:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 002 | valid on 'valid' subset:  37% 31/84 [00:23\u003c00:42,  1.25it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  38% 32/84 [00:23\u003c00:36,  1.41it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  39% 33/84 [00:24\u003c00:36,  1.41it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  40% 34/84 [00:25\u003c00:33,  1.48it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  42% 35/84 [00:25\u003c00:31,  1.54it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  43% 36/84 [00:26\u003c00:32,  1.46it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  44% 37/84 [00:27\u003c00:31,  1.49it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  45% 38/84 [00:27\u003c00:30,  1.49it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  46% 39/84 [00:28\u003c00:30,  1.47it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  48% 40/84 [00:29\u003c00:30,  1.45it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  49% 41/84 [00:29\u003c00:29,  1.48it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  50% 42/84 [00:30\u003c00:29,  1.44it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  51% 43/84 [00:31\u003c00:27,  1.48it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  52% 44/84 [00:32\u003c00:32,  1.23it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  54% 45/84 [00:33\u003c00:31,  1.22it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  55% 46/84 [00:33\u003c00:30,  1.24it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  56% 47/84 [00:34\u003c00:31,  1.17it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  57% 48/84 [00:35\u003c00:30,  1.18it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  58% 49/84 [00:37\u003c00:35,  1.01s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  60% 50/84 [00:38\u003c00:34,  1.03s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  61% 51/84 [00:39\u003c00:32,  1.01it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  62% 52/84 [00:39\u003c00:29,  1.09it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  63% 53/84 [00:40\u003c00:26,  1.19it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  64% 54/84 [00:41\u003c00:23,  1.29it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  65% 55/84 [00:42\u003c00:24,  1.20it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  67% 56/84 [00:42\u003c00:23,  1.20it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  68% 57/84 [00:43\u003c00:23,  1.16it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  69% 58/84 [00:44\u003c00:20,  1.25it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  70% 59/84 [00:45\u003c00:20,  1.20it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  71% 60/84 [00:46\u003c00:19,  1.25it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  73% 61/84 [00:46\u003c00:17,  1.34it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  74% 62/84 [00:47\u003c00:16,  1.32it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  75% 63/84 [00:48\u003c00:19,  1.10it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  76% 64/84 [00:50\u003c00:19,  1.01it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  77% 65/84 [00:50\u003c00:18,  1.02it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  79% 66/84 [00:52\u003c00:18,  1.05s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  80% 67/84 [00:53\u003c00:18,  1.11s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  81% 68/84 [00:54\u003c00:16,  1.02s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  82% 69/84 [00:55\u003c00:14,  1.06it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  83% 70/84 [00:55\u003c00:13,  1.07it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  85% 71/84 [00:56\u003c00:11,  1.14it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  86% 72/84 [00:57\u003c00:10,  1.16it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  87% 73/84 [00:58\u003c00:09,  1.16it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  88% 74/84 [00:59\u003c00:08,  1.22it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  89% 75/84 [00:59\u003c00:07,  1.21it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  90% 76/84 [01:00\u003c00:06,  1.29it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  92% 77/84 [01:01\u003c00:05,  1.33it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  93% 78/84 [01:02\u003c00:04,  1.27it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  94% 79/84 [01:02\u003c00:03,  1.28it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  95% 80/84 [01:03\u003c00:03,  1.20it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  96% 81/84 [01:05\u003c00:02,  1.02it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  98% 82/84 [01:06\u003c00:02,  1.10s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  99% 83/84 [01:07\u003c00:01,  1.17s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset: 100% 84/84 [01:09\u003c00:00,  1.23s/it]\u001b[A\n","                                                                        \u001b[A2024-10-24 11:05:09 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 5.408 | nll_loss 4.185 | ppl 18.19 | bleu 11.38 | wps 2941.5 | wpb 2417.9 | bsz 75.4 | num_updates 10000 | best_bleu 11.38\n","2024-10-24 11:05:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 10000 updates\n","2024-10-24 11:05:09 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_2_10000.pt\n","2024-10-24 11:05:10 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_2_10000.pt\n","2024-10-24 11:05:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpeDROP/checkpoint_2_10000.pt (epoch 2 @ 10000 updates, score 11.38) (writing took 2.094098022000253 seconds)\n","epoch 002: 100% 5159/5161 [11:18\u003c00:00, 11.80it/s, loss=5.726, nll_loss=4.626, ppl=24.69, wps=47816.9, ups=12.24, wpb=3906.2, bsz=105.4, num_updates=10300, lr=9.34765e-05, gnorm=0.963, train_wall=8, gb_free=14.1, wall=1404]2024-10-24 11:05:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 11:05:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 002 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   1% 1/84 [00:00\u003c01:11,  1.16it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   2% 2/84 [00:01\u003c01:00,  1.35it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   4% 3/84 [00:02\u003c01:18,  1.03it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   5% 4/84 [00:03\u003c01:17,  1.04it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   6% 5/84 [00:04\u003c01:13,  1.08it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   7% 6/84 [00:05\u003c01:02,  1.25it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   8% 7/84 [00:06\u003c01:09,  1.11it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  10% 8/84 [00:06\u003c00:57,  1.32it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  11% 9/84 [00:07\u003c00:48,  1.54it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  12% 10/84 [00:07\u003c00:42,  1.75it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  13% 11/84 [00:07\u003c00:39,  1.85it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  14% 12/84 [00:08\u003c00:38,  1.87it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  15% 13/84 [00:09\u003c00:39,  1.82it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  17% 14/84 [00:09\u003c00:38,  1.80it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  18% 15/84 [00:10\u003c00:39,  1.75it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  19% 16/84 [00:10\u003c00:35,  1.93it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  20% 17/84 [00:11\u003c00:37,  1.77it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  21% 18/84 [00:11\u003c00:34,  1.91it/s]\u001b[A\n","epoch 002: 100% 5159/5161 [11:31\u003c00:00, 11.80it/s, loss=5.726, nll_loss=4.626, ppl=24.69, wps=47816.9, ups=12.24, wpb=3906.2, bsz=105.4, num_updates=10300, lr=9.34765e-05, gnorm=0.963, train_wall=8, gb_free=14.1, wall=1404]\n","epoch 002 | valid on 'valid' subset:  24% 20/84 [00:12\u003c00:32,  1.97it/s]\u001b[A2024-10-24 11:05:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:05:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:05:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 002 | valid on 'valid' subset:  25% 21/84 [00:13\u003c00:34,  1.84it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  26% 22/84 [00:13\u003c00:32,  1.92it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  27% 23/84 [00:14\u003c00:31,  1.94it/s]\u001b[A2024-10-24 11:05:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:05:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:05:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 002 | valid on 'valid' subset:  29% 24/84 [00:14\u003c00:32,  1.84it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  30% 25/84 [00:15\u003c00:31,  1.89it/s]\u001b[A2024-10-24 11:05:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:05:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:05:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 002 | valid on 'valid' subset:  31% 26/84 [00:16\u003c00:41,  1.38it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  32% 27/84 [00:17\u003c00:41,  1.36it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  33% 28/84 [00:18\u003c00:40,  1.39it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  35% 29/84 [00:18\u003c00:39,  1.38it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  36% 30/84 [00:19\u003c00:39,  1.36it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  37% 31/84 [00:20\u003c00:44,  1.19it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  38% 32/84 [00:21\u003c00:41,  1.24it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  39% 33/84 [00:22\u003c00:42,  1.21it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  40% 34/84 [00:22\u003c00:36,  1.35it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  42% 35/84 [00:23\u003c00:32,  1.49it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  43% 36/84 [00:23\u003c00:31,  1.54it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  44% 37/84 [00:24\u003c00:27,  1.68it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  45% 38/84 [00:24\u003c00:27,  1.68it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  46% 39/84 [00:25\u003c00:26,  1.67it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  48% 40/84 [00:26\u003c00:26,  1.66it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  49% 41/84 [00:26\u003c00:24,  1.72it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  50% 42/84 [00:27\u003c00:25,  1.68it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  51% 43/84 [00:27\u003c00:23,  1.75it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  52% 44/84 [00:28\u003c00:23,  1.68it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  54% 45/84 [00:29\u003c00:22,  1.73it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  55% 46/84 [00:29\u003c00:21,  1.77it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  56% 47/84 [00:30\u003c00:24,  1.50it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  57% 48/84 [00:31\u003c00:22,  1.59it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  58% 49/84 [00:31\u003c00:21,  1.61it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  60% 50/84 [00:32\u003c00:20,  1.68it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  61% 51/84 [00:33\u003c00:22,  1.47it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  62% 52/84 [00:33\u003c00:23,  1.35it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  63% 53/84 [00:34\u003c00:23,  1.30it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  64% 54/84 [00:35\u003c00:23,  1.27it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  65% 55/84 [00:36\u003c00:24,  1.20it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  67% 56/84 [00:37\u003c00:24,  1.13it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  68% 57/84 [00:38\u003c00:23,  1.14it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  69% 58/84 [00:38\u003c00:20,  1.27it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  70% 59/84 [00:39\u003c00:19,  1.31it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  71% 60/84 [00:40\u003c00:16,  1.45it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  73% 61/84 [00:40\u003c00:15,  1.52it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  74% 62/84 [00:41\u003c00:13,  1.61it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  75% 63/84 [00:41\u003c00:12,  1.63it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  76% 64/84 [00:42\u003c00:12,  1.61it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  77% 65/84 [00:43\u003c00:11,  1.61it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  79% 66/84 [00:43\u003c00:11,  1.53it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  80% 67/84 [00:44\u003c00:10,  1.56it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  81% 68/84 [00:45\u003c00:10,  1.55it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  82% 69/84 [00:45\u003c00:10,  1.44it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  83% 70/84 [00:46\u003c00:09,  1.44it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  85% 71/84 [00:47\u003c00:08,  1.48it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  86% 72/84 [00:48\u003c00:09,  1.32it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  87% 73/84 [00:49\u003c00:09,  1.16it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  88% 74/84 [00:50\u003c00:08,  1.14it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  89% 75/84 [00:51\u003c00:08,  1.09it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  90% 76/84 [00:52\u003c00:07,  1.06it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  92% 77/84 [00:53\u003c00:07,  1.03s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  93% 78/84 [00:54\u003c00:06,  1.07s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  94% 79/84 [00:55\u003c00:05,  1.03s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  95% 80/84 [00:56\u003c00:04,  1.00s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  96% 81/84 [00:57\u003c00:02,  1.08it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  98% 82/84 [00:58\u003c00:01,  1.09it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  99% 83/84 [00:59\u003c00:00,  1.08it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset: 100% 84/84 [01:00\u003c00:00,  1.09it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 11:06:41 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 5.384 | nll_loss 4.155 | ppl 17.81 | bleu 10.93 | wps 3401.4 | wpb 2417.9 | bsz 75.4 | num_updates 10322 | best_bleu 11.38\n","2024-10-24 11:06:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 10322 updates\n","2024-10-24 11:06:41 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_last.pt\n","2024-10-24 11:06:42 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_last.pt\n","2024-10-24 11:06:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpeDROP/checkpoint_last.pt (epoch 2 @ 10322 updates, score 10.93) (writing took 0.3834153990010236 seconds)\n","2024-10-24 11:06:42 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n","2024-10-24 11:06:42 | INFO | train | epoch 002 | loss 5.982 | nll_loss 4.924 | ppl 30.36 | wps 26947.6 | ups 6.98 | wpb 3860.3 | bsz 101.7 | num_updates 10322 | lr 9.33769e-05 | gnorm 0.938 | train_wall 421 | gb_free 14.1 | wall 1466\n","2024-10-24 11:06:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 11:06:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5161\n","epoch 003:   0% 0/5161 [00:00\u003c?, ?it/s]2024-10-24 11:06:42 | INFO | fairseq.trainer | begin training epoch 3\n","2024-10-24 11:06:42 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 003:  32% 1676/5161 [02:28\u003c04:47, 12.13it/s, loss=5.675, nll_loss=4.567, ppl=23.7, wps=41702.8, ups=10.82, wpb=3854.1, bsz=104.4, num_updates=11900, lr=8.69657e-05, gnorm=1.023, train_wall=8, gb_free=14.1, wall=1607]2024-10-24 11:09:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 11:09:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 003 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   1% 1/84 [00:00\u003c00:52,  1.57it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   2% 2/84 [00:01\u003c01:10,  1.17it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   4% 3/84 [00:02\u003c01:22,  1.02s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:   5% 4/84 [00:04\u003c01:27,  1.09s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:   6% 5/84 [00:04\u003c01:12,  1.09it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   7% 6/84 [00:05\u003c01:03,  1.22it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   8% 7/84 [00:06\u003c01:16,  1.00it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  10% 8/84 [00:07\u003c01:08,  1.10it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  11% 9/84 [00:08\u003c01:03,  1.18it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  12% 10/84 [00:08\u003c00:53,  1.38it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  13% 11/84 [00:09\u003c00:51,  1.42it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  14% 12/84 [00:10\u003c00:53,  1.34it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  15% 13/84 [00:10\u003c00:50,  1.41it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  17% 14/84 [00:11\u003c00:42,  1.64it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  18% 15/84 [00:11\u003c00:43,  1.57it/s]\u001b[A\n","epoch 003:  32% 1676/5161 [02:41\u003c04:47, 12.13it/s, loss=5.626, nll_loss=4.51, ppl=22.79, wps=46932.6, ups=12.21, wpb=3845.3, bsz=107, num_updates=12000, lr=8.66025e-05, gnorm=1.02, train_wall=8, gb_free=14.1, wall=1615]   \n","epoch 003 | valid on 'valid' subset:  20% 17/84 [00:12\u003c00:41,  1.61it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  21% 18/84 [00:13\u003c00:37,  1.77it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  23% 19/84 [00:14\u003c00:39,  1.64it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  24% 20/84 [00:14\u003c00:35,  1.78it/s]\u001b[A2024-10-24 11:09:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:09:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:09:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 003 | valid on 'valid' subset:  25% 21/84 [00:15\u003c00:41,  1.51it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  26% 22/84 [00:16\u003c00:45,  1.38it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  27% 23/84 [00:17\u003c00:45,  1.33it/s]\u001b[A2024-10-24 11:09:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:09:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:09:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 003 | valid on 'valid' subset:  29% 24/84 [00:17\u003c00:44,  1.35it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  30% 25/84 [00:18\u003c00:42,  1.38it/s]\u001b[A2024-10-24 11:09:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:09:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:09:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 003 | valid on 'valid' subset:  31% 26/84 [00:19\u003c00:47,  1.21it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  32% 27/84 [00:20\u003c00:46,  1.23it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  33% 28/84 [00:21\u003c00:44,  1.25it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  35% 29/84 [00:22\u003c00:45,  1.20it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  36% 30/84 [00:22\u003c00:43,  1.23it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  37% 31/84 [00:23\u003c00:46,  1.14it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  38% 32/84 [00:24\u003c00:42,  1.23it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  39% 33/84 [00:25\u003c00:38,  1.31it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  40% 34/84 [00:25\u003c00:37,  1.33it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  42% 35/84 [00:26\u003c00:33,  1.45it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  43% 36/84 [00:27\u003c00:32,  1.46it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  44% 37/84 [00:27\u003c00:29,  1.57it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  45% 38/84 [00:28\u003c00:29,  1.57it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  46% 39/84 [00:28\u003c00:27,  1.66it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  48% 40/84 [00:29\u003c00:27,  1.57it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  49% 41/84 [00:30\u003c00:27,  1.55it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  50% 42/84 [00:30\u003c00:26,  1.57it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  51% 43/84 [00:31\u003c00:25,  1.61it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  52% 44/84 [00:31\u003c00:24,  1.64it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  54% 45/84 [00:32\u003c00:23,  1.67it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  55% 46/84 [00:33\u003c00:21,  1.74it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  56% 47/84 [00:34\u003c00:25,  1.45it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  57% 48/84 [00:34\u003c00:25,  1.39it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  58% 49/84 [00:35\u003c00:27,  1.25it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  60% 50/84 [00:36\u003c00:27,  1.25it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  61% 51/84 [00:37\u003c00:28,  1.17it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  62% 52/84 [00:38\u003c00:31,  1.03it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  63% 53/84 [00:39\u003c00:30,  1.03it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  64% 54/84 [00:40\u003c00:28,  1.06it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  65% 55/84 [00:41\u003c00:25,  1.14it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  67% 56/84 [00:42\u003c00:22,  1.24it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  68% 57/84 [00:42\u003c00:21,  1.26it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  69% 58/84 [00:43\u003c00:19,  1.32it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  70% 59/84 [00:44\u003c00:19,  1.28it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  71% 60/84 [00:44\u003c00:17,  1.36it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  73% 61/84 [00:45\u003c00:16,  1.41it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  74% 62/84 [00:46\u003c00:14,  1.48it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  75% 63/84 [00:46\u003c00:14,  1.46it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  76% 64/84 [00:47\u003c00:14,  1.43it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  77% 65/84 [00:48\u003c00:13,  1.40it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  79% 66/84 [00:49\u003c00:13,  1.30it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  80% 67/84 [00:50\u003c00:14,  1.17it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  81% 68/84 [00:51\u003c00:13,  1.16it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  82% 69/84 [00:52\u003c00:13,  1.14it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  83% 70/84 [00:53\u003c00:12,  1.12it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  85% 71/84 [00:54\u003c00:13,  1.01s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  86% 72/84 [00:55\u003c00:13,  1.13s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  87% 73/84 [00:56\u003c00:12,  1.11s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  88% 74/84 [00:57\u003c00:10,  1.00s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  89% 75/84 [00:58\u003c00:08,  1.09it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  90% 76/84 [00:58\u003c00:06,  1.19it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  92% 77/84 [00:59\u003c00:05,  1.24it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  93% 78/84 [01:00\u003c00:04,  1.25it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  94% 79/84 [01:01\u003c00:04,  1.20it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  95% 80/84 [01:02\u003c00:03,  1.17it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  96% 81/84 [01:02\u003c00:02,  1.21it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  98% 82/84 [01:03\u003c00:01,  1.18it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  99% 83/84 [01:04\u003c00:00,  1.17it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset: 100% 84/84 [01:05\u003c00:00,  1.13it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 11:10:16 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.284 | nll_loss 4.032 | ppl 16.36 | bleu 12.83 | wps 3094.2 | wpb 2417.9 | bsz 75.4 | num_updates 12000 | best_bleu 12.83\n","2024-10-24 11:10:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 12000 updates\n","2024-10-24 11:10:16 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_3_12000.pt\n","2024-10-24 11:10:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_3_12000.pt\n","2024-10-24 11:10:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpeDROP/checkpoint_3_12000.pt (epoch 3 @ 12000 updates, score 12.83) (writing took 1.435173453999596 seconds)\n","epoch 003:  71% 3676/5161 [06:33\u003c02:21, 10.52it/s, loss=5.557, nll_loss=4.429, ppl=21.54, wps=41219.4, ups=10.73, wpb=3840.2, bsz=99.4, num_updates=13900, lr=8.04663e-05, gnorm=1.045, train_wall=8, gb_free=14.1, wall=1852]2024-10-24 11:13:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 11:13:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 003 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   1% 1/84 [00:00\u003c01:20,  1.03it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   2% 2/84 [00:02\u003c01:43,  1.27s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:   4% 3/84 [00:03\u003c01:45,  1.31s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:   5% 4/84 [00:05\u003c01:47,  1.34s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:   6% 5/84 [00:05\u003c01:19,  1.01s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:   7% 6/84 [00:06\u003c01:03,  1.22it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   8% 7/84 [00:07\u003c01:07,  1.13it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  10% 8/84 [00:07\u003c00:59,  1.27it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  11% 9/84 [00:08\u003c00:50,  1.49it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  12% 10/84 [00:08\u003c00:43,  1.70it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  13% 11/84 [00:09\u003c00:42,  1.74it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  14% 12/84 [00:09\u003c00:39,  1.82it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  15% 13/84 [00:10\u003c00:39,  1.78it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  17% 14/84 [00:10\u003c00:35,  1.99it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  18% 15/84 [00:11\u003c00:35,  1.96it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  19% 16/84 [00:11\u003c00:33,  2.06it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  20% 17/84 [00:12\u003c00:34,  1.92it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  21% 18/84 [00:12\u003c00:31,  2.07it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  23% 19/84 [00:13\u003c00:34,  1.91it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  24% 20/84 [00:13\u003c00:31,  2.03it/s]\u001b[A2024-10-24 11:13:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:13:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:13:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 003 | valid on 'valid' subset:  25% 21/84 [00:14\u003c00:35,  1.76it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  26% 22/84 [00:14\u003c00:32,  1.92it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  27% 23/84 [00:15\u003c00:30,  2.01it/s]\u001b[A2024-10-24 11:13:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:13:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:13:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 003 | valid on 'valid' subset:  29% 24/84 [00:15\u003c00:35,  1.71it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  30% 25/84 [00:16\u003c00:36,  1.63it/s]\u001b[A2024-10-24 11:13:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:13:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:13:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 003:  71% 3676/5161 [06:51\u003c02:21, 10.52it/s, loss=5.521, nll_loss=4.388, ppl=20.94, wps=45236.6, ups=11.86, wpb=3813.7, bsz=108.2, num_updates=14000, lr=8.01784e-05, gnorm=1.055, train_wall=8, gb_free=14.1, wall=1860]\n","epoch 003 | valid on 'valid' subset:  32% 27/84 [00:18\u003c00:44,  1.28it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  33% 28/84 [00:19\u003c00:40,  1.39it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  35% 29/84 [00:19\u003c00:42,  1.28it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  36% 30/84 [00:20\u003c00:40,  1.33it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  37% 31/84 [00:21\u003c00:41,  1.27it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  38% 32/84 [00:21\u003c00:35,  1.45it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  39% 33/84 [00:22\u003c00:34,  1.48it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  40% 34/84 [00:23\u003c00:31,  1.61it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  42% 35/84 [00:23\u003c00:29,  1.69it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  43% 36/84 [00:24\u003c00:31,  1.54it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  44% 37/84 [00:24\u003c00:28,  1.64it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  45% 38/84 [00:25\u003c00:27,  1.67it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  46% 39/84 [00:26\u003c00:25,  1.75it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  48% 40/84 [00:26\u003c00:26,  1.68it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  49% 41/84 [00:27\u003c00:24,  1.72it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  50% 42/84 [00:27\u003c00:25,  1.66it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  51% 43/84 [00:28\u003c00:24,  1.71it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  52% 44/84 [00:29\u003c00:23,  1.69it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  54% 45/84 [00:29\u003c00:23,  1.68it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  55% 46/84 [00:30\u003c00:21,  1.80it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  56% 47/84 [00:30\u003c00:22,  1.68it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  57% 48/84 [00:31\u003c00:21,  1.65it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  58% 49/84 [00:32\u003c00:26,  1.34it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  60% 50/84 [00:33\u003c00:25,  1.32it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  61% 51/84 [00:34\u003c00:26,  1.23it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  62% 52/84 [00:35\u003c00:27,  1.18it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  63% 53/84 [00:36\u003c00:26,  1.17it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  64% 54/84 [00:36\u003c00:25,  1.17it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  65% 55/84 [00:37\u003c00:23,  1.22it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  67% 56/84 [00:38\u003c00:22,  1.22it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  68% 57/84 [00:39\u003c00:21,  1.26it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  69% 58/84 [00:39\u003c00:18,  1.39it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  70% 59/84 [00:40\u003c00:17,  1.42it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  71% 60/84 [00:41\u003c00:16,  1.45it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  73% 61/84 [00:41\u003c00:15,  1.50it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  74% 62/84 [00:42\u003c00:14,  1.57it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  75% 63/84 [00:43\u003c00:14,  1.46it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  76% 64/84 [00:43\u003c00:13,  1.51it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  77% 65/84 [00:44\u003c00:12,  1.52it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  79% 66/84 [00:44\u003c00:12,  1.47it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  80% 67/84 [00:45\u003c00:11,  1.54it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  81% 68/84 [00:46\u003c00:10,  1.58it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  82% 69/84 [00:46\u003c00:09,  1.50it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  83% 70/84 [00:47\u003c00:10,  1.35it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  85% 71/84 [00:48\u003c00:11,  1.17it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  86% 72/84 [00:49\u003c00:10,  1.17it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  87% 73/84 [00:50\u003c00:10,  1.06it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  88% 74/84 [00:52\u003c00:09,  1.02it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  89% 75/84 [00:53\u003c00:08,  1.02it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  90% 76/84 [00:53\u003c00:07,  1.14it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  92% 77/84 [00:54\u003c00:06,  1.16it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  93% 78/84 [00:55\u003c00:05,  1.15it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  94% 79/84 [00:56\u003c00:04,  1.17it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  95% 80/84 [00:57\u003c00:03,  1.15it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  96% 81/84 [00:57\u003c00:02,  1.19it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  98% 82/84 [00:58\u003c00:01,  1.19it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  99% 83/84 [00:59\u003c00:00,  1.15it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset: 100% 84/84 [01:00\u003c00:00,  1.17it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 11:14:16 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.188 | nll_loss 3.917 | ppl 15.11 | bleu 13.16 | wps 3384.7 | wpb 2417.9 | bsz 75.4 | num_updates 14000 | best_bleu 13.16\n","2024-10-24 11:14:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 14000 updates\n","2024-10-24 11:14:16 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_3_14000.pt\n","2024-10-24 11:14:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_3_14000.pt\n","2024-10-24 11:14:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpeDROP/checkpoint_3_14000.pt (epoch 3 @ 14000 updates, score 13.16) (writing took 1.2949251339996408 seconds)\n","epoch 003: 100% 5160/5161 [09:48\u003c00:00, 11.26it/s, loss=5.5, nll_loss=4.363, ppl=20.57, wps=45056.3, ups=11.69, wpb=3854.8, bsz=105, num_updates=15400, lr=7.64471e-05, gnorm=1.077, train_wall=8, gb_free=14.1, wall=2048]2024-10-24 11:16:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 11:16:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 003 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   1% 1/84 [00:00\u003c01:16,  1.08it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   2% 2/84 [00:02\u003c01:34,  1.15s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:   4% 3/84 [00:03\u003c01:40,  1.24s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:   5% 4/84 [00:04\u003c01:18,  1.02it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   6% 5/84 [00:04\u003c01:01,  1.29it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   7% 6/84 [00:04\u003c00:49,  1.56it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   8% 7/84 [00:05\u003c00:55,  1.40it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  10% 8/84 [00:06\u003c00:47,  1.61it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  11% 9/84 [00:06\u003c00:42,  1.76it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  12% 10/84 [00:07\u003c00:36,  2.01it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  13% 11/84 [00:07\u003c00:35,  2.06it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  14% 12/84 [00:07\u003c00:33,  2.13it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  15% 13/84 [00:08\u003c00:33,  2.14it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  17% 14/84 [00:08\u003c00:30,  2.30it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  18% 15/84 [00:09\u003c00:30,  2.23it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  19% 16/84 [00:09\u003c00:29,  2.32it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  20% 17/84 [00:10\u003c00:30,  2.22it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  21% 18/84 [00:10\u003c00:27,  2.37it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  23% 19/84 [00:10\u003c00:29,  2.21it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  24% 20/84 [00:11\u003c00:27,  2.33it/s]\u001b[A2024-10-24 11:16:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:16:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:16:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 003 | valid on 'valid' subset:  25% 21/84 [00:11\u003c00:29,  2.11it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  26% 22/84 [00:12\u003c00:29,  2.09it/s]\u001b[A\n","epoch 003: 100% 5160/5161 [10:01\u003c00:00, 11.26it/s, loss=5.5, nll_loss=4.363, ppl=20.57, wps=45056.3, ups=11.69, wpb=3854.8, bsz=105, num_updates=15400, lr=7.64471e-05, gnorm=1.077, train_wall=8, gb_free=14.1, wall=2048]2024-10-24 11:16:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:16:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:16:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 003 | valid on 'valid' subset:  29% 24/84 [00:13\u003c00:30,  1.97it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  30% 25/84 [00:14\u003c00:31,  1.86it/s]\u001b[A2024-10-24 11:16:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:16:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:16:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 003 | valid on 'valid' subset:  31% 26/84 [00:14\u003c00:34,  1.68it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  32% 27/84 [00:15\u003c00:35,  1.60it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  33% 28/84 [00:16\u003c00:33,  1.68it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  35% 29/84 [00:16\u003c00:35,  1.54it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  36% 30/84 [00:17\u003c00:34,  1.56it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  37% 31/84 [00:18\u003c00:38,  1.36it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  38% 32/84 [00:19\u003c00:37,  1.38it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  39% 33/84 [00:19\u003c00:39,  1.30it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  40% 34/84 [00:20\u003c00:34,  1.44it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  42% 35/84 [00:20\u003c00:30,  1.61it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  43% 36/84 [00:21\u003c00:28,  1.67it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  44% 37/84 [00:21\u003c00:26,  1.77it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  45% 38/84 [00:22\u003c00:26,  1.71it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  46% 39/84 [00:23\u003c00:25,  1.80it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  48% 40/84 [00:23\u003c00:24,  1.78it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  49% 41/84 [00:24\u003c00:23,  1.85it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  50% 42/84 [00:24\u003c00:23,  1.75it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  51% 43/84 [00:25\u003c00:22,  1.81it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  52% 44/84 [00:25\u003c00:22,  1.76it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  54% 45/84 [00:26\u003c00:21,  1.77it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  55% 46/84 [00:26\u003c00:20,  1.82it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  56% 47/84 [00:27\u003c00:24,  1.52it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  57% 48/84 [00:28\u003c00:21,  1.64it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  58% 49/84 [00:29\u003c00:21,  1.60it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  60% 50/84 [00:29\u003c00:20,  1.69it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  61% 51/84 [00:30\u003c00:20,  1.62it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  62% 52/84 [00:31\u003c00:22,  1.45it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  63% 53/84 [00:31\u003c00:22,  1.36it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  64% 54/84 [00:32\u003c00:21,  1.38it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  65% 55/84 [00:33\u003c00:22,  1.27it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  67% 56/84 [00:34\u003c00:22,  1.23it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  68% 57/84 [00:35\u003c00:23,  1.17it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  69% 58/84 [00:36\u003c00:20,  1.25it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  70% 59/84 [00:36\u003c00:19,  1.31it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  71% 60/84 [00:37\u003c00:16,  1.45it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  73% 61/84 [00:37\u003c00:15,  1.50it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  74% 62/84 [00:38\u003c00:14,  1.53it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  75% 63/84 [00:39\u003c00:13,  1.54it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  76% 64/84 [00:39\u003c00:12,  1.57it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  77% 65/84 [00:40\u003c00:12,  1.57it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  79% 66/84 [00:41\u003c00:11,  1.50it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  80% 67/84 [00:41\u003c00:11,  1.54it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  81% 68/84 [00:42\u003c00:10,  1.50it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  82% 69/84 [00:43\u003c00:10,  1.46it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  83% 70/84 [00:43\u003c00:09,  1.46it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  85% 71/84 [00:44\u003c00:08,  1.46it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  86% 72/84 [00:45\u003c00:08,  1.44it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  87% 73/84 [00:46\u003c00:08,  1.23it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  88% 74/84 [00:47\u003c00:08,  1.17it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  89% 75/84 [00:48\u003c00:07,  1.13it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  90% 76/84 [00:49\u003c00:07,  1.06it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  92% 77/84 [00:50\u003c00:07,  1.02s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  93% 78/84 [00:51\u003c00:06,  1.11s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  94% 79/84 [00:52\u003c00:05,  1.04s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  95% 80/84 [00:53\u003c00:03,  1.02it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  96% 81/84 [00:54\u003c00:02,  1.08it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  98% 82/84 [00:55\u003c00:01,  1.07it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  99% 83/84 [00:56\u003c00:00,  1.09it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset: 100% 84/84 [00:57\u003c00:00,  1.12it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 11:17:27 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.15 | nll_loss 3.871 | ppl 14.63 | bleu 13.55 | wps 3590.4 | wpb 2417.9 | bsz 75.4 | num_updates 15483 | best_bleu 13.55\n","2024-10-24 11:17:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 15483 updates\n","2024-10-24 11:17:27 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_best.pt\n","2024-10-24 11:17:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_best.pt\n","2024-10-24 11:17:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpeDROP/checkpoint_best.pt (epoch 3 @ 15483 updates, score 13.55) (writing took 0.8425997609992919 seconds)\n","2024-10-24 11:17:28 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n","2024-10-24 11:17:28 | INFO | train | epoch 003 | loss 5.607 | nll_loss 4.488 | ppl 22.43 | wps 30814.5 | ups 7.98 | wpb 3860.3 | bsz 101.7 | num_updates 15483 | lr 7.62419e-05 | gnorm 1.024 | train_wall 420 | gb_free 14.1 | wall 2113\n","2024-10-24 11:17:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 11:17:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5161\n","epoch 004:   0% 0/5161 [00:00\u003c?, ?it/s]2024-10-24 11:17:28 | INFO | fairseq.trainer | begin training epoch 4\n","2024-10-24 11:17:28 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 004:  10% 515/5161 [00:46\u003c06:28, 11.96it/s, loss=5.442, nll_loss=4.295, ppl=19.63, wps=45542.3, ups=11.72, wpb=3886.7, bsz=102.6, num_updates=15900, lr=7.52355e-05, gnorm=1.062, train_wall=8, gb_free=14.1, wall=2151]2024-10-24 11:18:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 11:18:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 004 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   1% 1/84 [00:00\u003c00:56,  1.47it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   2% 2/84 [00:01\u003c00:43,  1.90it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   4% 3/84 [00:01\u003c00:53,  1.52it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   5% 4/84 [00:02\u003c00:50,  1.59it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   6% 5/84 [00:02\u003c00:44,  1.78it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   7% 6/84 [00:03\u003c00:37,  2.07it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   8% 7/84 [00:03\u003c00:35,  2.15it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  10% 8/84 [00:04\u003c00:36,  2.11it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  11% 9/84 [00:04\u003c00:34,  2.16it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  12% 10/84 [00:05\u003c00:32,  2.29it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  13% 11/84 [00:05\u003c00:35,  2.04it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  14% 12/84 [00:06\u003c00:38,  1.89it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  15% 13/84 [00:07\u003c00:43,  1.64it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  17% 14/84 [00:07\u003c00:39,  1.75it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  18% 15/84 [00:08\u003c00:46,  1.50it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  19% 16/84 [00:09\u003c00:43,  1.55it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  20% 17/84 [00:09\u003c00:46,  1.45it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  21% 18/84 [00:10\u003c00:44,  1.50it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  23% 19/84 [00:11\u003c00:44,  1.45it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  24% 20/84 [00:11\u003c00:38,  1.67it/s]\u001b[A2024-10-24 11:18:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:18:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:18:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 004 | valid on 'valid' subset:  25% 21/84 [00:12\u003c00:38,  1.62it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  26% 22/84 [00:12\u003c00:35,  1.77it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  27% 23/84 [00:13\u003c00:31,  1.92it/s]\u001b[A2024-10-24 11:18:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:18:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:18:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 004 | valid on 'valid' subset:  29% 24/84 [00:13\u003c00:31,  1.90it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  30% 25/84 [00:14\u003c00:29,  1.98it/s]\u001b[A2024-10-24 11:18:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:18:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:18:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 004 | valid on 'valid' subset:  31% 26/84 [00:14\u003c00:31,  1.87it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  32% 27/84 [00:15\u003c00:31,  1.83it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  33% 28/84 [00:15\u003c00:28,  1.96it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  35% 29/84 [00:16\u003c00:34,  1.58it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  36% 30/84 [00:17\u003c00:31,  1.73it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  37% 31/84 [00:17\u003c00:31,  1.69it/s]\u001b[A\n","epoch 004:  10% 515/5161 [01:05\u003c06:28, 11.96it/s, loss=5.429, nll_loss=4.28, ppl=19.42, wps=44071.8, ups=11.31, wpb=3897.4, bsz=107.5, num_updates=16000, lr=7.5e-05, gnorm=1.054, train_wall=8, gb_free=14, wall=2160]       \n","epoch 004 | valid on 'valid' subset:  39% 33/84 [00:18\u003c00:28,  1.77it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  40% 34/84 [00:19\u003c00:28,  1.76it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  42% 35/84 [00:19\u003c00:26,  1.84it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  43% 36/84 [00:20\u003c00:26,  1.79it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  44% 37/84 [00:20\u003c00:25,  1.84it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  45% 38/84 [00:21\u003c00:28,  1.60it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  46% 39/84 [00:22\u003c00:29,  1.51it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  48% 40/84 [00:23\u003c00:31,  1.38it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  49% 41/84 [00:24\u003c00:32,  1.32it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  50% 42/84 [00:25\u003c00:34,  1.23it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  51% 43/84 [00:25\u003c00:33,  1.22it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  52% 44/84 [00:26\u003c00:34,  1.16it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  54% 45/84 [00:27\u003c00:31,  1.25it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  55% 46/84 [00:28\u003c00:26,  1.42it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  56% 47/84 [00:28\u003c00:24,  1.48it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  57% 48/84 [00:29\u003c00:22,  1.62it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  58% 49/84 [00:29\u003c00:22,  1.54it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  60% 50/84 [00:30\u003c00:21,  1.58it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  61% 51/84 [00:31\u003c00:20,  1.59it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  62% 52/84 [00:31\u003c00:19,  1.62it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  63% 53/84 [00:32\u003c00:19,  1.59it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  64% 54/84 [00:32\u003c00:18,  1.65it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  65% 55/84 [00:33\u003c00:17,  1.62it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  67% 56/84 [00:34\u003c00:16,  1.67it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  68% 57/84 [00:34\u003c00:16,  1.63it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  69% 58/84 [00:35\u003c00:15,  1.64it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  70% 59/84 [00:35\u003c00:15,  1.59it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  71% 60/84 [00:36\u003c00:14,  1.63it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  73% 61/84 [00:37\u003c00:14,  1.61it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  74% 62/84 [00:37\u003c00:14,  1.50it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  75% 63/84 [00:38\u003c00:15,  1.37it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  76% 64/84 [00:39\u003c00:15,  1.30it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  77% 65/84 [00:40\u003c00:16,  1.19it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  79% 66/84 [00:41\u003c00:15,  1.14it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  80% 67/84 [00:42\u003c00:14,  1.15it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  81% 68/84 [00:43\u003c00:14,  1.08it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  82% 69/84 [00:44\u003c00:13,  1.12it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  83% 70/84 [00:45\u003c00:12,  1.16it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  85% 71/84 [00:45\u003c00:10,  1.27it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  86% 72/84 [00:46\u003c00:08,  1.36it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  87% 73/84 [00:47\u003c00:08,  1.36it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  88% 74/84 [00:47\u003c00:07,  1.38it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  89% 75/84 [00:48\u003c00:06,  1.38it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  90% 76/84 [00:49\u003c00:05,  1.43it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  92% 77/84 [00:50\u003c00:05,  1.34it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  93% 78/84 [00:51\u003c00:04,  1.24it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  94% 79/84 [00:51\u003c00:04,  1.20it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  95% 80/84 [00:52\u003c00:03,  1.21it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  96% 81/84 [00:53\u003c00:02,  1.23it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  98% 82/84 [00:54\u003c00:01,  1.11it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  99% 83/84 [00:55\u003c00:00,  1.00it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset: 100% 84/84 [00:56\u003c00:00,  1.05s/it]\u001b[A\n","                                                                        \u001b[A2024-10-24 11:19:12 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.104 | nll_loss 3.818 | ppl 14.11 | bleu 14.07 | wps 3574.8 | wpb 2417.9 | bsz 75.4 | num_updates 16000 | best_bleu 14.07\n","2024-10-24 11:19:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 16000 updates\n","2024-10-24 11:19:12 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_4_16000.pt\n","2024-10-24 11:19:13 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_4_16000.pt\n","2024-10-24 11:19:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpeDROP/checkpoint_4_16000.pt (epoch 4 @ 16000 updates, score 14.07) (writing took 1.7781437070007087 seconds)\n","epoch 004:  49% 2515/5161 [04:43\u003c03:36, 12.20it/s, loss=5.387, nll_loss=4.232, ppl=18.79, wps=46781.6, ups=12.23, wpb=3824.2, bsz=101.8, num_updates=17900, lr=7.09079e-05, gnorm=1.096, train_wall=8, gb_free=14, wall=2387]2024-10-24 11:22:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 11:22:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 004 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   1% 1/84 [00:00\u003c00:59,  1.39it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   2% 2/84 [00:01\u003c00:45,  1.82it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   4% 3/84 [00:01\u003c00:43,  1.87it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   5% 4/84 [00:02\u003c00:39,  2.01it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   6% 5/84 [00:02\u003c00:36,  2.18it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   7% 6/84 [00:02\u003c00:34,  2.28it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   8% 7/84 [00:03\u003c00:37,  2.06it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  10% 8/84 [00:04\u003c00:38,  2.00it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  11% 9/84 [00:04\u003c00:36,  2.06it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  12% 10/84 [00:04\u003c00:35,  2.06it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  13% 11/84 [00:05\u003c00:35,  2.03it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  14% 12/84 [00:06\u003c00:37,  1.93it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  15% 13/84 [00:06\u003c00:37,  1.92it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  17% 14/84 [00:06\u003c00:33,  2.06it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  18% 15/84 [00:07\u003c00:35,  1.94it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  19% 16/84 [00:08\u003c00:36,  1.85it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  20% 17/84 [00:08\u003c00:41,  1.62it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  21% 18/84 [00:09\u003c00:39,  1.66it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  23% 19/84 [00:10\u003c00:43,  1.51it/s]\u001b[A\n","epoch 004:  49% 2515/5161 [04:55\u003c03:36, 12.20it/s, loss=5.408, nll_loss=4.256, ppl=19.1, wps=41632.3, ups=10.91, wpb=3815.6, bsz=94.7, num_updates=18000, lr=7.07107e-05, gnorm=1.127, train_wall=8, gb_free=14.1, wall=2397]2024-10-24 11:22:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:22:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:22:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 004 | valid on 'valid' subset:  25% 21/84 [00:12\u003c00:50,  1.25it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  26% 22/84 [00:12\u003c00:47,  1.30it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  27% 23/84 [00:13\u003c00:44,  1.36it/s]\u001b[A2024-10-24 11:22:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:22:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:22:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 004 | valid on 'valid' subset:  29% 24/84 [00:14\u003c00:43,  1.37it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  30% 25/84 [00:14\u003c00:39,  1.51it/s]\u001b[A2024-10-24 11:22:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:22:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:22:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 004 | valid on 'valid' subset:  31% 26/84 [00:15\u003c00:40,  1.42it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  32% 27/84 [00:16\u003c00:38,  1.48it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  33% 28/84 [00:16\u003c00:34,  1.64it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  35% 29/84 [00:17\u003c00:33,  1.64it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  36% 30/84 [00:17\u003c00:31,  1.70it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  37% 31/84 [00:18\u003c00:33,  1.57it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  38% 32/84 [00:18\u003c00:31,  1.65it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  39% 33/84 [00:19\u003c00:30,  1.66it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  40% 34/84 [00:20\u003c00:31,  1.61it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  42% 35/84 [00:20\u003c00:29,  1.66it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  43% 36/84 [00:21\u003c00:29,  1.61it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  44% 37/84 [00:22\u003c00:28,  1.63it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  45% 38/84 [00:22\u003c00:30,  1.51it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  46% 39/84 [00:23\u003c00:28,  1.60it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  48% 40/84 [00:24\u003c00:31,  1.42it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  49% 41/84 [00:25\u003c00:31,  1.36it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  50% 42/84 [00:25\u003c00:33,  1.27it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  51% 43/84 [00:27\u003c00:37,  1.09it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  52% 44/84 [00:28\u003c00:36,  1.09it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  54% 45/84 [00:28\u003c00:34,  1.12it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  55% 46/84 [00:29\u003c00:32,  1.15it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  56% 47/84 [00:30\u003c00:32,  1.13it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  57% 48/84 [00:31\u003c00:28,  1.28it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  58% 49/84 [00:32\u003c00:28,  1.25it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  60% 50/84 [00:32\u003c00:25,  1.32it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  61% 51/84 [00:33\u003c00:24,  1.37it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  62% 52/84 [00:34\u003c00:22,  1.41it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  63% 53/84 [00:34\u003c00:21,  1.44it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  64% 54/84 [00:35\u003c00:22,  1.33it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  65% 55/84 [00:36\u003c00:22,  1.30it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  67% 56/84 [00:37\u003c00:20,  1.36it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  68% 57/84 [00:37\u003c00:19,  1.36it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  69% 58/84 [00:38\u003c00:18,  1.42it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  70% 59/84 [00:39\u003c00:18,  1.38it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  71% 60/84 [00:39\u003c00:16,  1.43it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  73% 61/84 [00:40\u003c00:16,  1.37it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  74% 62/84 [00:41\u003c00:16,  1.31it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  75% 63/84 [00:42\u003c00:17,  1.21it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  76% 64/84 [00:43\u003c00:17,  1.11it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  77% 65/84 [00:44\u003c00:18,  1.04it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  79% 66/84 [00:45\u003c00:17,  1.02it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  80% 67/84 [00:46\u003c00:16,  1.01it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  81% 68/84 [00:47\u003c00:14,  1.07it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  82% 69/84 [00:48\u003c00:13,  1.09it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  83% 70/84 [00:49\u003c00:12,  1.13it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  85% 71/84 [00:49\u003c00:10,  1.19it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  86% 72/84 [00:50\u003c00:09,  1.23it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  87% 73/84 [00:51\u003c00:09,  1.22it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  88% 74/84 [00:52\u003c00:08,  1.23it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  89% 75/84 [00:52\u003c00:07,  1.28it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  90% 76/84 [00:53\u003c00:06,  1.32it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  92% 77/84 [00:54\u003c00:05,  1.25it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  93% 78/84 [00:55\u003c00:04,  1.22it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  94% 79/84 [00:56\u003c00:04,  1.16it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  95% 80/84 [00:57\u003c00:03,  1.07it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  96% 81/84 [00:58\u003c00:02,  1.02it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  98% 82/84 [00:59\u003c00:02,  1.04s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  99% 83/84 [01:01\u003c00:01,  1.11s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset: 100% 84/84 [01:02\u003c00:00,  1.12s/it]\u001b[A\n","                                                                        \u001b[A2024-10-24 11:23:14 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.031 | nll_loss 3.729 | ppl 13.26 | bleu 15.3 | wps 3274.6 | wpb 2417.9 | bsz 75.4 | num_updates 18000 | best_bleu 15.3\n","2024-10-24 11:23:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 18000 updates\n","2024-10-24 11:23:14 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_4_18000.pt\n","2024-10-24 11:23:15 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_4_18000.pt\n","2024-10-24 11:23:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpeDROP/checkpoint_4_18000.pt (epoch 4 @ 18000 updates, score 15.3) (writing took 1.5412309460007236 seconds)\n","epoch 004:  88% 4516/5161 [08:44\u003c01:18,  8.24it/s, loss=5.343, nll_loss=4.179, ppl=18.11, wps=43675.4, ups=11.21, wpb=3896, bsz=93.7, num_updates=19900, lr=6.72504e-05, gnorm=1.091, train_wall=8, gb_free=14.1, wall=2628]2024-10-24 11:26:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 11:26:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 004 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   1% 1/84 [00:01\u003c01:35,  1.15s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:   2% 2/84 [00:02\u003c01:20,  1.02it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   4% 3/84 [00:02\u003c01:14,  1.09it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   5% 4/84 [00:03\u003c01:16,  1.04it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   6% 5/84 [00:04\u003c01:10,  1.13it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   7% 6/84 [00:05\u003c01:03,  1.23it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   8% 7/84 [00:05\u003c00:58,  1.32it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  10% 8/84 [00:06\u003c00:59,  1.28it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  11% 9/84 [00:07\u003c00:54,  1.37it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  12% 10/84 [00:07\u003c00:49,  1.49it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  13% 11/84 [00:08\u003c00:48,  1.50it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  14% 12/84 [00:09\u003c00:45,  1.57it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  15% 13/84 [00:09\u003c00:42,  1.68it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  17% 14/84 [00:10\u003c00:36,  1.92it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  18% 15/84 [00:10\u003c00:36,  1.89it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  19% 16/84 [00:10\u003c00:34,  2.00it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  20% 17/84 [00:11\u003c00:34,  1.95it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  21% 18/84 [00:11\u003c00:31,  2.06it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  23% 19/84 [00:12\u003c00:32,  2.01it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  24% 20/84 [00:12\u003c00:30,  2.10it/s]\u001b[A2024-10-24 11:26:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:26:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:26:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 004 | valid on 'valid' subset:  25% 21/84 [00:13\u003c00:32,  1.93it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  26% 22/84 [00:13\u003c00:30,  2.02it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  27% 23/84 [00:14\u003c00:28,  2.12it/s]\u001b[A2024-10-24 11:26:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:26:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:26:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 004 | valid on 'valid' subset:  29% 24/84 [00:14\u003c00:30,  1.99it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  30% 25/84 [00:15\u003c00:29,  1.97it/s]\u001b[A2024-10-24 11:26:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:26:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:26:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 004 | valid on 'valid' subset:  31% 26/84 [00:16\u003c00:31,  1.85it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  32% 27/84 [00:16\u003c00:30,  1.88it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  33% 28/84 [00:17\u003c00:30,  1.85it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  35% 29/84 [00:17\u003c00:33,  1.63it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  36% 30/84 [00:18\u003c00:33,  1.60it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  37% 31/84 [00:19\u003c00:36,  1.46it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  38% 32/84 [00:20\u003c00:35,  1.45it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  39% 33/84 [00:21\u003c00:38,  1.33it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  40% 34/84 [00:21\u003c00:36,  1.35it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  42% 35/84 [00:22\u003c00:36,  1.36it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  43% 36/84 [00:23\u003c00:35,  1.36it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  44% 37/84 [00:23\u003c00:31,  1.48it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  45% 38/84 [00:24\u003c00:29,  1.55it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  46% 39/84 [00:24\u003c00:27,  1.62it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  48% 40/84 [00:25\u003c00:26,  1.64it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  49% 41/84 [00:26\u003c00:25,  1.66it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  50% 42/84 [00:26\u003c00:25,  1.66it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  51% 43/84 [00:27\u003c00:24,  1.67it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  52% 44/84 [00:27\u003c00:23,  1.70it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  54% 45/84 [00:28\u003c00:21,  1.78it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  55% 46/84 [00:28\u003c00:20,  1.90it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  56% 47/84 [00:29\u003c00:20,  1.77it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  57% 48/84 [00:29\u003c00:19,  1.81it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  58% 49/84 [00:30\u003c00:20,  1.75it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  60% 50/84 [00:31\u003c00:19,  1.74it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  61% 51/84 [00:31\u003c00:18,  1.74it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  62% 52/84 [00:32\u003c00:18,  1.69it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  63% 53/84 [00:33\u003c00:19,  1.61it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  64% 54/84 [00:33\u003c00:19,  1.54it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  65% 55/84 [00:34\u003c00:20,  1.42it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  67% 56/84 [00:35\u003c00:19,  1.41it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  68% 57/84 [00:36\u003c00:21,  1.27it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  69% 58/84 [00:37\u003c00:20,  1.24it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  70% 59/84 [00:37\u003c00:20,  1.22it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  71% 60/84 [00:38\u003c00:19,  1.21it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  73% 61/84 [00:39\u003c00:18,  1.26it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  74% 62/84 [00:40\u003c00:15,  1.40it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  75% 63/84 [00:40\u003c00:14,  1.44it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  76% 64/84 [00:41\u003c00:13,  1.46it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  77% 65/84 [00:41\u003c00:12,  1.49it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  79% 66/84 [00:42\u003c00:12,  1.49it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  80% 67/84 [00:43\u003c00:11,  1.51it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  81% 68/84 [00:43\u003c00:10,  1.49it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  82% 69/84 [00:44\u003c00:09,  1.50it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  83% 70/84 [00:45\u003c00:09,  1.50it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  85% 71/84 [00:45\u003c00:08,  1.53it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  86% 72/84 [00:46\u003c00:07,  1.55it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  87% 73/84 [00:47\u003c00:07,  1.53it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  88% 74/84 [00:47\u003c00:06,  1.56it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  89% 75/84 [00:48\u003c00:05,  1.56it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  90% 76/84 [00:49\u003c00:05,  1.56it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  92% 77/84 [00:50\u003c00:05,  1.25it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  93% 78/84 [00:51\u003c00:05,  1.16it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  94% 79/84 [00:52\u003c00:04,  1.09it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  95% 80/84 [00:53\u003c00:03,  1.05it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  96% 81/84 [00:54\u003c00:03,  1.01s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  98% 82/84 [00:55\u003c00:02,  1.02s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  99% 83/84 [00:56\u003c00:00,  1.02it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset: 100% 84/84 [00:57\u003c00:00,  1.07it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 11:27:10 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 4.972 | nll_loss 3.657 | ppl 12.62 | bleu 15.53 | wps 3586.9 | wpb 2417.9 | bsz 75.4 | num_updates 20000 | best_bleu 15.53\n","2024-10-24 11:27:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 20000 updates\n","2024-10-24 11:27:10 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_4_20000.pt\n","2024-10-24 11:27:10 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_4_20000.pt\n","2024-10-24 11:27:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpeDROP/checkpoint_4_20000.pt (epoch 4 @ 20000 updates, score 15.53) (writing took 1.264878599999065 seconds)\n","epoch 004: 100% 5159/5161 [10:41\u003c00:00, 11.72it/s, loss=5.318, nll_loss=4.153, ppl=17.79, wps=46970.9, ups=12.17, wpb=3861, bsz=98.2, num_updates=20600, lr=6.60979e-05, gnorm=1.118, train_wall=8, gb_free=14.1, wall=2750]2024-10-24 11:28:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 11:28:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 004 | valid on 'valid' subset:   0% 0/84 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   1% 1/84 [00:00\u003c01:15,  1.10it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   2% 2/84 [00:01\u003c01:01,  1.32it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   4% 3/84 [00:02\u003c01:01,  1.32it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   5% 4/84 [00:03\u003c01:03,  1.27it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   6% 5/84 [00:04\u003c01:05,  1.20it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   7% 6/84 [00:04\u003c00:53,  1.45it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   8% 7/84 [00:04\u003c00:46,  1.65it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  10% 8/84 [00:05\u003c00:41,  1.83it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  11% 9/84 [00:05\u003c00:37,  2.03it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  12% 10/84 [00:06\u003c00:34,  2.16it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  13% 11/84 [00:06\u003c00:33,  2.17it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  14% 12/84 [00:06\u003c00:32,  2.21it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  15% 13/84 [00:07\u003c00:33,  2.14it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  17% 14/84 [00:08\u003c00:36,  1.91it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  18% 15/84 [00:08\u003c00:35,  1.94it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  19% 16/84 [00:09\u003c00:33,  2.03it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  20% 17/84 [00:09\u003c00:33,  2.01it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  21% 18/84 [00:10\u003c00:31,  2.10it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  23% 19/84 [00:10\u003c00:32,  2.00it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  24% 20/84 [00:10\u003c00:30,  2.13it/s]\u001b[A2024-10-24 11:28:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:28:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:28:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 004 | valid on 'valid' subset:  25% 21/84 [00:11\u003c00:30,  2.04it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  26% 22/84 [00:11\u003c00:29,  2.13it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  27% 23/84 [00:12\u003c00:27,  2.19it/s]\u001b[A2024-10-24 11:28:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:28:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:28:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 004 | valid on 'valid' subset:  29% 24/84 [00:12\u003c00:29,  2.03it/s]\u001b[A\n","epoch 004: 100% 5159/5161 [10:55\u003c00:00, 11.72it/s, loss=5.318, nll_loss=4.153, ppl=17.79, wps=46970.9, ups=12.17, wpb=3861, bsz=98.2, num_updates=20600, lr=6.60979e-05, gnorm=1.118, train_wall=8, gb_free=14.1, wall=2750]2024-10-24 11:28:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 11:28:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 11:28:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 004 | valid on 'valid' subset:  31% 26/84 [00:14\u003c00:31,  1.86it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  32% 27/84 [00:14\u003c00:33,  1.72it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  33% 28/84 [00:15\u003c00:31,  1.76it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  35% 29/84 [00:16\u003c00:34,  1.58it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  36% 30/84 [00:16\u003c00:34,  1.55it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  37% 31/84 [00:17\u003c00:37,  1.41it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  38% 32/84 [00:18\u003c00:35,  1.46it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  39% 33/84 [00:19\u003c00:38,  1.33it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  40% 34/84 [00:19\u003c00:36,  1.36it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  42% 35/84 [00:20\u003c00:34,  1.42it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  43% 36/84 [00:21\u003c00:32,  1.49it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  44% 37/84 [00:21\u003c00:28,  1.63it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  45% 38/84 [00:22\u003c00:27,  1.68it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  46% 39/84 [00:22\u003c00:25,  1.78it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  48% 40/84 [00:23\u003c00:25,  1.73it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  49% 41/84 [00:23\u003c00:23,  1.80it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  50% 42/84 [00:24\u003c00:24,  1.71it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  51% 43/84 [00:24\u003c00:23,  1.75it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  52% 44/84 [00:25\u003c00:23,  1.73it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  54% 45/84 [00:25\u003c00:21,  1.81it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  55% 46/84 [00:26\u003c00:20,  1.87it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  56% 47/84 [00:27\u003c00:20,  1.84it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  57% 48/84 [00:27\u003c00:19,  1.87it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  58% 49/84 [00:28\u003c00:19,  1.83it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  60% 50/84 [00:28\u003c00:18,  1.84it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  61% 51/84 [00:29\u003c00:19,  1.66it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  62% 52/84 [00:29\u003c00:18,  1.70it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  63% 53/84 [00:30\u003c00:19,  1.57it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  64% 54/84 [00:31\u003c00:19,  1.50it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  65% 55/84 [00:32\u003c00:21,  1.38it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  67% 56/84 [00:33\u003c00:20,  1.34it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  68% 57/84 [00:34\u003c00:21,  1.24it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  69% 58/84 [00:34\u003c00:21,  1.21it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  70% 59/84 [00:35\u003c00:21,  1.17it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  71% 60/84 [00:36\u003c00:18,  1.27it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  73% 61/84 [00:37\u003c00:16,  1.38it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  74% 62/84 [00:37\u003c00:14,  1.50it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  75% 63/84 [00:38\u003c00:13,  1.55it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  76% 64/84 [00:38\u003c00:12,  1.55it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  77% 65/84 [00:39\u003c00:12,  1.54it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  79% 66/84 [00:40\u003c00:11,  1.51it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  80% 67/84 [00:40\u003c00:11,  1.51it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  81% 68/84 [00:41\u003c00:10,  1.50it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  82% 69/84 [00:42\u003c00:10,  1.50it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  83% 70/84 [00:42\u003c00:09,  1.43it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  85% 71/84 [00:43\u003c00:08,  1.46it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  86% 72/84 [00:44\u003c00:08,  1.48it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  87% 73/84 [00:44\u003c00:07,  1.51it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  88% 74/84 [00:45\u003c00:06,  1.52it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  89% 75/84 [00:46\u003c00:06,  1.49it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  90% 76/84 [00:47\u003c00:05,  1.39it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  92% 77/84 [00:48\u003c00:05,  1.18it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  93% 78/84 [00:49\u003c00:05,  1.06it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  94% 79/84 [00:50\u003c00:05,  1.00s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  95% 80/84 [00:51\u003c00:03,  1.00it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  96% 81/84 [00:52\u003c00:03,  1.01s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  98% 82/84 [00:53\u003c00:01,  1.07it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  99% 83/84 [00:54\u003c00:00,  1.08it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset: 100% 84/84 [00:55\u003c00:00,  1.13it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 11:29:05 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 4.974 | nll_loss 3.658 | ppl 12.62 | bleu 15.4 | wps 3720.8 | wpb 2417.9 | bsz 75.4 | num_updates 20644 | best_bleu 15.53\n","2024-10-24 11:29:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 20644 updates\n","2024-10-24 11:29:05 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_last.pt\n","2024-10-24 11:29:05 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/bpeDROP/checkpoints-bpeDROP/checkpoint_last.pt\n","2024-10-24 11:29:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-bpeDROP/checkpoint_last.pt (epoch 4 @ 20644 updates, score 15.4) (writing took 0.3879343950011389 seconds)\n","2024-10-24 11:29:05 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n","2024-10-24 11:29:05 | INFO | train | epoch 004 | loss 5.382 | nll_loss 4.226 | ppl 18.71 | wps 28582.8 | ups 7.4 | wpb 3860.3 | bsz 101.7 | num_updates 20644 | lr 6.60274e-05 | gnorm 1.089 | train_wall 422 | gb_free 14.1 | wall 2810\n","2024-10-24 11:29:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 11:29:05 | INFO | fairseq_cli.train | done training in 2801.8 seconds\n"]}],"source":["!fairseq-train data-bin-25 \\\n","--arch transformer \\\n","--activation-fn relu \\\n","--share-decoder-input-output-embed \\\n","--share-all-embeddings \\\n","--encoder-layers 3 \\\n","--encoder-attention-heads 4 \\\n","--encoder-embed-dim 256 \\\n","--encoder-ffn-embed-dim 1024 \\\n","--decoder-layers 3 \\\n","--decoder-attention-heads 4 \\\n","--decoder-embed-dim 256 \\\n","--decoder-ffn-embed-dim 1024 \\\n","--dropout 0.25 \\\n","--seed 2024 \\\n","--optimizer 'adam' \\\n","--adam-betas '(0.9, 0.999)' \\\n","--lr-scheduler 'inverse_sqrt' \\\n","--patience 5 \\\n","--warmup-updates 1000 \\\n","--criterion 'label_smoothed_cross_entropy' \\\n","--label-smoothing 0.1 \\\n","--lr 0.0003 \\\n","--weight-decay 0.0 \\\n","--max-tokens 4096 \\\n","--max-tokens-valid 3600 \\\n","--required-batch-size-multiple 1 \\\n","--best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n","--max-epoch 4 \\\n","--validate-interval 1 \\\n","--save-interval 1 \\\n","--validate-interval-updates 2000 \\\n","--save-interval-updates 2000 \\\n","--log-interval 100 \\\n","--curriculum 0 \\\n","--no-epoch-checkpoints \\\n","--eval-bleu \\\n","--eval-bleu-args '{\"beam\": 5, \"max_len_a\": 0, \"max_len_b\": 100, \"len_pen\": 1}' \\\n","--eval-bleu-detok space \\\n","--eval-bleu-remove-bpe \\\n","--save-dir checkpoints-bpeDROP \\\n","--ddp-backend=no_c10d \\\n","--wandb-project 'fairseq-standard-subword-tok-eng-to-nso'"]},{"cell_type":"markdown","metadata":{"id":"Yg7n7DCPlm5r"},"source":["## Training NMT with ULM with Subword Regularization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mn3y1c8ZmKrW"},"outputs":[],"source":["# change working directory\n","os.chdir(f'/content/drive/MyDrive/Research/eng-to-{target_code}/ulmSR')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"p-WzSsQ9mQ79","outputId":"eef7276e-8583-4d8e-e79a-07517133ea61"},"outputs":[{"name":"stdout","output_type":"stream","text":["2024-10-24 09:33:28.418871: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-10-24 09:33:28.444439: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-10-24 09:33:28.454752: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-10-24 09:33:28.473018: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-10-24 09:33:29.608792: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Exception ignored in: \u003cfunction _xla_gc_callback at 0x79a3d6583130\u003e\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 96, in _xla_gc_callback\n","    def _xla_gc_callback(*args):\n","KeyboardInterrupt: \n","2024-10-24 09:33:31 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n","2024-10-24 09:33:38 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': 'fairseq-standard-subword-tok-eng-to-nso', 'azureml_logging': False, 'seed': 2024, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 2000, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3600, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 4, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0003], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints-ulmSR', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 5, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project='fairseq-standard-subword-tok-eng-to-nso', azureml_logging=False, seed=2024, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, batch_size=None, required_batch_size_multiple=1, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=2000, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid='3600', batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=4, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0003], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='checkpoints-ulmSR', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='bleu', maximize_best_checkpoint_metric=True, patience=5, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='data-bin-25', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 0, \"max_len_b\": 100, \"len_pen\": 1}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe='sentencepiece', eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.999)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=1000, warmup_init_lr=-1, pad=1, eos=2, unk=3, activation_fn='relu', share_decoder_input_output_embed=True, share_all_embeddings=True, encoder_layers=3, encoder_attention_heads=4, encoder_embed_dim=256, encoder_ffn_embed_dim=1024, decoder_layers=3, decoder_attention_heads=4, decoder_embed_dim=256, decoder_ffn_embed_dim=1024, dropout=0.25, no_seed_provided=False, encoder_embed_path=None, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=256, decoder_input_dim=256, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': 'data-bin-25', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 0, \"max_len_b\": 100, \"len_pen\": 1}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': 'sentencepiece', 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0003]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 1000, 'warmup_init_lr': -1.0, 'lr': [0.0003]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n","2024-10-24 09:33:39 | INFO | fairseq.tasks.translation | [eng] dictionary: 3992 types\n","2024-10-24 09:33:39 | INFO | fairseq.tasks.translation | [nso] dictionary: 3992 types\n","2024-10-24 09:33:39 | INFO | fairseq_cli.train | TransformerModel(\n","  (encoder): TransformerEncoderBase(\n","    (dropout_module): FairseqDropout()\n","    (embed_tokens): Embedding(3992, 256, padding_idx=1)\n","    (embed_positions): SinusoidalPositionalEmbedding()\n","    (layers): ModuleList(\n","      (0-2): 3 x TransformerEncoderLayerBase(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (decoder): TransformerDecoderBase(\n","    (dropout_module): FairseqDropout()\n","    (embed_tokens): Embedding(3992, 256, padding_idx=1)\n","    (embed_positions): SinusoidalPositionalEmbedding()\n","    (layers): ModuleList(\n","      (0-2): 3 x TransformerDecoderLayerBase(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (output_projection): Linear(in_features=256, out_features=3992, bias=False)\n","  )\n",")\n","2024-10-24 09:33:39 | INFO | fairseq_cli.train | task: TranslationTask\n","2024-10-24 09:33:39 | INFO | fairseq_cli.train | model: TransformerModel\n","2024-10-24 09:33:39 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n","2024-10-24 09:33:39 | INFO | fairseq_cli.train | num. shared model params: 6,551,552 (num. trained: 6,551,552)\n","2024-10-24 09:33:39 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n","2024-10-24 09:33:40 | INFO | fairseq.data.data_utils | loaded 6,336 examples from: data-bin-25/valid.eng-nso.eng\n","2024-10-24 09:33:40 | INFO | fairseq.data.data_utils | loaded 6,336 examples from: data-bin-25/valid.eng-nso.nso\n","2024-10-24 09:33:40 | INFO | fairseq.tasks.translation | data-bin-25 valid eng-nso 6336 examples\n","2024-10-24 09:33:41 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight \u003c- decoder.embed_tokens.weight\n","2024-10-24 09:33:41 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight \u003c- decoder.output_projection.weight\n","2024-10-24 09:33:41 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2024-10-24 09:33:41 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n","2024-10-24 09:33:41 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2024-10-24 09:33:41 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n","2024-10-24 09:33:41 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n","2024-10-24 09:33:41 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints-ulmSR/checkpoint_last.pt\n","2024-10-24 09:33:41 | INFO | fairseq.trainer | No existing checkpoint found checkpoints-ulmSR/checkpoint_last.pt\n","2024-10-24 09:33:41 | INFO | fairseq.trainer | loading train data for epoch 1\n","2024-10-24 09:33:44 | INFO | fairseq.data.data_utils | loaded 524,850 examples from: data-bin-25/train.eng-nso.eng\n","2024-10-24 09:33:47 | INFO | fairseq.data.data_utils | loaded 524,850 examples from: data-bin-25/train.eng-nso.nso\n","2024-10-24 09:33:47 | INFO | fairseq.tasks.translation | data-bin-25 train eng-nso 524850 examples\n","2024-10-24 09:33:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:33:47 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2024-10-24 09:33:47 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2024-10-24 09:33:47 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2024-10-24 09:33:50 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n","2024-10-24 09:33:50 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n","2024-10-24 09:33:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:33:50 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2024-10-24 09:33:50 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2024-10-24 09:33:50 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2024-10-24 09:33:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 6758\n","epoch 001:   0% 0/6758 [00:00\u003c?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtyobeka-mandisa\u001b[0m (\u001b[33mtyobeka-mandisa-university-of-cape-town\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.5\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/Research/eng-to-nso/ulmSR/wandb/run-20241024_093352-b42jhm0p\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcheckpoints-ulmSR\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 猸锔 View project at \u001b[34m\u001b[4mhttps://wandb.ai/tyobeka-mandisa-university-of-cape-town/fairseq-standard-subword-tok-eng-to-nso\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/tyobeka-mandisa-university-of-cape-town/fairseq-standard-subword-tok-eng-to-nso/runs/b42jhm0p\u001b[0m\n","2024-10-24 09:33:53 | INFO | fairseq.trainer | begin training epoch 1\n","2024-10-24 09:33:53 | INFO | fairseq_cli.train | Start iterating over samples\n","/content/drive/MyDrive/Research/eng-to-nso/fairseq/fairseq/tasks/fairseq_task.py:531: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","/content/drive/MyDrive/Research/eng-to-nso/fairseq/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n","  warnings.warn(\n","epoch 001:  30% 1999/6758 [02:50\u003c06:43, 11.80it/s, loss=6.848, nll_loss=5.959, ppl=62.21, wps=47083.5, ups=12.26, wpb=3841.6, bsz=71.8, num_updates=1900, lr=0.000217643, gnorm=0.698, train_wall=8, gb_free=14, wall=173]2024-10-24 09:36:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 09:36:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 001 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   1% 1/86 [00:01\u003c01:29,  1.05s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   2% 2/86 [00:02\u003c01:30,  1.08s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   3% 3/86 [00:03\u003c01:31,  1.10s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   5% 4/86 [00:04\u003c01:28,  1.08s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   6% 5/86 [00:05\u003c01:22,  1.02s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   7% 6/86 [00:06\u003c01:20,  1.00s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   8% 7/86 [00:07\u003c01:18,  1.01it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   9% 8/86 [00:08\u003c01:16,  1.02it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  10% 9/86 [00:09\u003c01:15,  1.02it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  12% 10/86 [00:10\u003c01:20,  1.05s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  13% 11/86 [00:11\u003c01:29,  1.19s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  14% 12/86 [00:13\u003c01:34,  1.27s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  15% 13/86 [00:14\u003c01:34,  1.30s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  16% 14/86 [00:16\u003c01:40,  1.40s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  17% 15/86 [00:17\u003c01:28,  1.24s/it]\u001b[A\n","epoch 001:  30% 1999/6758 [03:10\u003c06:43, 11.80it/s, loss=6.824, nll_loss=5.933, ppl=61.09, wps=43646.7, ups=11.31, wpb=3857.7, bsz=74.9, num_updates=2000, lr=0.000212132, gnorm=0.714, train_wall=8, gb_free=14.1, wall=182]\n","epoch 001 | valid on 'valid' subset:  20% 17/86 [00:19\u003c01:22,  1.20s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  21% 18/86 [00:20\u003c01:23,  1.23s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  22% 19/86 [00:21\u003c01:16,  1.15s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  23% 20/86 [00:23\u003c01:20,  1.22s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  24% 21/86 [00:24\u003c01:14,  1.15s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  26% 22/86 [00:25\u003c01:19,  1.24s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  27% 23/86 [00:26\u003c01:19,  1.27s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  28% 24/86 [00:28\u003c01:18,  1.27s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  29% 25/86 [00:29\u003c01:26,  1.42s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  30% 26/86 [00:31\u003c01:22,  1.38s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  31% 27/86 [00:32\u003c01:24,  1.43s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  33% 28/86 [00:33\u003c01:17,  1.34s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  34% 29/86 [00:34\u003c01:10,  1.24s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  35% 30/86 [00:36\u003c01:10,  1.25s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  36% 31/86 [00:37\u003c01:03,  1.15s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  37% 32/86 [00:38\u003c00:58,  1.08s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  38% 33/86 [00:39\u003c00:59,  1.13s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  40% 34/86 [00:40\u003c00:55,  1.06s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  41% 35/86 [00:41\u003c00:58,  1.14s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  42% 36/86 [00:42\u003c00:59,  1.19s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  43% 37/86 [00:44\u003c01:05,  1.34s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  44% 38/86 [00:46\u003c01:07,  1.41s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  45% 39/86 [00:47\u003c01:04,  1.38s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  47% 40/86 [00:48\u003c01:03,  1.38s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  48% 41/86 [00:49\u003c00:55,  1.23s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  49% 42/86 [00:50\u003c00:53,  1.22s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  50% 43/86 [00:51\u003c00:49,  1.15s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  51% 44/86 [00:53\u003c00:48,  1.16s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  52% 45/86 [00:54\u003c00:46,  1.13s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  53% 46/86 [00:55\u003c00:45,  1.14s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  55% 47/86 [00:56\u003c00:42,  1.09s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  56% 48/86 [00:57\u003c00:42,  1.11s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  57% 49/86 [00:58\u003c00:38,  1.05s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  58% 50/86 [00:59\u003c00:41,  1.16s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  59% 51/86 [01:01\u003c00:42,  1.21s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  60% 52/86 [01:02\u003c00:45,  1.34s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  62% 53/86 [01:04\u003c00:45,  1.36s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  63% 54/86 [01:05\u003c00:43,  1.35s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  64% 55/86 [01:06\u003c00:37,  1.22s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  65% 56/86 [01:07\u003c00:35,  1.19s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  66% 57/86 [01:08\u003c00:31,  1.09s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  67% 58/86 [01:09\u003c00:30,  1.09s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  69% 59/86 [01:10\u003c00:28,  1.05s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  70% 60/86 [01:11\u003c00:27,  1.06s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  71% 61/86 [01:12\u003c00:24,  1.00it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  72% 62/86 [01:13\u003c00:23,  1.00it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  73% 63/86 [01:14\u003c00:22,  1.01it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  74% 64/86 [01:15\u003c00:23,  1.06s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  76% 65/86 [01:16\u003c00:23,  1.12s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  77% 66/86 [01:17\u003c00:22,  1.15s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  78% 67/86 [01:19\u003c00:22,  1.18s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  79% 68/86 [01:20\u003c00:21,  1.21s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  80% 69/86 [01:21\u003c00:19,  1.16s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  81% 70/86 [01:22\u003c00:17,  1.11s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  83% 71/86 [01:23\u003c00:15,  1.05s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  84% 72/86 [01:24\u003c00:14,  1.02s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  85% 73/86 [01:25\u003c00:12,  1.01it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  86% 74/86 [01:26\u003c00:11,  1.03it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  87% 75/86 [01:27\u003c00:10,  1.07it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  88% 76/86 [01:27\u003c00:09,  1.09it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  90% 77/86 [01:28\u003c00:08,  1.10it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  91% 78/86 [01:29\u003c00:07,  1.08it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  92% 79/86 [01:30\u003c00:06,  1.08it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  93% 80/86 [01:31\u003c00:06,  1.00s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  94% 81/86 [01:33\u003c00:05,  1.06s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  95% 82/86 [01:34\u003c00:04,  1.09s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  97% 83/86 [01:35\u003c00:03,  1.12s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  98% 84/86 [01:36\u003c00:02,  1.16s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  99% 85/86 [01:37\u003c00:01,  1.14s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset: 100% 86/86 [01:38\u003c00:00,  1.03s/it]\u001b[A\n","                                                                        \u001b[A2024-10-24 09:38:21 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 7.591 | nll_loss 6.755 | ppl 107.99 | bleu 0.22 | wps 2093.7 | wpb 2386.9 | bsz 73.7 | num_updates 2000\n","2024-10-24 09:38:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 2000 updates\n","2024-10-24 09:38:21 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_1_2000.pt\n","2024-10-24 09:38:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_1_2000.pt\n","2024-10-24 09:38:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulmSR/checkpoint_1_2000.pt (epoch 1 @ 2000 updates, score 0.22) (writing took 1.3074458910004978 seconds)\n","epoch 001:  59% 3998/6758 [07:23\u003c03:46, 12.17it/s, loss=6.399, nll_loss=5.439, ppl=43.39, wps=48122.4, ups=12.34, wpb=3898.9, bsz=73.8, num_updates=3900, lr=0.000151911, gnorm=0.696, train_wall=8, gb_free=14.1, wall=446]2024-10-24 09:41:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 09:41:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 001 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   1% 1/86 [00:01\u003c01:49,  1.29s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   2% 2/86 [00:02\u003c01:45,  1.26s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   3% 3/86 [00:03\u003c01:43,  1.25s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   5% 4/86 [00:04\u003c01:34,  1.15s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   6% 5/86 [00:05\u003c01:27,  1.08s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   7% 6/86 [00:06\u003c01:27,  1.09s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   8% 7/86 [00:07\u003c01:23,  1.06s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   9% 8/86 [00:08\u003c01:23,  1.07s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  10% 9/86 [00:10\u003c01:26,  1.13s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  12% 10/86 [00:11\u003c01:30,  1.19s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  13% 11/86 [00:13\u003c01:40,  1.34s/it]\u001b[A\n","epoch 001:  59% 3998/6758 [07:40\u003c03:46, 12.17it/s, loss=6.396, nll_loss=5.436, ppl=43.3, wps=42721.6, ups=11.15, wpb=3829.9, bsz=78, num_updates=4000, lr=0.00015, gnorm=0.735, train_wall=8, gb_free=14.1, wall=455]       \n","epoch 001 | valid on 'valid' subset:  15% 13/86 [00:16\u003c01:44,  1.43s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  16% 14/86 [00:17\u003c01:50,  1.53s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  17% 15/86 [00:19\u003c01:42,  1.44s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  19% 16/86 [00:20\u003c01:45,  1.50s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  20% 17/86 [00:21\u003c01:33,  1.35s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  21% 18/86 [00:23\u003c01:29,  1.31s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  22% 19/86 [00:24\u003c01:21,  1.21s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  23% 20/86 [00:25\u003c01:22,  1.25s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  24% 21/86 [00:26\u003c01:20,  1.24s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  26% 22/86 [00:29\u003c01:41,  1.58s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  27% 23/86 [00:30\u003c01:38,  1.57s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  28% 24/86 [00:31\u003c01:33,  1.50s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  29% 25/86 [00:33\u003c01:28,  1.45s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  30% 26/86 [00:34\u003c01:16,  1.28s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  31% 27/86 [00:35\u003c01:16,  1.30s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  33% 28/86 [00:36\u003c01:10,  1.22s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  34% 29/86 [00:37\u003c01:05,  1.14s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  35% 30/86 [00:38\u003c01:05,  1.18s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  36% 31/86 [00:39\u003c01:00,  1.10s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  37% 32/86 [00:40\u003c00:57,  1.06s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  38% 33/86 [00:41\u003c01:00,  1.15s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  40% 34/86 [00:43\u003c01:00,  1.16s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  41% 35/86 [00:44\u003c01:06,  1.30s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  42% 36/86 [00:46\u003c01:08,  1.36s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  43% 37/86 [00:48\u003c01:12,  1.49s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  44% 38/86 [00:49\u003c01:04,  1.35s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  45% 39/86 [00:50\u003c00:57,  1.23s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  47% 40/86 [00:51\u003c00:55,  1.21s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  48% 41/86 [00:52\u003c00:50,  1.12s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  49% 42/86 [00:53\u003c00:50,  1.15s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  50% 43/86 [00:54\u003c00:46,  1.09s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  51% 44/86 [00:55\u003c00:47,  1.12s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  52% 45/86 [00:56\u003c00:46,  1.13s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  53% 46/86 [00:57\u003c00:45,  1.15s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  55% 47/86 [00:58\u003c00:45,  1.16s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  56% 48/86 [01:00\u003c00:47,  1.26s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  57% 49/86 [01:01\u003c00:49,  1.33s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  58% 50/86 [01:03\u003c00:50,  1.41s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  59% 51/86 [01:04\u003c00:48,  1.37s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  60% 52/86 [01:06\u003c00:45,  1.32s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  62% 53/86 [01:06\u003c00:39,  1.21s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  63% 54/86 [01:08\u003c00:38,  1.19s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  64% 55/86 [01:09\u003c00:35,  1.13s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  65% 56/86 [01:10\u003c00:33,  1.13s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  66% 57/86 [01:11\u003c00:30,  1.06s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  67% 58/86 [01:12\u003c00:30,  1.09s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  69% 59/86 [01:13\u003c00:28,  1.06s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  70% 60/86 [01:14\u003c00:28,  1.10s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  71% 61/86 [01:15\u003c00:28,  1.12s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  72% 62/86 [01:17\u003c00:28,  1.21s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  73% 63/86 [01:18\u003c00:28,  1.26s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  74% 64/86 [01:19\u003c00:29,  1.32s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  76% 65/86 [01:21\u003c00:27,  1.29s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  77% 66/86 [01:22\u003c00:23,  1.19s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  78% 67/86 [01:23\u003c00:21,  1.11s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  79% 68/86 [01:23\u003c00:18,  1.04s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  80% 69/86 [01:24\u003c00:16,  1.00it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  81% 70/86 [01:25\u003c00:15,  1.01it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  83% 71/86 [01:26\u003c00:14,  1.03it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  84% 72/86 [01:27\u003c00:13,  1.05it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  85% 73/86 [01:28\u003c00:12,  1.05it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  86% 74/86 [01:29\u003c00:11,  1.07it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  87% 75/86 [01:30\u003c00:10,  1.08it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  88% 76/86 [01:31\u003c00:09,  1.03it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  90% 77/86 [01:32\u003c00:09,  1.09s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  91% 78/86 [01:34\u003c00:09,  1.17s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  92% 79/86 [01:35\u003c00:08,  1.22s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  93% 80/86 [01:36\u003c00:07,  1.26s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  94% 81/86 [01:37\u003c00:05,  1.16s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  95% 82/86 [01:38\u003c00:04,  1.08s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  97% 83/86 [01:39\u003c00:03,  1.01s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  98% 84/86 [01:40\u003c00:01,  1.02it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  99% 85/86 [01:41\u003c00:00,  1.03it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset: 100% 86/86 [01:42\u003c00:00,  1.10it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 09:42:58 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 7.011 | nll_loss 6.088 | ppl 68.05 | bleu 1.12 | wps 2024.8 | wpb 2386.9 | bsz 73.7 | num_updates 4000 | best_bleu 1.12\n","2024-10-24 09:42:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 4000 updates\n","2024-10-24 09:42:58 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_1_4000.pt\n","2024-10-24 09:42:58 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_1_4000.pt\n","2024-10-24 09:42:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulmSR/checkpoint_1_4000.pt (epoch 1 @ 4000 updates, score 1.12) (writing took 1.2925977069999135 seconds)\n","epoch 001:  89% 5998/6758 [12:01\u003c00:59, 12.86it/s, loss=6.149, nll_loss=5.15, ppl=35.5, wps=46467.5, ups=12.05, wpb=3857.3, bsz=79.6, num_updates=5900, lr=0.000123508, gnorm=0.767, train_wall=8, gb_free=14.1, wall=724]2024-10-24 09:45:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 09:45:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 001 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   1% 1/86 [00:01\u003c01:40,  1.18s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   2% 2/86 [00:02\u003c01:31,  1.09s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   3% 3/86 [00:03\u003c01:28,  1.07s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   5% 4/86 [00:04\u003c01:25,  1.05s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   6% 5/86 [00:05\u003c01:22,  1.02s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   7% 6/86 [00:06\u003c01:28,  1.11s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   8% 7/86 [00:07\u003c01:33,  1.19s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   9% 8/86 [00:09\u003c01:37,  1.25s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  10% 9/86 [00:10\u003c01:39,  1.29s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  12% 10/86 [00:11\u003c01:39,  1.31s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  13% 11/86 [00:13\u003c01:33,  1.24s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  14% 12/86 [00:13\u003c01:24,  1.14s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  15% 13/86 [00:14\u003c01:18,  1.07s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  16% 14/86 [00:16\u003c01:21,  1.13s/it]\u001b[A\n","epoch 001:  89% 5998/6758 [12:20\u003c00:59, 12.86it/s, loss=6.151, nll_loss=5.152, ppl=35.56, wps=43882.4, ups=11.48, wpb=3823.4, bsz=75.8, num_updates=6000, lr=0.000122474, gnorm=0.74, train_wall=8, gb_free=14.2, wall=733]\n","epoch 001 | valid on 'valid' subset:  19% 16/86 [00:18\u003c01:17,  1.11s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  20% 17/86 [00:19\u003c01:12,  1.05s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  21% 18/86 [00:20\u003c01:12,  1.07s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  22% 19/86 [00:21\u003c01:07,  1.01s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  23% 20/86 [00:22\u003c01:15,  1.14s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  24% 21/86 [00:23\u003c01:17,  1.19s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  26% 22/86 [00:25\u003c01:25,  1.33s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  27% 23/86 [00:27\u003c01:25,  1.36s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  28% 24/86 [00:28\u003c01:23,  1.34s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  29% 25/86 [00:29\u003c01:19,  1.30s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  30% 26/86 [00:30\u003c01:10,  1.18s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  31% 27/86 [00:31\u003c01:10,  1.19s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  33% 28/86 [00:32\u003c01:06,  1.14s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  34% 29/86 [00:33\u003c01:01,  1.08s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  35% 30/86 [00:34\u003c01:02,  1.11s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  36% 31/86 [00:35\u003c00:57,  1.05s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  37% 32/86 [00:36\u003c00:54,  1.01s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  38% 33/86 [00:37\u003c00:55,  1.05s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  40% 34/86 [00:38\u003c00:56,  1.10s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  41% 35/86 [00:40\u003c01:03,  1.24s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  42% 36/86 [00:41\u003c01:05,  1.30s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  43% 37/86 [00:43\u003c01:09,  1.42s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  44% 38/86 [00:44\u003c01:04,  1.35s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  45% 39/86 [00:45\u003c00:57,  1.23s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  47% 40/86 [00:46\u003c00:54,  1.19s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  48% 41/86 [00:47\u003c00:49,  1.10s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  49% 42/86 [00:48\u003c00:49,  1.12s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  50% 43/86 [00:49\u003c00:45,  1.07s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  51% 44/86 [00:51\u003c00:46,  1.10s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  52% 45/86 [00:52\u003c00:43,  1.06s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  53% 46/86 [00:53\u003c00:43,  1.09s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  55% 47/86 [00:54\u003c00:40,  1.04s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  56% 48/86 [00:55\u003c00:43,  1.15s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  57% 49/86 [00:56\u003c00:42,  1.16s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  58% 50/86 [00:58\u003c00:46,  1.30s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  59% 51/86 [00:59\u003c00:47,  1.34s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  60% 52/86 [01:01\u003c00:44,  1.31s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  62% 53/86 [01:02\u003c00:40,  1.22s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  63% 54/86 [01:03\u003c00:37,  1.19s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  64% 55/86 [01:04\u003c00:34,  1.12s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  65% 56/86 [01:05\u003c00:33,  1.11s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  66% 57/86 [01:06\u003c00:30,  1.04s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  67% 58/86 [01:07\u003c00:29,  1.05s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  69% 59/86 [01:08\u003c00:28,  1.05s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  70% 60/86 [01:09\u003c00:28,  1.10s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  71% 61/86 [01:10\u003c00:26,  1.07s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  72% 62/86 [01:11\u003c00:27,  1.16s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  73% 63/86 [01:13\u003c00:28,  1.25s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  74% 64/86 [01:14\u003c00:29,  1.35s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  76% 65/86 [01:16\u003c00:29,  1.43s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  77% 66/86 [01:17\u003c00:28,  1.41s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  78% 67/86 [01:19\u003c00:27,  1.47s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  79% 68/86 [01:20\u003c00:25,  1.43s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  80% 69/86 [01:22\u003c00:24,  1.41s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  81% 70/86 [01:23\u003c00:20,  1.31s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  83% 71/86 [01:24\u003c00:17,  1.19s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  84% 72/86 [01:25\u003c00:15,  1.11s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  85% 73/86 [01:26\u003c00:13,  1.06s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  86% 74/86 [01:26\u003c00:12,  1.03s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  87% 75/86 [01:27\u003c00:10,  1.01it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  88% 76/86 [01:28\u003c00:09,  1.04it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  90% 77/86 [01:29\u003c00:09,  1.04s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  91% 78/86 [01:31\u003c00:08,  1.11s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  92% 79/86 [01:32\u003c00:08,  1.18s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  93% 80/86 [01:33\u003c00:07,  1.22s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  94% 81/86 [01:35\u003c00:06,  1.23s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  95% 82/86 [01:36\u003c00:04,  1.17s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  97% 83/86 [01:37\u003c00:03,  1.08s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  98% 84/86 [01:37\u003c00:02,  1.02s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  99% 85/86 [01:38\u003c00:00,  1.03it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset: 100% 86/86 [01:39\u003c00:00,  1.09it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 09:47:33 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 6.653 | nll_loss 5.664 | ppl 50.69 | bleu 2.62 | wps 2075.7 | wpb 2386.9 | bsz 73.7 | num_updates 6000 | best_bleu 2.62\n","2024-10-24 09:47:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 6000 updates\n","2024-10-24 09:47:33 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_1_6000.pt\n","2024-10-24 09:47:33 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_1_6000.pt\n","2024-10-24 09:47:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulmSR/checkpoint_1_6000.pt (epoch 1 @ 6000 updates, score 2.62) (writing took 1.324653074001617 seconds)\n","epoch 001: 100% 6757/6758 [14:49\u003c00:00, 12.58it/s, loss=6.069, nll_loss=5.056, ppl=33.27, wps=43886.2, ups=11.28, wpb=3889.2, bsz=78.7, num_updates=6700, lr=0.0001159, gnorm=0.758, train_wall=8, gb_free=14, wall=896]2024-10-24 09:48:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 09:48:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 001 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   1% 1/86 [00:01\u003c01:47,  1.26s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   2% 2/86 [00:02\u003c01:53,  1.36s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   3% 3/86 [00:04\u003c01:57,  1.42s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   5% 4/86 [00:05\u003c01:56,  1.42s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   6% 5/86 [00:06\u003c01:52,  1.39s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   7% 6/86 [00:08\u003c01:48,  1.36s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:   8% 7/86 [00:09\u003c01:40,  1.27s/it]\u001b[A\n","epoch 001: 100% 6757/6758 [15:00\u003c00:00, 12.58it/s, loss=6.069, nll_loss=5.056, ppl=33.27, wps=43886.2, ups=11.28, wpb=3889.2, bsz=78.7, num_updates=6700, lr=0.0001159, gnorm=0.758, train_wall=8, gb_free=14, wall=896]\n","epoch 001 | valid on 'valid' subset:  10% 9/86 [00:11\u003c01:25,  1.10s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  12% 10/86 [00:12\u003c01:19,  1.05s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  13% 11/86 [00:13\u003c01:19,  1.06s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  14% 12/86 [00:14\u003c01:16,  1.03s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  15% 13/86 [00:15\u003c01:13,  1.01s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  16% 14/86 [00:16\u003c01:16,  1.06s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  17% 15/86 [00:17\u003c01:11,  1.01s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  19% 16/86 [00:18\u003c01:19,  1.14s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  20% 17/86 [00:20\u003c01:22,  1.19s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  21% 18/86 [00:21\u003c01:29,  1.31s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  22% 19/86 [00:23\u003c01:30,  1.35s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  23% 20/86 [00:24\u003c01:33,  1.41s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  24% 21/86 [00:25\u003c01:23,  1.29s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  26% 22/86 [00:27\u003c01:28,  1.38s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  27% 23/86 [00:28\u003c01:19,  1.26s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  28% 24/86 [00:29\u003c01:11,  1.15s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  29% 25/86 [00:30\u003c01:11,  1.18s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  30% 26/86 [00:31\u003c01:05,  1.09s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  31% 27/86 [00:32\u003c01:07,  1.14s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  33% 28/86 [00:33\u003c01:03,  1.10s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  34% 29/86 [00:34\u003c00:59,  1.04s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  35% 30/86 [00:35\u003c01:06,  1.18s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  36% 31/86 [00:37\u003c01:06,  1.21s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  37% 32/86 [00:38\u003c01:06,  1.23s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  38% 33/86 [00:40\u003c01:11,  1.35s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  40% 34/86 [00:41\u003c01:08,  1.32s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  41% 35/86 [00:42\u003c01:04,  1.27s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  42% 36/86 [00:43\u003c00:59,  1.18s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  43% 37/86 [00:44\u003c00:57,  1.17s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  44% 38/86 [00:45\u003c00:53,  1.12s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  45% 39/86 [00:46\u003c00:49,  1.06s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  47% 40/86 [00:47\u003c00:49,  1.07s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  48% 41/86 [00:48\u003c00:46,  1.03s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  49% 42/86 [00:49\u003c00:46,  1.05s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  50% 43/86 [00:50\u003c00:44,  1.03s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  51% 44/86 [00:52\u003c00:47,  1.14s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  52% 45/86 [00:53\u003c00:50,  1.22s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  53% 46/86 [00:55\u003c00:53,  1.33s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  55% 47/86 [00:56\u003c00:52,  1.35s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  56% 48/86 [00:57\u003c00:51,  1.36s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  57% 49/86 [00:58\u003c00:45,  1.22s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  58% 50/86 [00:59\u003c00:43,  1.21s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  59% 51/86 [01:00\u003c00:40,  1.15s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  60% 52/86 [01:01\u003c00:38,  1.13s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  62% 53/86 [01:02\u003c00:35,  1.08s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  63% 54/86 [01:03\u003c00:34,  1.08s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  64% 55/86 [01:04\u003c00:32,  1.04s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  65% 56/86 [01:06\u003c00:31,  1.05s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  66% 57/86 [01:06\u003c00:29,  1.00s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  67% 58/86 [01:08\u003c00:31,  1.12s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  69% 59/86 [01:09\u003c00:31,  1.18s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  70% 60/86 [01:11\u003c00:33,  1.30s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  71% 61/86 [01:12\u003c00:33,  1.33s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  72% 62/86 [01:13\u003c00:30,  1.29s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  73% 63/86 [01:14\u003c00:27,  1.18s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  74% 64/86 [01:15\u003c00:25,  1.14s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  76% 65/86 [01:16\u003c00:23,  1.11s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  77% 66/86 [01:17\u003c00:21,  1.08s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  78% 67/86 [01:18\u003c00:19,  1.03s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  79% 68/86 [01:19\u003c00:17,  1.02it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  80% 69/86 [01:20\u003c00:16,  1.02it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  81% 70/86 [01:21\u003c00:15,  1.02it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  83% 71/86 [01:22\u003c00:14,  1.05it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  84% 72/86 [01:23\u003c00:13,  1.02it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  85% 73/86 [01:24\u003c00:13,  1.06s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  86% 74/86 [01:25\u003c00:13,  1.12s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  87% 75/86 [01:27\u003c00:13,  1.20s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  88% 76/86 [01:28\u003c00:12,  1.24s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  90% 77/86 [01:29\u003c00:11,  1.23s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  91% 78/86 [01:30\u003c00:09,  1.15s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  92% 79/86 [01:31\u003c00:07,  1.07s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  93% 80/86 [01:32\u003c00:06,  1.03s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  94% 81/86 [01:33\u003c00:05,  1.03s/it]\u001b[A\n","epoch 001 | valid on 'valid' subset:  95% 82/86 [01:34\u003c00:03,  1.01it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  97% 83/86 [01:35\u003c00:02,  1.05it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  98% 84/86 [01:36\u003c00:01,  1.06it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  99% 85/86 [01:37\u003c00:00,  1.10it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset: 100% 86/86 [01:38\u003c00:00,  1.14it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 09:50:19 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 6.556 | nll_loss 5.554 | ppl 46.97 | bleu 3.36 | wps 2110 | wpb 2386.9 | bsz 73.7 | num_updates 6758 | best_bleu 3.36\n","2024-10-24 09:50:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 6758 updates\n","2024-10-24 09:50:19 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_best.pt\n","2024-10-24 09:50:19 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_best.pt\n","2024-10-24 09:50:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulmSR/checkpoint_best.pt (epoch 1 @ 6758 updates, score 3.36) (writing took 0.8316304970012425 seconds)\n","2024-10-24 09:50:20 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n","2024-10-24 09:50:20 | INFO | train | epoch 001 | loss 6.752 | nll_loss 5.853 | ppl 57.81 | wps 26437.9 | ups 6.86 | wpb 3856 | bsz 77.7 | num_updates 6758 | lr 0.000115402 | gnorm 0.772 | train_wall 537 | gb_free 14 | wall 999\n","2024-10-24 09:50:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 09:50:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 6758\n","epoch 002:   0% 0/6758 [00:00\u003c?, ?it/s]2024-10-24 09:50:20 | INFO | fairseq.trainer | begin training epoch 2\n","2024-10-24 09:50:20 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 002:  18% 1241/6758 [01:50\u003c07:29, 12.26it/s, loss=5.961, nll_loss=4.932, ppl=30.53, wps=45507.6, ups=11.75, wpb=3872.8, bsz=81.2, num_updates=7900, lr=0.000106735, gnorm=0.765, train_wall=8, gb_free=14.1, wall=1101]2024-10-24 09:52:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 09:52:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 002 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   1% 1/86 [00:01\u003c01:44,  1.23s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:   2% 2/86 [00:02\u003c01:35,  1.13s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:   3% 3/86 [00:03\u003c01:31,  1.11s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:   5% 4/86 [00:04\u003c01:40,  1.22s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:   6% 5/86 [00:06\u003c01:40,  1.24s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:   7% 6/86 [00:07\u003c01:45,  1.32s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:   8% 7/86 [00:08\u003c01:47,  1.37s/it]\u001b[A\n","epoch 002:  18% 1241/6758 [02:01\u003c07:29, 12.26it/s, loss=6.008, nll_loss=4.985, ppl=31.66, wps=44655.8, ups=11.82, wpb=3777.2, bsz=71.9, num_updates=8000, lr=0.000106066, gnorm=0.788, train_wall=8, gb_free=14.1, wall=1110]\n","epoch 002 | valid on 'valid' subset:  10% 9/86 [00:11\u003c01:31,  1.19s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  12% 10/86 [00:12\u003c01:25,  1.12s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  13% 11/86 [00:13\u003c01:22,  1.10s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  14% 12/86 [00:14\u003c01:19,  1.07s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  15% 13/86 [00:15\u003c01:14,  1.03s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  16% 14/86 [00:16\u003c01:17,  1.08s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  17% 15/86 [00:17\u003c01:12,  1.02s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  19% 16/86 [00:18\u003c01:13,  1.05s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  20% 17/86 [00:19\u003c01:09,  1.01s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  21% 18/86 [00:20\u003c01:13,  1.08s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  22% 19/86 [00:21\u003c01:16,  1.14s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  23% 20/86 [00:23\u003c01:25,  1.29s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  24% 21/86 [00:24\u003c01:27,  1.35s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  26% 22/86 [00:26\u003c01:30,  1.41s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  27% 23/86 [00:27\u003c01:20,  1.28s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  28% 24/86 [00:28\u003c01:11,  1.16s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  29% 25/86 [00:29\u003c01:10,  1.16s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  30% 26/86 [00:30\u003c01:04,  1.07s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  31% 27/86 [00:31\u003c01:05,  1.11s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  33% 28/86 [00:32\u003c01:02,  1.08s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  34% 29/86 [00:33\u003c00:58,  1.02s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  35% 30/86 [00:34\u003c01:00,  1.07s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  36% 31/86 [00:35\u003c00:57,  1.04s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  37% 32/86 [00:36\u003c00:56,  1.05s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  38% 33/86 [00:38\u003c01:03,  1.19s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  40% 34/86 [00:39\u003c01:02,  1.20s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  41% 35/86 [00:41\u003c01:10,  1.38s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  42% 36/86 [00:42\u003c01:10,  1.41s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  43% 37/86 [00:44\u003c01:15,  1.55s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  44% 38/86 [00:46\u003c01:16,  1.59s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  45% 39/86 [00:47\u003c01:07,  1.45s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  47% 40/86 [00:48\u003c01:02,  1.35s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  48% 41/86 [00:49\u003c00:54,  1.22s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  49% 42/86 [00:50\u003c00:51,  1.18s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  50% 43/86 [00:51\u003c00:47,  1.11s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  51% 44/86 [00:52\u003c00:46,  1.10s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  52% 45/86 [00:53\u003c00:43,  1.06s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  53% 46/86 [00:54\u003c00:43,  1.08s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  55% 47/86 [00:55\u003c00:40,  1.05s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  56% 48/86 [00:56\u003c00:43,  1.15s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  57% 49/86 [00:58\u003c00:43,  1.18s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  58% 50/86 [00:59\u003c00:46,  1.29s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  59% 51/86 [01:01\u003c00:46,  1.32s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  60% 52/86 [01:02\u003c00:45,  1.33s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  62% 53/86 [01:03\u003c00:39,  1.20s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  63% 54/86 [01:04\u003c00:37,  1.16s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  64% 55/86 [01:05\u003c00:33,  1.09s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  65% 56/86 [01:06\u003c00:32,  1.09s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  66% 57/86 [01:07\u003c00:30,  1.05s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  67% 58/86 [01:08\u003c00:29,  1.05s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  69% 59/86 [01:09\u003c00:27,  1.02s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  70% 60/86 [01:10\u003c00:26,  1.02s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  71% 61/86 [01:11\u003c00:24,  1.04it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  72% 62/86 [01:12\u003c00:23,  1.03it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  73% 63/86 [01:13\u003c00:23,  1.04s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  74% 64/86 [01:14\u003c00:24,  1.13s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  76% 65/86 [01:16\u003c00:25,  1.22s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  77% 66/86 [01:17\u003c00:25,  1.26s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  78% 67/86 [01:18\u003c00:24,  1.27s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  79% 68/86 [01:19\u003c00:20,  1.14s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  80% 69/86 [01:20\u003c00:18,  1.06s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  81% 70/86 [01:21\u003c00:16,  1.03s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  83% 71/86 [01:22\u003c00:14,  1.02it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  84% 72/86 [01:23\u003c00:13,  1.04it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  85% 73/86 [01:24\u003c00:12,  1.03it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  86% 74/86 [01:25\u003c00:11,  1.04it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  87% 75/86 [01:26\u003c00:10,  1.06it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  88% 76/86 [01:26\u003c00:09,  1.09it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  90% 77/86 [01:27\u003c00:08,  1.10it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  91% 78/86 [01:28\u003c00:07,  1.04it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  92% 79/86 [01:30\u003c00:07,  1.04s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  93% 80/86 [01:31\u003c00:06,  1.09s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  94% 81/86 [01:32\u003c00:05,  1.15s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  95% 82/86 [01:33\u003c00:04,  1.19s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  97% 83/86 [01:35\u003c00:03,  1.19s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  98% 84/86 [01:36\u003c00:02,  1.10s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  99% 85/86 [01:36\u003c00:01,  1.03s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset: 100% 86/86 [01:37\u003c00:00,  1.04it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 09:53:48 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 6.424 | nll_loss 5.403 | ppl 42.33 | bleu 4.13 | wps 2117 | wpb 2386.9 | bsz 73.7 | num_updates 8000 | best_bleu 4.13\n","2024-10-24 09:53:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 8000 updates\n","2024-10-24 09:53:48 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_2_8000.pt\n","2024-10-24 09:53:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_2_8000.pt\n","2024-10-24 09:53:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulmSR/checkpoint_2_8000.pt (epoch 2 @ 8000 updates, score 4.13) (writing took 1.6586997760005033 seconds)\n","epoch 002:  48% 3241/6758 [06:24\u003c04:49, 12.16it/s, loss=5.851, nll_loss=4.805, ppl=27.95, wps=47783.7, ups=12.48, wpb=3829.5, bsz=80.1, num_updates=9900, lr=9.53463e-05, gnorm=0.825, train_wall=8, gb_free=14.1, wall=1375]2024-10-24 09:56:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 09:56:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 002 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   1% 1/86 [00:00\u003c00:56,  1.50it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   2% 2/86 [00:01\u003c00:48,  1.73it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   3% 3/86 [00:02\u003c01:03,  1.30it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   5% 4/86 [00:03\u003c01:06,  1.23it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   6% 5/86 [00:03\u003c01:08,  1.18it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   7% 6/86 [00:04\u003c01:09,  1.15it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   8% 7/86 [00:05\u003c01:08,  1.16it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   9% 8/86 [00:06\u003c01:07,  1.16it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  10% 9/86 [00:07\u003c01:08,  1.12it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  12% 10/86 [00:08\u003c01:14,  1.03it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  13% 11/86 [00:10\u003c01:21,  1.09s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  14% 12/86 [00:11\u003c01:30,  1.22s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  15% 13/86 [00:12\u003c01:31,  1.25s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  16% 14/86 [00:14\u003c01:31,  1.27s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  17% 15/86 [00:15\u003c01:21,  1.14s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  19% 16/86 [00:16\u003c01:20,  1.14s/it]\u001b[A\n","epoch 002:  48% 3241/6758 [06:41\u003c04:49, 12.16it/s, loss=5.883, nll_loss=4.839, ppl=28.62, wps=42266.1, ups=11, wpb=3842.7, bsz=69.5, num_updates=10000, lr=9.48683e-05, gnorm=0.805, train_wall=8, gb_free=14.1, wall=1384]  \n","epoch 002 | valid on 'valid' subset:  21% 18/86 [00:18\u003c01:13,  1.08s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  22% 19/86 [00:19\u003c01:07,  1.01s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  23% 20/86 [00:20\u003c01:08,  1.03s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  24% 21/86 [00:21\u003c01:05,  1.01s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  26% 22/86 [00:22\u003c01:05,  1.02s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  27% 23/86 [00:23\u003c01:04,  1.02s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  28% 24/86 [00:24\u003c01:04,  1.05s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  29% 25/86 [00:26\u003c01:19,  1.31s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  30% 26/86 [00:27\u003c01:19,  1.32s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  31% 27/86 [00:29\u003c01:23,  1.42s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  33% 28/86 [00:30\u003c01:17,  1.33s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  34% 29/86 [00:31\u003c01:08,  1.20s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  35% 30/86 [00:32\u003c01:04,  1.16s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  36% 31/86 [00:33\u003c00:59,  1.08s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  37% 32/86 [00:33\u003c00:54,  1.01s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  38% 33/86 [00:35\u003c00:54,  1.02s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  40% 34/86 [00:35\u003c00:50,  1.02it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  41% 35/86 [00:36\u003c00:51,  1.00s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  42% 36/86 [00:37\u003c00:49,  1.01it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  43% 37/86 [00:39\u003c00:50,  1.03s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  44% 38/86 [00:40\u003c00:49,  1.03s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  45% 39/86 [00:41\u003c00:51,  1.09s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  47% 40/86 [00:42\u003c00:53,  1.16s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  48% 41/86 [00:43\u003c00:53,  1.20s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  49% 42/86 [00:45\u003c00:57,  1.30s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  50% 43/86 [00:46\u003c00:53,  1.25s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  51% 44/86 [00:47\u003c00:49,  1.19s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  52% 45/86 [00:48\u003c00:46,  1.12s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  53% 46/86 [00:49\u003c00:44,  1.11s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  55% 47/86 [00:50\u003c00:40,  1.05s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  56% 48/86 [00:51\u003c00:39,  1.04s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  57% 49/86 [00:52\u003c00:37,  1.02s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  58% 50/86 [00:53\u003c00:37,  1.03s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  59% 51/86 [00:54\u003c00:35,  1.00s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  60% 52/86 [00:55\u003c00:34,  1.01s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  62% 53/86 [00:56\u003c00:34,  1.03s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  63% 54/86 [00:58\u003c00:36,  1.15s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  64% 55/86 [00:59\u003c00:37,  1.21s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  65% 56/86 [01:00\u003c00:38,  1.30s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  66% 57/86 [01:02\u003c00:37,  1.28s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  67% 58/86 [01:03\u003c00:34,  1.22s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  69% 59/86 [01:04\u003c00:30,  1.14s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  70% 60/86 [01:05\u003c00:28,  1.10s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  71% 61/86 [01:06\u003c00:25,  1.04s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  72% 62/86 [01:07\u003c00:24,  1.03s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  73% 63/86 [01:08\u003c00:22,  1.00it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  74% 64/86 [01:09\u003c00:22,  1.01s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  76% 65/86 [01:10\u003c00:21,  1.00s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  77% 66/86 [01:11\u003c00:19,  1.01it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  78% 67/86 [01:12\u003c00:19,  1.00s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  79% 68/86 [01:13\u003c00:18,  1.03s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  80% 69/86 [01:14\u003c00:18,  1.09s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  81% 70/86 [01:15\u003c00:18,  1.18s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  83% 71/86 [01:17\u003c00:18,  1.23s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  84% 72/86 [01:18\u003c00:16,  1.21s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  85% 73/86 [01:19\u003c00:14,  1.13s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  86% 74/86 [01:20\u003c00:12,  1.06s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  87% 75/86 [01:21\u003c00:11,  1.00s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  88% 76/86 [01:21\u003c00:09,  1.04it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  90% 77/86 [01:22\u003c00:08,  1.07it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  91% 78/86 [01:23\u003c00:07,  1.06it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  92% 79/86 [01:24\u003c00:06,  1.08it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  93% 80/86 [01:25\u003c00:05,  1.07it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  94% 81/86 [01:26\u003c00:04,  1.10it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  95% 82/86 [01:27\u003c00:03,  1.12it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  97% 83/86 [01:28\u003c00:02,  1.08it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  98% 84/86 [01:29\u003c00:02,  1.01s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  99% 85/86 [01:30\u003c00:01,  1.04s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset: 100% 86/86 [01:31\u003c00:00,  1.09s/it]\u001b[A\n","                                                                        \u001b[A2024-10-24 09:58:16 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 6.233 | nll_loss 5.184 | ppl 36.35 | bleu 5.6 | wps 2240.9 | wpb 2386.9 | bsz 73.7 | num_updates 10000 | best_bleu 5.6\n","2024-10-24 09:58:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 10000 updates\n","2024-10-24 09:58:16 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_2_10000.pt\n","2024-10-24 09:58:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_2_10000.pt\n","2024-10-24 09:58:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulmSR/checkpoint_2_10000.pt (epoch 2 @ 10000 updates, score 5.6) (writing took 1.6242276129996753 seconds)\n","epoch 002:  78% 5240/6758 [10:53\u003c02:01, 12.51it/s, loss=5.754, nll_loss=4.691, ppl=25.83, wps=42836.4, ups=10.98, wpb=3901, bsz=73.5, num_updates=11900, lr=8.69657e-05, gnorm=0.803, train_wall=8, gb_free=14.1, wall=1645]2024-10-24 10:01:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 10:01:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 002 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   1% 1/86 [00:01\u003c02:04,  1.46s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:   2% 2/86 [00:02\u003c01:55,  1.37s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:   3% 3/86 [00:04\u003c02:00,  1.45s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:   5% 4/86 [00:05\u003c01:56,  1.42s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:   6% 5/86 [00:06\u003c01:45,  1.30s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:   7% 6/86 [00:07\u003c01:36,  1.21s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:   8% 7/86 [00:08\u003c01:30,  1.14s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:   9% 8/86 [00:09\u003c01:23,  1.07s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  10% 9/86 [00:10\u003c01:17,  1.01s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  12% 10/86 [00:11\u003c01:13,  1.03it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  13% 11/86 [00:12\u003c01:13,  1.02it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  14% 12/86 [00:13\u003c01:11,  1.04it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  15% 13/86 [00:14\u003c01:07,  1.07it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  16% 14/86 [00:15\u003c01:10,  1.02it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  17% 15/86 [00:16\u003c01:06,  1.06it/s]\u001b[A\n","epoch 002:  78% 5240/6758 [11:11\u003c02:01, 12.51it/s, loss=5.739, nll_loss=4.674, ppl=25.53, wps=48329.9, ups=12.46, wpb=3879.7, bsz=76.6, num_updates=12000, lr=8.66025e-05, gnorm=0.823, train_wall=8, gb_free=14.1, wall=1653]\n","epoch 002 | valid on 'valid' subset:  20% 17/86 [00:18\u003c01:18,  1.14s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  21% 18/86 [00:20\u003c01:26,  1.27s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  22% 19/86 [00:21\u003c01:23,  1.25s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  23% 20/86 [00:23\u003c01:24,  1.29s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  24% 21/86 [00:23\u003c01:16,  1.18s/it]\u001b[A2024-10-24 10:01:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:01:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:01:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 002 | valid on 'valid' subset:  26% 22/86 [00:25\u003c01:14,  1.16s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  27% 23/86 [00:26\u003c01:10,  1.12s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  28% 24/86 [00:27\u003c01:05,  1.05s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  29% 25/86 [00:28\u003c01:04,  1.05s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  30% 26/86 [00:29\u003c01:01,  1.02s/it]\u001b[A2024-10-24 10:01:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:01:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:01:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 002 | valid on 'valid' subset:  31% 27/86 [00:30\u003c01:01,  1.05s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  33% 28/86 [00:31\u003c00:59,  1.02s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  34% 29/86 [00:31\u003c00:56,  1.01it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  35% 30/86 [00:33\u003c01:00,  1.07s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  36% 31/86 [00:34\u003c01:00,  1.10s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  37% 32/86 [00:35\u003c01:01,  1.13s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  38% 33/86 [00:37\u003c01:06,  1.25s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  40% 34/86 [00:38\u003c01:05,  1.26s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  41% 35/86 [00:39\u003c01:05,  1.28s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  42% 36/86 [00:40\u003c00:59,  1.18s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  43% 37/86 [00:41\u003c00:57,  1.17s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  44% 38/86 [00:42\u003c00:53,  1.11s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  45% 39/86 [00:43\u003c00:49,  1.06s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  47% 40/86 [00:44\u003c00:48,  1.05s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  48% 41/86 [00:45\u003c00:45,  1.01s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  49% 42/86 [00:46\u003c00:45,  1.03s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  50% 43/86 [00:47\u003c00:42,  1.01it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  51% 44/86 [00:48\u003c00:42,  1.01s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  52% 45/86 [00:50\u003c00:45,  1.12s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  53% 46/86 [00:51\u003c00:47,  1.20s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  55% 47/86 [00:52\u003c00:48,  1.25s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  56% 48/86 [00:54\u003c00:50,  1.34s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  57% 49/86 [00:55\u003c00:48,  1.31s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  58% 50/86 [00:56\u003c00:44,  1.25s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  59% 51/86 [00:57\u003c00:40,  1.15s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  60% 52/86 [00:58\u003c00:37,  1.11s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  62% 53/86 [00:59\u003c00:34,  1.05s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  63% 54/86 [01:00\u003c00:33,  1.06s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  64% 55/86 [01:01\u003c00:31,  1.02s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  65% 56/86 [01:02\u003c00:30,  1.03s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  66% 57/86 [01:03\u003c00:28,  1.03it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  67% 58/86 [01:04\u003c00:27,  1.01it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  69% 59/86 [01:05\u003c00:26,  1.03it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  70% 60/86 [01:06\u003c00:27,  1.07s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  71% 61/86 [01:08\u003c00:28,  1.14s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  72% 62/86 [01:09\u003c00:29,  1.22s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  73% 63/86 [01:10\u003c00:29,  1.29s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  74% 64/86 [01:12\u003c00:27,  1.25s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  76% 65/86 [01:13\u003c00:24,  1.15s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  77% 66/86 [01:13\u003c00:21,  1.07s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  78% 67/86 [01:14\u003c00:19,  1.02s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  79% 68/86 [01:15\u003c00:17,  1.02it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  80% 69/86 [01:16\u003c00:16,  1.04it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  81% 70/86 [01:17\u003c00:15,  1.03it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  83% 71/86 [01:18\u003c00:14,  1.04it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  84% 72/86 [01:19\u003c00:13,  1.06it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  85% 73/86 [01:20\u003c00:12,  1.05it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  86% 74/86 [01:21\u003c00:11,  1.06it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  87% 75/86 [01:22\u003c00:11,  1.01s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  88% 76/86 [01:23\u003c00:10,  1.07s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  90% 77/86 [01:25\u003c00:10,  1.13s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  91% 78/86 [01:26\u003c00:09,  1.20s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  92% 79/86 [01:27\u003c00:08,  1.25s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  93% 80/86 [01:28\u003c00:07,  1.19s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  94% 81/86 [01:29\u003c00:05,  1.10s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  95% 82/86 [01:30\u003c00:04,  1.03s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  97% 83/86 [01:31\u003c00:02,  1.02it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  98% 84/86 [01:32\u003c00:01,  1.05it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  99% 85/86 [01:33\u003c00:00,  1.08it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset: 100% 86/86 [01:33\u003c00:00,  1.13it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 10:02:47 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 6.118 | nll_loss 5.043 | ppl 32.97 | bleu 6.85 | wps 2208 | wpb 2386.9 | bsz 73.7 | num_updates 12000 | best_bleu 6.85\n","2024-10-24 10:02:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 12000 updates\n","2024-10-24 10:02:47 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_2_12000.pt\n","2024-10-24 10:02:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_2_12000.pt\n","2024-10-24 10:02:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulmSR/checkpoint_2_12000.pt (epoch 2 @ 12000 updates, score 6.85) (writing took 1.600184758000978 seconds)\n","epoch 002: 100% 6757/6758 [14:43\u003c00:00, 11.39it/s, loss=5.669, nll_loss=4.593, ppl=24.13, wps=48873.8, ups=12.31, wpb=3969.2, bsz=77.7, num_updates=13500, lr=8.16497e-05, gnorm=0.82, train_wall=8, gb_free=14.1, wall=1881]2024-10-24 10:05:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 10:05:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 002 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   1% 1/86 [00:00\u003c01:08,  1.24it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   2% 2/86 [00:01\u003c01:07,  1.24it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   3% 3/86 [00:03\u003c01:38,  1.18s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:   5% 4/86 [00:04\u003c01:41,  1.24s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:   6% 5/86 [00:05\u003c01:36,  1.19s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:   7% 6/86 [00:06\u003c01:30,  1.13s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:   8% 7/86 [00:07\u003c01:20,  1.02s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:   9% 8/86 [00:08\u003c01:17,  1.01it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  10% 9/86 [00:09\u003c01:14,  1.03it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  12% 10/86 [00:10\u003c01:11,  1.06it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  13% 11/86 [00:11\u003c01:12,  1.04it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  14% 12/86 [00:12\u003c01:08,  1.07it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  15% 13/86 [00:12\u003c01:06,  1.10it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  16% 14/86 [00:14\u003c01:08,  1.04it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  17% 15/86 [00:14\u003c01:05,  1.09it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  19% 16/86 [00:15\u003c01:06,  1.06it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  20% 17/86 [00:16\u003c01:09,  1.01s/it]\u001b[A\n","epoch 002: 100% 6757/6758 [15:01\u003c00:00, 11.39it/s, loss=5.669, nll_loss=4.593, ppl=24.13, wps=48873.8, ups=12.31, wpb=3969.2, bsz=77.7, num_updates=13500, lr=8.16497e-05, gnorm=0.82, train_wall=8, gb_free=14.1, wall=1881]\n","epoch 002 | valid on 'valid' subset:  22% 19/86 [00:19\u003c01:17,  1.16s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  23% 20/86 [00:21\u003c01:25,  1.30s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  24% 21/86 [00:22\u003c01:21,  1.25s/it]\u001b[A2024-10-24 10:05:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:05:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:05:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 002 | valid on 'valid' subset:  26% 22/86 [00:23\u003c01:16,  1.19s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  27% 23/86 [00:24\u003c01:09,  1.11s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  28% 24/86 [00:25\u003c01:04,  1.04s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  29% 25/86 [00:26\u003c01:03,  1.04s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  30% 26/86 [00:27\u003c00:59,  1.01it/s]\u001b[A2024-10-24 10:05:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:05:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:05:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 002 | valid on 'valid' subset:  31% 27/86 [00:28\u003c01:00,  1.02s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  33% 28/86 [00:29\u003c00:57,  1.00it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  34% 29/86 [00:30\u003c00:54,  1.04it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  35% 30/86 [00:31\u003c00:55,  1.02it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  36% 31/86 [00:31\u003c00:52,  1.04it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  37% 32/86 [00:33\u003c00:55,  1.03s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  38% 33/86 [00:34\u003c00:59,  1.13s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  40% 34/86 [00:35\u003c01:01,  1.18s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  41% 35/86 [00:37\u003c01:05,  1.29s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  42% 36/86 [00:38\u003c01:00,  1.20s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  43% 37/86 [00:39\u003c00:58,  1.19s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  44% 38/86 [00:40\u003c00:53,  1.11s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  45% 39/86 [00:41\u003c00:50,  1.07s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  47% 40/86 [00:42\u003c00:48,  1.06s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  48% 41/86 [00:43\u003c00:45,  1.00s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  49% 42/86 [00:44\u003c00:44,  1.00s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  50% 43/86 [00:45\u003c00:41,  1.03it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  51% 44/86 [00:46\u003c00:41,  1.01it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  52% 45/86 [00:47\u003c00:40,  1.02it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  53% 46/86 [00:48\u003c00:43,  1.08s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  55% 47/86 [00:49\u003c00:45,  1.16s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  56% 48/86 [00:51\u003c00:49,  1.29s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  57% 49/86 [00:52\u003c00:47,  1.29s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  58% 50/86 [00:54\u003c00:48,  1.35s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  59% 51/86 [00:55\u003c00:43,  1.23s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  60% 52/86 [00:56\u003c00:40,  1.19s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  62% 53/86 [00:57\u003c00:36,  1.10s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  63% 54/86 [00:58\u003c00:34,  1.08s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  64% 55/86 [00:59\u003c00:31,  1.01s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  65% 56/86 [01:00\u003c00:30,  1.02s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  66% 57/86 [01:01\u003c00:28,  1.03it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  67% 58/86 [01:02\u003c00:27,  1.01it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  69% 59/86 [01:03\u003c00:26,  1.02it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  70% 60/86 [01:04\u003c00:25,  1.01it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  71% 61/86 [01:05\u003c00:25,  1.02s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  72% 62/86 [01:06\u003c00:26,  1.10s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  73% 63/86 [01:07\u003c00:26,  1.14s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  74% 64/86 [01:09\u003c00:26,  1.22s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  76% 65/86 [01:10\u003c00:26,  1.27s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  77% 66/86 [01:11\u003c00:23,  1.17s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  78% 67/86 [01:12\u003c00:20,  1.10s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  79% 68/86 [01:13\u003c00:19,  1.06s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  80% 69/86 [01:14\u003c00:17,  1.02s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  81% 70/86 [01:15\u003c00:16,  1.01s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  83% 71/86 [01:16\u003c00:14,  1.04it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  84% 72/86 [01:17\u003c00:13,  1.03it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  85% 73/86 [01:17\u003c00:12,  1.05it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  86% 74/86 [01:18\u003c00:11,  1.05it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  87% 75/86 [01:19\u003c00:10,  1.06it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  88% 76/86 [01:20\u003c00:09,  1.01it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  90% 77/86 [01:22\u003c00:09,  1.04s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  91% 78/86 [01:23\u003c00:08,  1.11s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  92% 79/86 [01:24\u003c00:08,  1.18s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  93% 80/86 [01:26\u003c00:07,  1.23s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  94% 81/86 [01:27\u003c00:06,  1.20s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  95% 82/86 [01:28\u003c00:04,  1.10s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  97% 83/86 [01:28\u003c00:03,  1.04s/it]\u001b[A\n","epoch 002 | valid on 'valid' subset:  98% 84/86 [01:29\u003c00:01,  1.00it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  99% 85/86 [01:30\u003c00:00,  1.04it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset: 100% 86/86 [01:31\u003c00:00,  1.12it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 10:06:35 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 5.995 | nll_loss 4.905 | ppl 29.97 | bleu 7.88 | wps 2253.8 | wpb 2386.9 | bsz 73.7 | num_updates 13516 | best_bleu 7.88\n","2024-10-24 10:06:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 13516 updates\n","2024-10-24 10:06:35 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_best.pt\n","2024-10-24 10:06:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_best.pt\n","2024-10-24 10:06:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulmSR/checkpoint_best.pt (epoch 2 @ 13516 updates, score 7.88) (writing took 0.8322761910003464 seconds)\n","2024-10-24 10:06:35 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n","2024-10-24 10:06:35 | INFO | train | epoch 002 | loss 5.852 | nll_loss 4.805 | ppl 27.96 | wps 26707.1 | ups 6.93 | wpb 3856 | bsz 77.7 | num_updates 13516 | lr 8.16013e-05 | gnorm 0.81 | train_wall 545 | gb_free 14.1 | wall 1975\n","2024-10-24 10:06:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 10:06:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 6758\n","epoch 003:   0% 0/6758 [00:00\u003c?, ?it/s]2024-10-24 10:06:36 | INFO | fairseq.trainer | begin training epoch 3\n","2024-10-24 10:06:36 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 003:   7% 483/6758 [00:44\u003c08:35, 12.17it/s, loss=5.646, nll_loss=4.567, ppl=23.71, wps=46047.8, ups=12.11, wpb=3803, bsz=84.8, num_updates=13900, lr=8.04663e-05, gnorm=0.879, train_wall=8, gb_free=14, wall=2010]2024-10-24 10:07:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 10:07:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 003 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   1% 1/86 [00:00\u003c00:44,  1.90it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   2% 2/86 [00:01\u003c00:47,  1.78it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   3% 3/86 [00:02\u003c01:07,  1.24it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   5% 4/86 [00:03\u003c01:10,  1.16it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   6% 5/86 [00:03\u003c01:04,  1.25it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   7% 6/86 [00:04\u003c01:03,  1.25it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   8% 7/86 [00:05\u003c01:06,  1.18it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   9% 8/86 [00:06\u003c01:08,  1.14it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  10% 9/86 [00:07\u003c01:06,  1.15it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  12% 10/86 [00:08\u003c01:07,  1.12it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  13% 11/86 [00:09\u003c01:12,  1.03it/s]\u001b[A\n","epoch 003:   7% 483/6758 [00:56\u003c08:35, 12.17it/s, loss=5.648, nll_loss=4.569, ppl=23.73, wps=40796.3, ups=10.65, wpb=3831.7, bsz=82.7, num_updates=14000, lr=8.01784e-05, gnorm=0.922, train_wall=8, gb_free=14.2, wall=2019]\n","epoch 003 | valid on 'valid' subset:  15% 13/86 [00:11\u003c01:21,  1.11s/it]\u001b[A2024-10-24 10:07:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:07:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:07:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 003 | valid on 'valid' subset:  16% 14/86 [00:13\u003c01:30,  1.26s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  17% 15/86 [00:14\u003c01:28,  1.25s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  19% 16/86 [00:16\u003c01:29,  1.28s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  20% 17/86 [00:17\u003c01:20,  1.17s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  21% 18/86 [00:18\u003c01:16,  1.12s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  22% 19/86 [00:18\u003c01:09,  1.04s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  23% 20/86 [00:19\u003c01:09,  1.06s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  24% 21/86 [00:20\u003c01:05,  1.01s/it]\u001b[A2024-10-24 10:07:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:07:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:07:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 003 | valid on 'valid' subset:  26% 22/86 [00:22\u003c01:06,  1.05s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  27% 23/86 [00:23\u003c01:05,  1.05s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  28% 24/86 [00:23\u003c01:01,  1.00it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  29% 25/86 [00:25\u003c01:02,  1.03s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  30% 26/86 [00:26\u003c01:03,  1.06s/it]\u001b[A2024-10-24 10:07:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:07:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:07:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 003 | valid on 'valid' subset:  31% 27/86 [00:27\u003c01:10,  1.20s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  33% 28/86 [00:29\u003c01:14,  1.29s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  34% 29/86 [00:30\u003c01:15,  1.32s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  35% 30/86 [00:31\u003c01:14,  1.33s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  36% 31/86 [00:32\u003c01:06,  1.20s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  37% 32/86 [00:33\u003c00:59,  1.11s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  38% 33/86 [00:34\u003c00:58,  1.11s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  40% 34/86 [00:35\u003c00:54,  1.06s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  41% 35/86 [00:36\u003c00:54,  1.07s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  42% 36/86 [00:37\u003c00:51,  1.03s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  43% 37/86 [00:38\u003c00:51,  1.05s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  44% 38/86 [00:39\u003c00:49,  1.04s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  45% 39/86 [00:40\u003c00:47,  1.01s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  47% 40/86 [00:42\u003c00:49,  1.08s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  48% 41/86 [00:43\u003c00:53,  1.18s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  49% 42/86 [00:45\u003c00:56,  1.29s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  50% 43/86 [00:46\u003c00:56,  1.32s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  51% 44/86 [00:48\u003c00:58,  1.39s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  52% 45/86 [00:49\u003c00:52,  1.28s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  53% 46/86 [00:50\u003c00:48,  1.21s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  55% 47/86 [00:51\u003c00:44,  1.15s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  56% 48/86 [00:52\u003c00:42,  1.13s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  57% 49/86 [00:53\u003c00:40,  1.08s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  58% 50/86 [00:54\u003c00:39,  1.10s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  59% 51/86 [00:55\u003c00:37,  1.06s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  60% 52/86 [00:56\u003c00:35,  1.05s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  62% 53/86 [00:57\u003c00:33,  1.01s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  63% 54/86 [00:58\u003c00:34,  1.07s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  64% 55/86 [00:59\u003c00:34,  1.10s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  65% 56/86 [01:00\u003c00:35,  1.18s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  66% 57/86 [01:02\u003c00:35,  1.21s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  67% 58/86 [01:03\u003c00:36,  1.31s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  69% 59/86 [01:04\u003c00:32,  1.22s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  70% 60/86 [01:05\u003c00:30,  1.16s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  71% 61/86 [01:06\u003c00:27,  1.08s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  72% 62/86 [01:07\u003c00:25,  1.06s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  73% 63/86 [01:08\u003c00:23,  1.03s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  74% 64/86 [01:09\u003c00:22,  1.02s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  76% 65/86 [01:10\u003c00:20,  1.00it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  77% 66/86 [01:11\u003c00:19,  1.03it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  78% 67/86 [01:12\u003c00:18,  1.03it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  79% 68/86 [01:13\u003c00:17,  1.05it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  80% 69/86 [01:14\u003c00:17,  1.02s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  81% 70/86 [01:15\u003c00:17,  1.10s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  83% 71/86 [01:17\u003c00:17,  1.17s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  84% 72/86 [01:18\u003c00:17,  1.24s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  85% 73/86 [01:20\u003c00:16,  1.27s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  86% 74/86 [01:21\u003c00:14,  1.24s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  87% 75/86 [01:22\u003c00:12,  1.13s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  88% 76/86 [01:22\u003c00:10,  1.07s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  90% 77/86 [01:23\u003c00:09,  1.02s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  91% 78/86 [01:24\u003c00:08,  1.02s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  92% 79/86 [01:25\u003c00:06,  1.01it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  93% 80/86 [01:26\u003c00:05,  1.02it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  94% 81/86 [01:27\u003c00:04,  1.04it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  95% 82/86 [01:28\u003c00:03,  1.07it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  97% 83/86 [01:29\u003c00:02,  1.06it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  98% 84/86 [01:30\u003c00:01,  1.07it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  99% 85/86 [01:31\u003c00:01,  1.02s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset: 100% 86/86 [01:32\u003c00:00,  1.03s/it]\u001b[A\n","                                                                        \u001b[A2024-10-24 10:08:53 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.012 | nll_loss 4.914 | ppl 30.16 | bleu 7.8 | wps 2214.9 | wpb 2386.9 | bsz 73.7 | num_updates 14000 | best_bleu 7.88\n","2024-10-24 10:08:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 14000 updates\n","2024-10-24 10:08:53 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_3_14000.pt\n","2024-10-24 10:08:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_3_14000.pt\n","2024-10-24 10:08:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulmSR/checkpoint_3_14000.pt (epoch 3 @ 14000 updates, score 7.8) (writing took 1.2134280350001063 seconds)\n","epoch 003:  37% 2482/6758 [05:13\u003c07:00, 10.17it/s, loss=5.59, nll_loss=4.501, ppl=22.64, wps=45321.2, ups=11.64, wpb=3893.1, bsz=82.8, num_updates=15900, lr=7.52355e-05, gnorm=0.884, train_wall=8, gb_free=14.1, wall=2280]2024-10-24 10:11:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 10:11:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 003 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   1% 1/86 [00:01\u003c01:29,  1.05s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:   2% 2/86 [00:02\u003c01:39,  1.18s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:   3% 3/86 [00:03\u003c01:33,  1.12s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:   5% 4/86 [00:04\u003c01:25,  1.04s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:   6% 5/86 [00:05\u003c01:18,  1.03it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   7% 6/86 [00:06\u003c01:17,  1.03it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   8% 7/86 [00:07\u003c01:17,  1.02it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   9% 8/86 [00:08\u003c01:16,  1.03it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  10% 9/86 [00:09\u003c01:14,  1.04it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  12% 10/86 [00:09\u003c01:11,  1.07it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  13% 11/86 [00:10\u003c01:12,  1.04it/s]\u001b[A\n","epoch 003:  37% 2482/6758 [05:26\u003c07:00, 10.17it/s, loss=5.643, nll_loss=4.562, ppl=23.62, wps=43074.7, ups=11.26, wpb=3825.5, bsz=75.5, num_updates=16000, lr=7.5e-05, gnorm=0.898, train_wall=8, gb_free=14.2, wall=2289]   \n","epoch 003 | valid on 'valid' subset:  15% 13/86 [00:12\u003c01:13,  1.01s/it]\u001b[A2024-10-24 10:12:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:12:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:12:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 003 | valid on 'valid' subset:  16% 14/86 [00:14\u003c01:22,  1.14s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  17% 15/86 [00:15\u003c01:23,  1.18s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  19% 16/86 [00:17\u003c01:32,  1.32s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  20% 17/86 [00:18\u003c01:32,  1.34s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  21% 18/86 [00:19\u003c01:25,  1.25s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  22% 19/86 [00:20\u003c01:17,  1.15s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  23% 20/86 [00:21\u003c01:15,  1.15s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  24% 21/86 [00:22\u003c01:10,  1.09s/it]\u001b[A2024-10-24 10:12:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:12:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:12:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 003 | valid on 'valid' subset:  26% 22/86 [00:23\u003c01:12,  1.13s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  27% 23/86 [00:25\u003c01:08,  1.09s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  28% 24/86 [00:25\u003c01:04,  1.03s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  29% 25/86 [00:27\u003c01:06,  1.09s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  30% 26/86 [00:28\u003c01:03,  1.06s/it]\u001b[A2024-10-24 10:12:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:12:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:12:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 003 | valid on 'valid' subset:  31% 27/86 [00:29\u003c01:07,  1.15s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  33% 28/86 [00:30\u003c01:10,  1.21s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  34% 29/86 [00:32\u003c01:09,  1.23s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  35% 30/86 [00:33\u003c01:13,  1.31s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  36% 31/86 [00:34\u003c01:12,  1.31s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  37% 32/86 [00:35\u003c01:03,  1.18s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  38% 33/86 [00:36\u003c01:02,  1.18s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  40% 34/86 [00:37\u003c00:58,  1.12s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  41% 35/86 [00:39\u003c00:57,  1.13s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  42% 36/86 [00:40\u003c00:54,  1.09s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  43% 37/86 [00:41\u003c00:54,  1.10s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  44% 38/86 [00:42\u003c00:50,  1.06s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  45% 39/86 [00:43\u003c00:48,  1.04s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  47% 40/86 [00:44\u003c00:48,  1.05s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  48% 41/86 [00:45\u003c00:47,  1.06s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  49% 42/86 [00:46\u003c00:52,  1.19s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  50% 43/86 [00:48\u003c00:53,  1.24s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  51% 44/86 [00:49\u003c00:57,  1.36s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  52% 45/86 [00:51\u003c00:56,  1.39s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  53% 46/86 [00:52\u003c00:52,  1.32s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  55% 47/86 [00:53\u003c00:47,  1.23s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  56% 48/86 [00:54\u003c00:44,  1.17s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  57% 49/86 [00:55\u003c00:40,  1.09s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  58% 50/86 [00:56\u003c00:39,  1.09s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  59% 51/86 [00:57\u003c00:37,  1.06s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  60% 52/86 [00:58\u003c00:36,  1.07s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  62% 53/86 [00:59\u003c00:33,  1.01s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  63% 54/86 [01:00\u003c00:32,  1.03s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  64% 55/86 [01:01\u003c00:32,  1.03s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  65% 56/86 [01:02\u003c00:33,  1.13s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  66% 57/86 [01:04\u003c00:33,  1.14s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  67% 58/86 [01:05\u003c00:36,  1.29s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  69% 59/86 [01:07\u003c00:36,  1.34s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  70% 60/86 [01:08\u003c00:34,  1.31s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  71% 61/86 [01:09\u003c00:29,  1.17s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  72% 62/86 [01:10\u003c00:27,  1.14s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  73% 63/86 [01:11\u003c00:25,  1.10s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  74% 64/86 [01:12\u003c00:23,  1.08s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  76% 65/86 [01:13\u003c00:21,  1.05s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  77% 66/86 [01:14\u003c00:19,  1.00it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  78% 67/86 [01:15\u003c00:18,  1.02it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  79% 68/86 [01:16\u003c00:17,  1.04it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  80% 69/86 [01:17\u003c00:16,  1.06it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  81% 70/86 [01:17\u003c00:15,  1.05it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  83% 71/86 [01:19\u003c00:15,  1.01s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  84% 72/86 [01:20\u003c00:15,  1.08s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  85% 73/86 [01:21\u003c00:15,  1.20s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  86% 74/86 [01:23\u003c00:15,  1.27s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  87% 75/86 [01:24\u003c00:13,  1.25s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  88% 76/86 [01:25\u003c00:11,  1.16s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  90% 77/86 [01:26\u003c00:09,  1.07s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  91% 78/86 [01:27\u003c00:08,  1.04s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  92% 79/86 [01:28\u003c00:06,  1.00it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  93% 80/86 [01:29\u003c00:05,  1.01it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  94% 81/86 [01:30\u003c00:04,  1.02it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  95% 82/86 [01:30\u003c00:03,  1.06it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  97% 83/86 [01:31\u003c00:02,  1.09it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  98% 84/86 [01:32\u003c00:01,  1.07it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  99% 85/86 [01:33\u003c00:00,  1.08it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset: 100% 86/86 [01:34\u003c00:00,  1.09it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 10:13:24 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.925 | nll_loss 4.813 | ppl 28.11 | bleu 8.4 | wps 2183.5 | wpb 2386.9 | bsz 73.7 | num_updates 16000 | best_bleu 8.4\n","2024-10-24 10:13:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 16000 updates\n","2024-10-24 10:13:24 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_3_16000.pt\n","2024-10-24 10:13:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_3_16000.pt\n","2024-10-24 10:13:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulmSR/checkpoint_3_16000.pt (epoch 3 @ 16000 updates, score 8.4) (writing took 1.5318925980009226 seconds)\n","epoch 003:  66% 4482/6758 [09:46\u003c03:04, 12.31it/s, loss=5.54, nll_loss=4.444, ppl=21.77, wps=41680.3, ups=10.91, wpb=3819.3, bsz=78.5, num_updates=17900, lr=7.09079e-05, gnorm=0.899, train_wall=8, gb_free=14, wall=2553]2024-10-24 10:16:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 10:16:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 003 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   1% 1/86 [00:00\u003c01:10,  1.20it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   2% 2/86 [00:01\u003c01:21,  1.03it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   3% 3/86 [00:03\u003c01:34,  1.13s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:   5% 4/86 [00:04\u003c01:36,  1.18s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:   6% 5/86 [00:05\u003c01:28,  1.09s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:   7% 6/86 [00:06\u003c01:38,  1.23s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:   8% 7/86 [00:08\u003c01:38,  1.25s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:   9% 8/86 [00:09\u003c01:28,  1.13s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  10% 9/86 [00:09\u003c01:20,  1.05s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  12% 10/86 [00:10\u003c01:15,  1.00it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  13% 11/86 [00:11\u003c01:15,  1.01s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  14% 12/86 [00:12\u003c01:12,  1.02it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  15% 13/86 [00:13\u003c01:10,  1.04it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  16% 14/86 [00:14\u003c01:12,  1.00s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  17% 15/86 [00:15\u003c01:07,  1.05it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  19% 16/86 [00:16\u003c01:10,  1.01s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  20% 17/86 [00:17\u003c01:09,  1.01s/it]\u001b[A\n","epoch 003:  66% 4482/6758 [10:06\u003c03:04, 12.31it/s, loss=5.548, nll_loss=4.452, ppl=21.89, wps=46334.6, ups=12.24, wpb=3784.4, bsz=78, num_updates=18000, lr=7.07107e-05, gnorm=0.921, train_wall=8, gb_free=14.1, wall=2561]\n","epoch 003 | valid on 'valid' subset:  22% 19/86 [00:20\u003c01:15,  1.13s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  23% 20/86 [00:21\u003c01:25,  1.29s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  24% 21/86 [00:23\u003c01:25,  1.32s/it]\u001b[A2024-10-24 10:16:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:16:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:16:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 003 | valid on 'valid' subset:  26% 22/86 [00:24\u003c01:29,  1.40s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  27% 23/86 [00:25\u003c01:21,  1.29s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  28% 24/86 [00:26\u003c01:12,  1.18s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  29% 25/86 [00:28\u003c01:11,  1.18s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  30% 26/86 [00:28\u003c01:05,  1.09s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  31% 27/86 [00:30\u003c01:04,  1.10s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  33% 28/86 [00:31\u003c01:02,  1.08s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  34% 29/86 [00:31\u003c00:58,  1.02s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  35% 30/86 [00:33\u003c00:59,  1.06s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  36% 31/86 [00:34\u003c00:55,  1.02s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  37% 32/86 [00:35\u003c00:57,  1.06s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  38% 33/86 [00:37\u003c01:09,  1.32s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  40% 34/86 [00:38\u003c01:11,  1.38s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  41% 35/86 [00:40\u003c01:14,  1.45s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  42% 36/86 [00:41\u003c01:09,  1.39s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  43% 37/86 [00:42\u003c01:04,  1.32s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  44% 38/86 [00:43\u003c00:59,  1.24s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  45% 39/86 [00:44\u003c00:55,  1.18s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  47% 40/86 [00:45\u003c00:53,  1.16s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  48% 41/86 [00:46\u003c00:48,  1.08s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  49% 42/86 [00:47\u003c00:48,  1.09s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  50% 43/86 [00:48\u003c00:45,  1.07s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  51% 44/86 [00:50\u003c00:45,  1.08s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  52% 45/86 [00:51\u003c00:43,  1.07s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  53% 46/86 [00:52\u003c00:47,  1.19s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  55% 47/86 [00:53\u003c00:48,  1.23s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  56% 48/86 [00:55\u003c00:50,  1.34s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  57% 49/86 [00:56\u003c00:48,  1.31s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  58% 50/86 [00:58\u003c00:47,  1.32s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  59% 51/86 [00:59\u003c00:42,  1.23s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  60% 52/86 [01:00\u003c00:40,  1.20s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  62% 53/86 [01:01\u003c00:36,  1.12s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  63% 54/86 [01:02\u003c00:35,  1.10s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  64% 55/86 [01:03\u003c00:32,  1.04s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  65% 56/86 [01:04\u003c00:31,  1.05s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  66% 57/86 [01:04\u003c00:28,  1.01it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  67% 58/86 [01:06\u003c00:28,  1.01s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  69% 59/86 [01:07\u003c00:26,  1.00it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  70% 60/86 [01:08\u003c00:28,  1.08s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  71% 61/86 [01:09\u003c00:28,  1.13s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  72% 62/86 [01:10\u003c00:28,  1.20s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  73% 63/86 [01:12\u003c00:28,  1.24s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  74% 64/86 [01:13\u003c00:28,  1.30s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  76% 65/86 [01:14\u003c00:25,  1.22s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  77% 66/86 [01:15\u003c00:22,  1.12s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  78% 67/86 [01:16\u003c00:20,  1.07s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  79% 68/86 [01:17\u003c00:18,  1.01s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  80% 69/86 [01:18\u003c00:16,  1.03it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  81% 70/86 [01:19\u003c00:15,  1.02it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  83% 71/86 [01:20\u003c00:14,  1.04it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  84% 72/86 [01:21\u003c00:13,  1.05it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  85% 73/86 [01:22\u003c00:12,  1.06it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  86% 74/86 [01:22\u003c00:11,  1.08it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  87% 75/86 [01:23\u003c00:10,  1.08it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  88% 76/86 [01:25\u003c00:10,  1.01s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  90% 77/86 [01:26\u003c00:09,  1.05s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  91% 78/86 [01:27\u003c00:09,  1.16s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  92% 79/86 [01:28\u003c00:08,  1.21s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  93% 80/86 [01:30\u003c00:07,  1.25s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  94% 81/86 [01:31\u003c00:05,  1.19s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  95% 82/86 [01:32\u003c00:04,  1.09s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  97% 83/86 [01:33\u003c00:03,  1.03s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  98% 84/86 [01:33\u003c00:01,  1.02it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  99% 85/86 [01:34\u003c00:00,  1.06it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset: 100% 86/86 [01:35\u003c00:00,  1.10it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 10:17:58 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.881 | nll_loss 4.754 | ppl 26.98 | bleu 8.65 | wps 2153.7 | wpb 2386.9 | bsz 73.7 | num_updates 18000 | best_bleu 8.65\n","2024-10-24 10:17:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 18000 updates\n","2024-10-24 10:17:58 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_3_18000.pt\n","2024-10-24 10:17:58 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_3_18000.pt\n","2024-10-24 10:17:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulmSR/checkpoint_3_18000.pt (epoch 3 @ 18000 updates, score 8.65) (writing took 1.3086846250007511 seconds)\n","epoch 003:  96% 6483/6758 [14:18\u003c00:21, 12.55it/s, loss=5.503, nll_loss=4.399, ppl=21.09, wps=47308.2, ups=12.2, wpb=3877.9, bsz=75.7, num_updates=19900, lr=6.72504e-05, gnorm=0.891, train_wall=8, gb_free=14.1, wall=2825]2024-10-24 10:20:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 10:20:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 003 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   1% 1/86 [00:00\u003c01:06,  1.27it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   2% 2/86 [00:01\u003c00:57,  1.47it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   3% 3/86 [00:02\u003c01:11,  1.16it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   5% 4/86 [00:03\u003c01:12,  1.14it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   6% 5/86 [00:03\u003c01:01,  1.33it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   7% 6/86 [00:04\u003c01:01,  1.29it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   8% 7/86 [00:05\u003c00:57,  1.37it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   9% 8/86 [00:06\u003c01:02,  1.24it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  10% 9/86 [00:07\u003c01:09,  1.11it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  12% 10/86 [00:08\u003c01:19,  1.05s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  13% 11/86 [00:10\u003c01:30,  1.21s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  14% 12/86 [00:11\u003c01:36,  1.30s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  15% 13/86 [00:13\u003c01:36,  1.32s/it]\u001b[A2024-10-24 10:21:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:21:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:21:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 003 | valid on 'valid' subset:  16% 14/86 [00:15\u003c01:46,  1.48s/it]\u001b[A\n","epoch 003:  96% 6483/6758 [14:36\u003c00:21, 12.55it/s, loss=5.46, nll_loss=4.35, ppl=20.4, wps=43722, ups=11.25, wpb=3887.6, bsz=78.5, num_updates=20000, lr=6.7082e-05, gnorm=0.906, train_wall=8, gb_free=14.1, wall=2834]     \n","epoch 003 | valid on 'valid' subset:  19% 16/86 [00:17\u003c01:42,  1.46s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  20% 17/86 [00:18\u003c01:29,  1.30s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  21% 18/86 [00:20\u003c01:25,  1.26s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  22% 19/86 [00:20\u003c01:17,  1.15s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  23% 20/86 [00:22\u003c01:15,  1.14s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  24% 21/86 [00:23\u003c01:11,  1.10s/it]\u001b[A2024-10-24 10:21:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:21:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:21:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 003 | valid on 'valid' subset:  26% 22/86 [00:24\u003c01:10,  1.11s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  27% 23/86 [00:25\u003c01:07,  1.06s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  28% 24/86 [00:26\u003c01:07,  1.08s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  29% 25/86 [00:27\u003c01:13,  1.20s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  30% 26/86 [00:29\u003c01:13,  1.22s/it]\u001b[A2024-10-24 10:21:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:21:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:21:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 003 | valid on 'valid' subset:  31% 27/86 [00:30\u003c01:19,  1.35s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  33% 28/86 [00:32\u003c01:18,  1.35s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  34% 29/86 [00:32\u003c01:09,  1.21s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  35% 30/86 [00:33\u003c01:05,  1.16s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  36% 31/86 [00:34\u003c01:00,  1.10s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  37% 32/86 [00:35\u003c00:55,  1.03s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  38% 33/86 [00:36\u003c00:55,  1.06s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  40% 34/86 [00:37\u003c00:52,  1.01s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  41% 35/86 [00:38\u003c00:52,  1.03s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  42% 36/86 [00:39\u003c00:49,  1.01it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  43% 37/86 [00:40\u003c00:49,  1.02s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  44% 38/86 [00:41\u003c00:48,  1.00s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  45% 39/86 [00:43\u003c00:49,  1.06s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  47% 40/86 [00:44\u003c00:52,  1.14s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  48% 41/86 [00:45\u003c00:53,  1.19s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  49% 42/86 [00:47\u003c00:57,  1.30s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  50% 43/86 [00:48\u003c00:55,  1.29s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  51% 44/86 [00:49\u003c00:51,  1.22s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  52% 45/86 [00:50\u003c00:47,  1.17s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  53% 46/86 [00:51\u003c00:45,  1.13s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  55% 47/86 [00:52\u003c00:42,  1.09s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  56% 48/86 [00:53\u003c00:40,  1.07s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  57% 49/86 [00:54\u003c00:38,  1.03s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  58% 50/86 [00:55\u003c00:37,  1.04s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  59% 51/86 [00:56\u003c00:35,  1.02s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  60% 52/86 [00:57\u003c00:35,  1.05s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  62% 53/86 [00:58\u003c00:35,  1.08s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  63% 54/86 [01:00\u003c00:37,  1.17s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  64% 55/86 [01:01\u003c00:37,  1.20s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  65% 56/86 [01:03\u003c00:38,  1.28s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  66% 57/86 [01:04\u003c00:37,  1.28s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  67% 58/86 [01:05\u003c00:34,  1.22s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  69% 59/86 [01:06\u003c00:30,  1.14s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  70% 60/86 [01:07\u003c00:29,  1.12s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  71% 61/86 [01:08\u003c00:26,  1.05s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  72% 62/86 [01:09\u003c00:25,  1.06s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  73% 63/86 [01:10\u003c00:23,  1.01s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  74% 64/86 [01:11\u003c00:22,  1.01s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  76% 65/86 [01:12\u003c00:21,  1.01s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  77% 66/86 [01:13\u003c00:19,  1.02it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  78% 67/86 [01:14\u003c00:18,  1.03it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  79% 68/86 [01:15\u003c00:18,  1.01s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  80% 69/86 [01:16\u003c00:18,  1.08s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  81% 70/86 [01:17\u003c00:19,  1.20s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  83% 71/86 [01:19\u003c00:18,  1.25s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  84% 72/86 [01:20\u003c00:18,  1.29s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  85% 73/86 [01:21\u003c00:15,  1.18s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  86% 74/86 [01:22\u003c00:13,  1.10s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  87% 75/86 [01:23\u003c00:11,  1.04s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  88% 76/86 [01:24\u003c00:09,  1.01it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  90% 77/86 [01:25\u003c00:08,  1.02it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  91% 78/86 [01:26\u003c00:07,  1.02it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  92% 79/86 [01:27\u003c00:06,  1.05it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  93% 80/86 [01:28\u003c00:05,  1.05it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  94% 81/86 [01:29\u003c00:04,  1.05it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  95% 82/86 [01:30\u003c00:03,  1.05it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  97% 83/86 [01:30\u003c00:02,  1.05it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  98% 84/86 [01:32\u003c00:02,  1.03s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  99% 85/86 [01:33\u003c00:01,  1.05s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset: 100% 86/86 [01:34\u003c00:00,  1.09s/it]\u001b[A\n","                                                                        \u001b[A2024-10-24 10:22:29 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.771 | nll_loss 4.629 | ppl 24.75 | bleu 10.06 | wps 2180.2 | wpb 2386.9 | bsz 73.7 | num_updates 20000 | best_bleu 10.06\n","2024-10-24 10:22:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 20000 updates\n","2024-10-24 10:22:29 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_3_20000.pt\n","2024-10-24 10:22:29 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_3_20000.pt\n","2024-10-24 10:22:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulmSR/checkpoint_3_20000.pt (epoch 3 @ 20000 updates, score 10.06) (writing took 1.7594341399999394 seconds)\n","epoch 003: 100% 6757/6758 [16:19\u003c00:00, 13.08it/s, loss=5.488, nll_loss=4.382, ppl=20.84, wps=42212.6, ups=10.94, wpb=3857, bsz=81.6, num_updates=20200, lr=6.67491e-05, gnorm=0.942, train_wall=8, gb_free=14.1, wall=2948]2024-10-24 10:22:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 10:22:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 003 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   1% 1/86 [00:00\u003c00:46,  1.83it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   2% 2/86 [00:01\u003c01:06,  1.27it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   3% 3/86 [00:02\u003c01:15,  1.10it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   5% 4/86 [00:03\u003c01:23,  1.02s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:   6% 5/86 [00:04\u003c01:21,  1.00s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:   7% 6/86 [00:05\u003c01:14,  1.08it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   8% 7/86 [00:06\u003c01:14,  1.06it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   9% 8/86 [00:07\u003c01:21,  1.05s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  10% 9/86 [00:08\u003c01:22,  1.07s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  12% 10/86 [00:09\u003c01:18,  1.03s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  13% 11/86 [00:10\u003c01:16,  1.03s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  14% 12/86 [00:11\u003c01:15,  1.03s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  15% 13/86 [00:12\u003c01:10,  1.03it/s]\u001b[A2024-10-24 10:23:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:23:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:23:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 003 | valid on 'valid' subset:  16% 14/86 [00:13\u003c01:11,  1.00it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  17% 15/86 [00:14\u003c01:07,  1.05it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  19% 16/86 [00:15\u003c01:07,  1.03it/s]\u001b[A\n","epoch 003: 100% 6757/6758 [16:36\u003c00:00, 13.08it/s, loss=5.488, nll_loss=4.382, ppl=20.84, wps=42212.6, ups=10.94, wpb=3857, bsz=81.6, num_updates=20200, lr=6.67491e-05, gnorm=0.942, train_wall=8, gb_free=14.1, wall=2948]\n","epoch 003 | valid on 'valid' subset:  21% 18/86 [00:17\u003c01:05,  1.04it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  22% 19/86 [00:18\u003c01:02,  1.07it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  23% 20/86 [00:19\u003c01:10,  1.07s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  24% 21/86 [00:20\u003c01:12,  1.12s/it]\u001b[A2024-10-24 10:23:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:23:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:23:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 003 | valid on 'valid' subset:  26% 22/86 [00:22\u003c01:24,  1.32s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  27% 23/86 [00:24\u003c01:25,  1.36s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  28% 24/86 [00:25\u003c01:19,  1.29s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  29% 25/86 [00:26\u003c01:15,  1.24s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  30% 26/86 [00:27\u003c01:07,  1.12s/it]\u001b[A2024-10-24 10:23:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:23:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:23:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 003 | valid on 'valid' subset:  31% 27/86 [00:28\u003c01:05,  1.11s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  33% 28/86 [00:29\u003c01:01,  1.07s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  34% 29/86 [00:30\u003c00:58,  1.02s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  35% 30/86 [00:31\u003c00:58,  1.04s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  36% 31/86 [00:32\u003c00:54,  1.00it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  37% 32/86 [00:33\u003c00:51,  1.05it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  38% 33/86 [00:34\u003c00:53,  1.00s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  40% 34/86 [00:35\u003c00:52,  1.01s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  41% 35/86 [00:36\u003c00:58,  1.15s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  42% 36/86 [00:38\u003c01:01,  1.24s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  43% 37/86 [00:39\u003c01:05,  1.34s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  44% 38/86 [00:41\u003c01:05,  1.37s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  45% 39/86 [00:42\u003c00:57,  1.23s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  47% 40/86 [00:43\u003c00:53,  1.16s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  48% 41/86 [00:43\u003c00:48,  1.08s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  49% 42/86 [00:45\u003c00:47,  1.07s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  50% 43/86 [00:45\u003c00:43,  1.01s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  51% 44/86 [00:46\u003c00:42,  1.01s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  52% 45/86 [00:47\u003c00:40,  1.00it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  53% 46/86 [00:48\u003c00:40,  1.00s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  55% 47/86 [00:49\u003c00:37,  1.03it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  56% 48/86 [00:50\u003c00:37,  1.01it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  57% 49/86 [00:51\u003c00:37,  1.02s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  58% 50/86 [00:53\u003c00:40,  1.12s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  59% 51/86 [00:54\u003c00:41,  1.20s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  60% 52/86 [00:56\u003c00:43,  1.27s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  62% 53/86 [00:57\u003c00:42,  1.28s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  63% 54/86 [00:58\u003c00:39,  1.22s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  64% 55/86 [00:59\u003c00:34,  1.12s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  65% 56/86 [01:00\u003c00:32,  1.09s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  66% 57/86 [01:01\u003c00:29,  1.03s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  67% 58/86 [01:02\u003c00:28,  1.02s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  69% 59/86 [01:03\u003c00:27,  1.00s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  70% 60/86 [01:04\u003c00:26,  1.02s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  71% 61/86 [01:05\u003c00:24,  1.03it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  72% 62/86 [01:06\u003c00:23,  1.03it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  73% 63/86 [01:07\u003c00:22,  1.03it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  74% 64/86 [01:08\u003c00:23,  1.06s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  76% 65/86 [01:09\u003c00:23,  1.11s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  77% 66/86 [01:10\u003c00:23,  1.17s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  78% 67/86 [01:12\u003c00:23,  1.23s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  79% 68/86 [01:13\u003c00:22,  1.22s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  80% 69/86 [01:14\u003c00:19,  1.14s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  81% 70/86 [01:15\u003c00:17,  1.08s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  83% 71/86 [01:16\u003c00:15,  1.02s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  84% 72/86 [01:17\u003c00:13,  1.01it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  85% 73/86 [01:18\u003c00:12,  1.04it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  86% 74/86 [01:18\u003c00:11,  1.05it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  87% 75/86 [01:19\u003c00:10,  1.07it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  88% 76/86 [01:20\u003c00:09,  1.09it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  90% 77/86 [01:21\u003c00:08,  1.08it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  91% 78/86 [01:22\u003c00:07,  1.08it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  92% 79/86 [01:23\u003c00:06,  1.09it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  93% 80/86 [01:24\u003c00:06,  1.02s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  94% 81/86 [01:25\u003c00:05,  1.07s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  95% 82/86 [01:27\u003c00:04,  1.13s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  97% 83/86 [01:28\u003c00:03,  1.17s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  98% 84/86 [01:29\u003c00:02,  1.19s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset:  99% 85/86 [01:30\u003c00:01,  1.14s/it]\u001b[A\n","epoch 003 | valid on 'valid' subset: 100% 86/86 [01:31\u003c00:00,  1.03s/it]\u001b[A\n","                                                                        \u001b[A2024-10-24 10:24:26 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.758 | nll_loss 4.615 | ppl 24.51 | bleu 9.91 | wps 2244.1 | wpb 2386.9 | bsz 73.7 | num_updates 20274 | best_bleu 10.06\n","2024-10-24 10:24:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 20274 updates\n","2024-10-24 10:24:26 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_last.pt\n","2024-10-24 10:24:27 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_last.pt\n","2024-10-24 10:24:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulmSR/checkpoint_last.pt (epoch 3 @ 20274 updates, score 9.91) (writing took 0.40643842999998014 seconds)\n","2024-10-24 10:24:27 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n","2024-10-24 10:24:27 | INFO | train | epoch 003 | loss 5.576 | nll_loss 4.484 | ppl 22.38 | wps 24323.3 | ups 6.31 | wpb 3856 | bsz 77.7 | num_updates 20274 | lr 6.66272e-05 | gnorm 0.891 | train_wall 545 | gb_free 14.1 | wall 3046\n","2024-10-24 10:24:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 10:24:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 6758\n","epoch 004:   0% 0/6758 [00:00\u003c?, ?it/s]2024-10-24 10:24:27 | INFO | fairseq.trainer | begin training epoch 4\n","2024-10-24 10:24:27 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 004:  26% 1724/6758 [02:31\u003c06:39, 12.60it/s, loss=5.4, nll_loss=4.281, ppl=19.44, wps=38133.5, ups=9.96, wpb=3829.9, bsz=83.3, num_updates=21900, lr=6.41061e-05, gnorm=0.929, train_wall=9, gb_free=14, wall=3189]2024-10-24 10:26:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 10:26:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 004 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   1% 1/86 [00:00\u003c00:57,  1.47it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   2% 2/86 [00:01\u003c01:13,  1.15it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   3% 3/86 [00:02\u003c01:22,  1.00it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   5% 4/86 [00:04\u003c01:28,  1.07s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:   6% 5/86 [00:04\u003c01:15,  1.08it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   7% 6/86 [00:05\u003c01:09,  1.16it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   8% 7/86 [00:06\u003c01:06,  1.18it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   9% 8/86 [00:06\u003c01:03,  1.23it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  10% 9/86 [00:07\u003c01:02,  1.23it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  12% 10/86 [00:08\u003c01:08,  1.12it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  13% 11/86 [00:09\u003c01:04,  1.16it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  14% 12/86 [00:10\u003c01:02,  1.19it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  15% 13/86 [00:11\u003c01:01,  1.20it/s]\u001b[A2024-10-24 10:27:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:27:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:27:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 004 | valid on 'valid' subset:  16% 14/86 [00:12\u003c01:03,  1.13it/s]\u001b[A\n","epoch 004:  26% 1724/6758 [02:44\u003c06:39, 12.60it/s, loss=5.46, nll_loss=4.349, ppl=20.38, wps=46867.1, ups=12.28, wpb=3818, bsz=74.6, num_updates=22000, lr=6.39602e-05, gnorm=0.933, train_wall=8, gb_free=14, wall=3198]\n","epoch 004 | valid on 'valid' subset:  19% 16/86 [00:13\u003c00:57,  1.22it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  20% 17/86 [00:14\u003c00:52,  1.32it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  21% 18/86 [00:15\u003c00:51,  1.31it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  22% 19/86 [00:15\u003c00:48,  1.37it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  23% 20/86 [00:16\u003c00:49,  1.34it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  24% 21/86 [00:17\u003c00:49,  1.32it/s]\u001b[A2024-10-24 10:27:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:27:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:27:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 004 | valid on 'valid' subset:  26% 22/86 [00:18\u003c00:52,  1.22it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  27% 23/86 [00:19\u003c00:58,  1.07it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  28% 24/86 [00:20\u003c00:55,  1.12it/s]\u001b[A2024-10-24 10:27:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:27:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:27:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 004 | valid on 'valid' subset:  29% 25/86 [00:21\u003c01:04,  1.05s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  30% 26/86 [00:22\u003c00:59,  1.01it/s]\u001b[A2024-10-24 10:27:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:27:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:27:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 004 | valid on 'valid' subset:  31% 27/86 [00:23\u003c01:02,  1.05s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  33% 28/86 [00:24\u003c01:02,  1.07s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  34% 29/86 [00:25\u003c00:54,  1.05it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  35% 30/86 [00:26\u003c00:50,  1.12it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  36% 31/86 [00:27\u003c00:48,  1.13it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  37% 32/86 [00:27\u003c00:43,  1.23it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  38% 33/86 [00:28\u003c00:43,  1.22it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  40% 34/86 [00:29\u003c00:40,  1.28it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  41% 35/86 [00:30\u003c00:43,  1.18it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  42% 36/86 [00:31\u003c00:42,  1.17it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  43% 37/86 [00:32\u003c00:43,  1.12it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  44% 38/86 [00:33\u003c00:41,  1.16it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  45% 39/86 [00:33\u003c00:40,  1.17it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  47% 40/86 [00:34\u003c00:37,  1.22it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  48% 41/86 [00:35\u003c00:36,  1.22it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  49% 42/86 [00:36\u003c00:42,  1.04it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  50% 43/86 [00:37\u003c00:42,  1.02it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  51% 44/86 [00:39\u003c00:47,  1.13s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  52% 45/86 [00:40\u003c00:45,  1.10s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  53% 46/86 [00:41\u003c00:44,  1.11s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  55% 47/86 [00:42\u003c00:40,  1.03s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  56% 48/86 [00:43\u003c00:38,  1.01s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  57% 49/86 [00:43\u003c00:34,  1.08it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  58% 50/86 [00:44\u003c00:33,  1.07it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  59% 51/86 [00:45\u003c00:31,  1.11it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  60% 52/86 [00:46\u003c00:30,  1.13it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  62% 53/86 [00:47\u003c00:27,  1.18it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  63% 54/86 [00:48\u003c00:28,  1.14it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  64% 55/86 [00:49\u003c00:26,  1.18it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  65% 56/86 [00:49\u003c00:24,  1.22it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  66% 57/86 [00:50\u003c00:23,  1.24it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  67% 58/86 [00:51\u003c00:24,  1.15it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  69% 59/86 [00:52\u003c00:27,  1.00s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  70% 60/86 [00:54\u003c00:28,  1.10s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  71% 61/86 [00:55\u003c00:28,  1.16s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  72% 62/86 [00:56\u003c00:29,  1.23s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  73% 63/86 [00:57\u003c00:27,  1.18s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  74% 64/86 [00:58\u003c00:24,  1.11s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  76% 65/86 [00:59\u003c00:21,  1.01s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  77% 66/86 [01:00\u003c00:19,  1.04it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  78% 67/86 [01:01\u003c00:17,  1.12it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  79% 68/86 [01:02\u003c00:15,  1.18it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  80% 69/86 [01:02\u003c00:14,  1.15it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  81% 70/86 [01:03\u003c00:14,  1.13it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  83% 71/86 [01:04\u003c00:13,  1.14it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  84% 72/86 [01:05\u003c00:12,  1.12it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  85% 73/86 [01:06\u003c00:11,  1.13it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  86% 74/86 [01:07\u003c00:10,  1.11it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  87% 75/86 [01:08\u003c00:10,  1.03it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  88% 76/86 [01:09\u003c00:10,  1.03s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  90% 77/86 [01:11\u003c00:09,  1.09s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  91% 78/86 [01:12\u003c00:09,  1.15s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  92% 79/86 [01:13\u003c00:08,  1.20s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  93% 80/86 [01:14\u003c00:07,  1.17s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  94% 81/86 [01:15\u003c00:05,  1.09s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  95% 82/86 [01:16\u003c00:04,  1.02s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  97% 83/86 [01:17\u003c00:02,  1.03it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  98% 84/86 [01:18\u003c00:01,  1.04it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  99% 85/86 [01:19\u003c00:00,  1.05it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset: 100% 86/86 [01:19\u003c00:00,  1.11it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 10:28:18 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.688 | nll_loss 4.544 | ppl 23.33 | bleu 10.35 | wps 2574.9 | wpb 2386.9 | bsz 73.7 | num_updates 22000 | best_bleu 10.35\n","2024-10-24 10:28:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 22000 updates\n","2024-10-24 10:28:18 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_4_22000.pt\n","2024-10-24 10:28:19 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_4_22000.pt\n","2024-10-24 10:28:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulmSR/checkpoint_4_22000.pt (epoch 4 @ 22000 updates, score 10.35) (writing took 1.5885669479994249 seconds)\n","epoch 004:  55% 3724/6758 [06:47\u003c04:06, 12.31it/s, loss=5.384, nll_loss=4.262, ppl=19.19, wps=44499.9, ups=11.63, wpb=3825.8, bsz=83.2, num_updates=23900, lr=6.13652e-05, gnorm=0.959, train_wall=8, gb_free=14.1, wall=3445]2024-10-24 10:31:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 10:31:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 004 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   1% 1/86 [00:00\u003c00:57,  1.47it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   2% 2/86 [00:01\u003c00:49,  1.69it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   3% 3/86 [00:02\u003c01:05,  1.26it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   5% 4/86 [00:03\u003c01:06,  1.24it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   6% 5/86 [00:03\u003c00:59,  1.36it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   7% 6/86 [00:04\u003c01:00,  1.32it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   8% 7/86 [00:05\u003c01:05,  1.20it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   9% 8/86 [00:06\u003c01:13,  1.06it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  10% 9/86 [00:07\u003c01:21,  1.05s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  12% 10/86 [00:09\u003c01:28,  1.16s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  13% 11/86 [00:10\u003c01:27,  1.17s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  14% 12/86 [00:11\u003c01:21,  1.10s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  15% 13/86 [00:12\u003c01:15,  1.04s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  16% 14/86 [00:13\u003c01:15,  1.05s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  17% 15/86 [00:14\u003c01:09,  1.02it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  19% 16/86 [00:15\u003c01:10,  1.00s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  20% 17/86 [00:16\u003c01:07,  1.02it/s]\u001b[A\n","epoch 004:  55% 3724/6758 [07:04\u003c04:06, 12.31it/s, loss=5.401, nll_loss=4.282, ppl=19.45, wps=45928.9, ups=11.83, wpb=3882.1, bsz=74.6, num_updates=24000, lr=6.12372e-05, gnorm=0.943, train_wall=8, gb_free=14.2, wall=3454]\n","epoch 004 | valid on 'valid' subset:  22% 19/86 [00:18\u003c01:04,  1.04it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  23% 20/86 [00:19\u003c01:06,  1.01s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  24% 21/86 [00:20\u003c01:09,  1.06s/it]\u001b[A2024-10-24 10:31:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:31:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:31:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 004 | valid on 'valid' subset:  26% 22/86 [00:21\u003c01:15,  1.17s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  27% 23/86 [00:23\u003c01:18,  1.24s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  28% 24/86 [00:24\u003c01:17,  1.26s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  29% 25/86 [00:26\u003c01:24,  1.39s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  30% 26/86 [00:27\u003c01:14,  1.24s/it]\u001b[A2024-10-24 10:31:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:31:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:31:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 004 | valid on 'valid' subset:  31% 27/86 [00:28\u003c01:11,  1.21s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  33% 28/86 [00:29\u003c01:06,  1.14s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  34% 29/86 [00:30\u003c01:02,  1.09s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  35% 30/86 [00:31\u003c01:01,  1.09s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  36% 31/86 [00:32\u003c00:56,  1.04s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  37% 32/86 [00:33\u003c00:53,  1.01it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  38% 33/86 [00:34\u003c00:53,  1.01s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  40% 34/86 [00:35\u003c00:51,  1.02it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  41% 35/86 [00:36\u003c00:51,  1.01s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  42% 36/86 [00:37\u003c00:53,  1.08s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  43% 37/86 [00:38\u003c00:58,  1.19s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  44% 38/86 [00:40\u003c01:00,  1.27s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  45% 39/86 [00:41\u003c01:00,  1.29s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  47% 40/86 [00:42\u003c00:57,  1.26s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  48% 41/86 [00:43\u003c00:51,  1.14s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  49% 42/86 [00:44\u003c00:49,  1.12s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  50% 43/86 [00:45\u003c00:45,  1.06s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  51% 44/86 [00:46\u003c00:44,  1.05s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  52% 45/86 [00:47\u003c00:42,  1.02s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  53% 46/86 [00:48\u003c00:41,  1.03s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  55% 47/86 [00:49\u003c00:38,  1.01it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  56% 48/86 [00:50\u003c00:38,  1.01s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  57% 49/86 [00:51\u003c00:35,  1.04it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  58% 50/86 [00:52\u003c00:36,  1.01s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  59% 51/86 [00:53\u003c00:38,  1.11s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  60% 52/86 [00:55\u003c00:39,  1.17s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  62% 53/86 [00:56\u003c00:39,  1.21s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  63% 54/86 [00:58\u003c00:41,  1.31s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  64% 55/86 [00:59\u003c00:37,  1.22s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  65% 56/86 [01:00\u003c00:37,  1.24s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  66% 57/86 [01:01\u003c00:35,  1.22s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  67% 58/86 [01:03\u003c00:36,  1.32s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  69% 59/86 [01:04\u003c00:36,  1.34s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  70% 60/86 [01:05\u003c00:35,  1.37s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  71% 61/86 [01:06\u003c00:30,  1.22s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  72% 62/86 [01:07\u003c00:27,  1.15s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  73% 63/86 [01:08\u003c00:25,  1.12s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  74% 64/86 [01:10\u003c00:26,  1.20s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  76% 65/86 [01:11\u003c00:26,  1.24s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  77% 66/86 [01:13\u003c00:25,  1.28s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  78% 67/86 [01:14\u003c00:25,  1.32s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  79% 68/86 [01:15\u003c00:22,  1.24s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  80% 69/86 [01:16\u003c00:19,  1.14s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  81% 70/86 [01:17\u003c00:17,  1.09s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  83% 71/86 [01:18\u003c00:15,  1.04s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  84% 72/86 [01:19\u003c00:14,  1.02s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  85% 73/86 [01:20\u003c00:12,  1.00it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  86% 74/86 [01:21\u003c00:11,  1.03it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  87% 75/86 [01:22\u003c00:10,  1.05it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  88% 76/86 [01:22\u003c00:09,  1.08it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  90% 77/86 [01:23\u003c00:08,  1.10it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  91% 78/86 [01:24\u003c00:07,  1.08it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  92% 79/86 [01:25\u003c00:07,  1.01s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  93% 80/86 [01:27\u003c00:06,  1.09s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  94% 81/86 [01:28\u003c00:05,  1.16s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  95% 82/86 [01:29\u003c00:04,  1.20s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  97% 83/86 [01:31\u003c00:03,  1.21s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  98% 84/86 [01:31\u003c00:02,  1.12s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  99% 85/86 [01:32\u003c00:01,  1.05s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset: 100% 86/86 [01:33\u003c00:00,  1.04it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 10:32:48 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.667 | nll_loss 4.502 | ppl 22.66 | bleu 11.2 | wps 2197.6 | wpb 2386.9 | bsz 73.7 | num_updates 24000 | best_bleu 11.2\n","2024-10-24 10:32:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 24000 updates\n","2024-10-24 10:32:48 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_4_24000.pt\n","2024-10-24 10:32:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_4_24000.pt\n","2024-10-24 10:32:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulmSR/checkpoint_4_24000.pt (epoch 4 @ 24000 updates, score 11.2) (writing took 1.2321094109993282 seconds)\n","epoch 004:  85% 5724/6758 [11:15\u003c01:22, 12.57it/s, loss=5.381, nll_loss=4.258, ppl=19.13, wps=47184.5, ups=12.26, wpb=3849.7, bsz=74.2, num_updates=25900, lr=5.89483e-05, gnorm=0.937, train_wall=8, gb_free=14.1, wall=3713]2024-10-24 10:35:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 10:35:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 004 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   1% 1/86 [00:00\u003c00:52,  1.60it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   2% 2/86 [00:01\u003c01:08,  1.23it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   3% 3/86 [00:02\u003c01:15,  1.10it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   5% 4/86 [00:03\u003c01:03,  1.28it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   6% 5/86 [00:03\u003c00:54,  1.47it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   7% 6/86 [00:04\u003c00:52,  1.54it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   8% 7/86 [00:05\u003c00:58,  1.36it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   9% 8/86 [00:06\u003c01:02,  1.25it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  10% 9/86 [00:07\u003c01:05,  1.18it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  12% 10/86 [00:08\u003c01:12,  1.05it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  13% 11/86 [00:09\u003c01:22,  1.10s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  14% 12/86 [00:11\u003c01:26,  1.17s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  15% 13/86 [00:12\u003c01:27,  1.20s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  16% 14/86 [00:13\u003c01:22,  1.15s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  17% 15/86 [00:14\u003c01:14,  1.05s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  19% 16/86 [00:15\u003c01:12,  1.03s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  20% 17/86 [00:16\u003c01:08,  1.01it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  21% 18/86 [00:17\u003c01:07,  1.00it/s]\u001b[A\n","epoch 004:  85% 5724/6758 [11:34\u003c01:22, 12.57it/s, loss=5.361, nll_loss=4.235, ppl=18.83, wps=43127, ups=11.11, wpb=3881.1, bsz=77.5, num_updates=26000, lr=5.88348e-05, gnorm=0.96, train_wall=8, gb_free=14.1, wall=3722]   \n","epoch 004 | valid on 'valid' subset:  23% 20/86 [00:18\u003c01:04,  1.02it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  24% 21/86 [00:19\u003c01:03,  1.03it/s]\u001b[A2024-10-24 10:36:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:36:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:36:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 004 | valid on 'valid' subset:  26% 22/86 [00:20\u003c01:04,  1.01s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  27% 23/86 [00:21\u003c01:03,  1.00s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  28% 24/86 [00:23\u003c01:03,  1.02s/it]\u001b[A2024-10-24 10:36:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:36:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:36:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 004 | valid on 'valid' subset:  29% 25/86 [00:24\u003c01:10,  1.15s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  30% 26/86 [00:25\u003c01:09,  1.16s/it]\u001b[A2024-10-24 10:36:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:36:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:36:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 004 | valid on 'valid' subset:  31% 27/86 [00:27\u003c01:16,  1.30s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  33% 28/86 [00:28\u003c01:16,  1.32s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  34% 29/86 [00:29\u003c01:07,  1.19s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  35% 30/86 [00:30\u003c01:04,  1.15s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  36% 31/86 [00:31\u003c00:59,  1.08s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  37% 32/86 [00:32\u003c00:53,  1.00it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  38% 33/86 [00:33\u003c00:53,  1.00s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  40% 34/86 [00:34\u003c00:50,  1.03it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  41% 35/86 [00:35\u003c00:50,  1.01it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  42% 36/86 [00:36\u003c00:48,  1.03it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  43% 37/86 [00:37\u003c00:49,  1.01s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  44% 38/86 [00:38\u003c00:46,  1.02it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  45% 39/86 [00:39\u003c00:46,  1.01it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  47% 40/86 [00:40\u003c00:50,  1.10s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  48% 41/86 [00:41\u003c00:50,  1.11s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  49% 42/86 [00:43\u003c00:54,  1.24s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  50% 43/86 [00:44\u003c00:54,  1.26s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  51% 44/86 [00:45\u003c00:51,  1.23s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  52% 45/86 [00:46\u003c00:46,  1.14s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  53% 46/86 [00:47\u003c00:44,  1.11s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  55% 47/86 [00:48\u003c00:40,  1.05s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  56% 48/86 [00:49\u003c00:43,  1.15s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  57% 49/86 [00:50\u003c00:39,  1.07s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  58% 50/86 [00:51\u003c00:37,  1.05s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  59% 51/86 [00:52\u003c00:35,  1.02s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  60% 52/86 [00:53\u003c00:34,  1.01s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  62% 53/86 [00:54\u003c00:32,  1.03it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  63% 54/86 [00:55\u003c00:34,  1.06s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  64% 55/86 [00:57\u003c00:34,  1.11s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  65% 56/86 [00:58\u003c00:36,  1.22s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  66% 57/86 [00:59\u003c00:36,  1.25s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  67% 58/86 [01:01\u003c00:36,  1.30s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  69% 59/86 [01:02\u003c00:32,  1.22s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  70% 60/86 [01:03\u003c00:30,  1.17s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  71% 61/86 [01:04\u003c00:27,  1.09s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  72% 62/86 [01:05\u003c00:25,  1.08s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  73% 63/86 [01:06\u003c00:23,  1.03s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  74% 64/86 [01:07\u003c00:22,  1.02s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  76% 65/86 [01:08\u003c00:20,  1.01it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  77% 66/86 [01:09\u003c00:19,  1.02it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  78% 67/86 [01:10\u003c00:18,  1.04it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  79% 68/86 [01:11\u003c00:16,  1.07it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  80% 69/86 [01:12\u003c00:18,  1.06s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  81% 70/86 [01:13\u003c00:18,  1.14s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  83% 71/86 [01:14\u003c00:17,  1.19s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  84% 72/86 [01:16\u003c00:17,  1.26s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  85% 73/86 [01:17\u003c00:17,  1.32s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  86% 74/86 [01:18\u003c00:14,  1.21s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  87% 75/86 [01:19\u003c00:12,  1.11s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  88% 76/86 [01:20\u003c00:10,  1.05s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  90% 77/86 [01:21\u003c00:08,  1.00it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  91% 78/86 [01:22\u003c00:07,  1.02it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  92% 79/86 [01:23\u003c00:06,  1.05it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  93% 80/86 [01:24\u003c00:05,  1.04it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  94% 81/86 [01:25\u003c00:04,  1.05it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  95% 82/86 [01:26\u003c00:03,  1.07it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  97% 83/86 [01:26\u003c00:02,  1.10it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  98% 84/86 [01:27\u003c00:01,  1.11it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  99% 85/86 [01:28\u003c00:00,  1.03it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset: 100% 86/86 [01:30\u003c00:00,  1.01it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 10:37:13 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.614 | nll_loss 4.443 | ppl 21.74 | bleu 11.45 | wps 2284.3 | wpb 2386.9 | bsz 73.7 | num_updates 26000 | best_bleu 11.45\n","2024-10-24 10:37:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 26000 updates\n","2024-10-24 10:37:13 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_4_26000.pt\n","2024-10-24 10:37:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_4_26000.pt\n","2024-10-24 10:37:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulmSR/checkpoint_4_26000.pt (epoch 4 @ 26000 updates, score 11.45) (writing took 1.6656833060005738 seconds)\n","epoch 004: 100% 6757/6758 [14:19\u003c00:00, 13.63it/s, loss=5.345, nll_loss=4.216, ppl=18.59, wps=46776.6, ups=12.01, wpb=3894.4, bsz=75.5, num_updates=27000, lr=5.7735e-05, gnorm=0.945, train_wall=8, gb_free=14.1, wall=3903]2024-10-24 10:38:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-10-24 10:38:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 004 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   1% 1/86 [00:00\u003c00:51,  1.64it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   2% 2/86 [00:01\u003c01:25,  1.02s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:   3% 3/86 [00:03\u003c01:36,  1.16s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:   5% 4/86 [00:04\u003c01:32,  1.13s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:   6% 5/86 [00:05\u003c01:18,  1.03it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   7% 6/86 [00:06\u003c01:26,  1.08s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:   8% 7/86 [00:07\u003c01:20,  1.01s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:   9% 8/86 [00:08\u003c01:16,  1.01it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  10% 9/86 [00:08\u003c01:11,  1.07it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  12% 10/86 [00:09\u003c01:11,  1.07it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  13% 11/86 [00:10\u003c01:11,  1.04it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  14% 12/86 [00:11\u003c01:08,  1.08it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  15% 13/86 [00:12\u003c01:06,  1.10it/s]\u001b[A2024-10-24 10:39:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:39:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:39:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 004 | valid on 'valid' subset:  16% 14/86 [00:13\u003c01:09,  1.03it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  17% 15/86 [00:14\u003c01:04,  1.10it/s]\u001b[A\n","epoch 004: 100% 6757/6758 [14:34\u003c00:00, 13.63it/s, loss=5.345, nll_loss=4.216, ppl=18.59, wps=46776.6, ups=12.01, wpb=3894.4, bsz=75.5, num_updates=27000, lr=5.7735e-05, gnorm=0.945, train_wall=8, gb_free=14.1, wall=3903]\n","epoch 004 | valid on 'valid' subset:  20% 17/86 [00:16\u003c01:04,  1.07it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  21% 18/86 [00:17\u003c01:11,  1.06s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  22% 19/86 [00:19\u003c01:15,  1.12s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  23% 20/86 [00:20\u003c01:22,  1.26s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  24% 21/86 [00:21\u003c01:23,  1.29s/it]\u001b[A2024-10-24 10:39:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:39:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:39:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 004 | valid on 'valid' subset:  26% 22/86 [00:23\u003c01:24,  1.33s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  27% 23/86 [00:24\u003c01:15,  1.20s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  28% 24/86 [00:25\u003c01:08,  1.11s/it]\u001b[A2024-10-24 10:39:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:39:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:39:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 004 | valid on 'valid' subset:  29% 25/86 [00:26\u003c01:08,  1.13s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  30% 26/86 [00:27\u003c01:03,  1.06s/it]\u001b[A2024-10-24 10:39:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-10-24 10:39:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-10-24 10:39:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 004 | valid on 'valid' subset:  31% 27/86 [00:28\u003c01:02,  1.06s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  33% 28/86 [00:29\u003c00:59,  1.03s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  34% 29/86 [00:30\u003c00:57,  1.01s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  35% 30/86 [00:31\u003c00:56,  1.01s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  36% 31/86 [00:32\u003c00:54,  1.02it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  37% 32/86 [00:32\u003c00:50,  1.06it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  38% 33/86 [00:34\u003c00:57,  1.09s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  40% 34/86 [00:35\u003c00:58,  1.13s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  41% 35/86 [00:37\u003c01:04,  1.27s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  42% 36/86 [00:38\u003c01:05,  1.30s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  43% 37/86 [00:39\u003c01:03,  1.29s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  44% 38/86 [00:40\u003c00:57,  1.20s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  45% 39/86 [00:41\u003c00:52,  1.12s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  47% 40/86 [00:42\u003c00:50,  1.09s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  48% 41/86 [00:43\u003c00:45,  1.01s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  49% 42/86 [00:44\u003c00:44,  1.02s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  50% 43/86 [00:45\u003c00:42,  1.02it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  51% 44/86 [00:46\u003c00:41,  1.00it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  52% 45/86 [00:47\u003c00:40,  1.01it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  53% 46/86 [00:48\u003c00:39,  1.01it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  55% 47/86 [00:49\u003c00:39,  1.02s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  56% 48/86 [00:51\u003c00:42,  1.13s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  57% 49/86 [00:52\u003c00:42,  1.15s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  58% 50/86 [00:53\u003c00:45,  1.26s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  59% 51/86 [00:55\u003c00:45,  1.31s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  60% 52/86 [00:56\u003c00:42,  1.25s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  62% 53/86 [00:57\u003c00:37,  1.14s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  63% 54/86 [00:58\u003c00:36,  1.13s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  64% 55/86 [00:59\u003c00:32,  1.06s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  65% 56/86 [01:00\u003c00:31,  1.04s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  66% 57/86 [01:01\u003c00:28,  1.02it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  67% 58/86 [01:02\u003c00:27,  1.01it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  69% 59/86 [01:03\u003c00:26,  1.02it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  70% 60/86 [01:04\u003c00:25,  1.01it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  71% 61/86 [01:04\u003c00:24,  1.04it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  72% 62/86 [01:06\u003c00:25,  1.05s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  73% 63/86 [01:07\u003c00:25,  1.11s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  74% 64/86 [01:08\u003c00:26,  1.22s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  76% 65/86 [01:10\u003c00:26,  1.26s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  77% 66/86 [01:11\u003c00:25,  1.30s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  78% 67/86 [01:12\u003c00:22,  1.21s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  79% 68/86 [01:13\u003c00:20,  1.12s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  80% 69/86 [01:14\u003c00:17,  1.05s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  81% 70/86 [01:15\u003c00:16,  1.04s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  83% 71/86 [01:16\u003c00:15,  1.00s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  84% 72/86 [01:17\u003c00:13,  1.02it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  85% 73/86 [01:18\u003c00:12,  1.01it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  86% 74/86 [01:19\u003c00:11,  1.03it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  87% 75/86 [01:20\u003c00:10,  1.05it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  88% 76/86 [01:21\u003c00:09,  1.07it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  90% 77/86 [01:21\u003c00:08,  1.09it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  91% 78/86 [01:23\u003c00:08,  1.02s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  92% 79/86 [01:24\u003c00:07,  1.08s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  93% 80/86 [01:25\u003c00:07,  1.18s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  94% 81/86 [01:27\u003c00:06,  1.22s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  95% 82/86 [01:28\u003c00:04,  1.23s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  97% 83/86 [01:29\u003c00:03,  1.11s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  98% 84/86 [01:30\u003c00:02,  1.06s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset:  99% 85/86 [01:31\u003c00:01,  1.00s/it]\u001b[A\n","epoch 004 | valid on 'valid' subset: 100% 86/86 [01:31\u003c00:00,  1.09it/s]\u001b[A\n","                                                                        \u001b[A2024-10-24 10:40:18 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.595 | nll_loss 4.419 | ppl 21.39 | bleu 11.72 | wps 2240.8 | wpb 2386.9 | bsz 73.7 | num_updates 27032 | best_bleu 11.72\n","2024-10-24 10:40:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 27032 updates\n","2024-10-24 10:40:18 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_best.pt\n","2024-10-24 10:40:18 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/ulmSR/checkpoints-ulmSR/checkpoint_best.pt\n","2024-10-24 10:40:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-ulmSR/checkpoint_best.pt (epoch 4 @ 27032 updates, score 11.72) (writing took 0.8052180019985826 seconds)\n","2024-10-24 10:40:19 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n","2024-10-24 10:40:19 | INFO | train | epoch 004 | loss 5.404 | nll_loss 4.285 | ppl 19.5 | wps 27378.1 | ups 7.1 | wpb 3856 | bsz 77.7 | num_updates 27032 | lr 5.77008e-05 | gnorm 0.947 | train_wall 541 | gb_free 14.2 | wall 3998\n","2024-10-24 10:40:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-10-24 10:40:19 | INFO | fairseq_cli.train | done training in 3987.2 seconds\n"]}],"source":["!fairseq-train data-bin-25 \\\n","--arch transformer \\\n","--activation-fn relu \\\n","--share-decoder-input-output-embed \\\n","--share-all-embeddings \\\n","--encoder-layers 3 \\\n","--encoder-attention-heads 4 \\\n","--encoder-embed-dim 256 \\\n","--encoder-ffn-embed-dim 1024 \\\n","--decoder-layers 3 \\\n","--decoder-attention-heads 4 \\\n","--decoder-embed-dim 256 \\\n","--decoder-ffn-embed-dim 1024 \\\n","--dropout 0.25 \\\n","--seed 2024 \\\n","--optimizer 'adam' \\\n","--adam-betas '(0.9, 0.999)' \\\n","--lr-scheduler 'inverse_sqrt' \\\n","--patience 5 \\\n","--warmup-updates 1000 \\\n","--criterion 'label_smoothed_cross_entropy' \\\n","--label-smoothing 0.1 \\\n","--lr 0.0003 \\\n","--weight-decay 0.0 \\\n","--max-tokens 4096 \\\n","--max-tokens-valid 3600 \\\n","--required-batch-size-multiple 1 \\\n","--best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n","--max-epoch 4 \\\n","--validate-interval 1 \\\n","--save-interval 1 \\\n","--validate-interval-updates 2000 \\\n","--save-interval-updates 2000 \\\n","--log-interval 100 \\\n","--curriculum 0 \\\n","--no-epoch-checkpoints \\\n","--eval-bleu \\\n","--eval-bleu-args '{\"beam\": 5, \"max_len_a\": 0, \"max_len_b\": 100, \"len_pen\": 1}' \\\n","--eval-bleu-detok space \\\n","--eval-bleu-remove-bpe sentencepiece \\\n","--save-dir checkpoints-ulmSR \\\n","--ddp-backend=no_c10d \\\n","--wandb-project 'fairseq-standard-subword-tok-eng-to-nso'"]},{"cell_type":"markdown","metadata":{"id":"8An9zYviiGOv"},"source":["## Training NMT with Task-Specific Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FT6J6r3kiJrC"},"outputs":[],"source":["# change working directory\n","os.chdir(f'/content/drive/MyDrive/Research/eng-to-{target_code}/target-tok')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"26ZCZiAmiLSw"},"outputs":[{"name":"stdout","output_type":"stream","text":["2024-11-06 05:52:48.925798: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-11-06 05:52:48.947114: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-11-06 05:52:48.953235: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-11-06 05:52:48.968888: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-11-06 05:52:50.019874: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-11-06 05:52:51 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n","2024-11-06 05:53:06 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': 'fairseq-standard-subword-tok-eng-to-nso', 'azureml_logging': False, 'seed': 2024, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 25, 'validate_interval_updates': 2000, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3600, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0003], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints-task-nmt', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 25, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 5, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project='fairseq-standard-subword-tok-eng-to-nso', azureml_logging=False, seed=2024, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, batch_size=None, required_batch_size_multiple=1, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=25, validate_interval_updates=2000, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid='3600', batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=100, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0003], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='checkpoints-task-nmt', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=25, save_interval_updates=2000, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='bleu', maximize_best_checkpoint_metric=True, patience=5, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='data-bin-nmt', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 0, \"max_len_b\": 100, \"len_pen\": 1}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe='sentencepiece', eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.999)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=1000, warmup_init_lr=-1, pad=1, eos=2, unk=3, activation_fn='relu', share_decoder_input_output_embed=True, share_all_embeddings=True, encoder_layers=3, encoder_attention_heads=4, encoder_embed_dim=256, encoder_ffn_embed_dim=1024, decoder_layers=3, decoder_attention_heads=4, decoder_embed_dim=256, decoder_ffn_embed_dim=1024, dropout=0.25, no_seed_provided=False, encoder_embed_path=None, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=256, decoder_input_dim=256, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': 'data-bin-nmt', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 0, \"max_len_b\": 100, \"len_pen\": 1}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': 'sentencepiece', 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0003]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 1000, 'warmup_init_lr': -1.0, 'lr': [0.0003]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n","2024-11-06 05:53:06 | INFO | fairseq.tasks.translation | [eng] dictionary: 3992 types\n","2024-11-06 05:53:06 | INFO | fairseq.tasks.translation | [nso] dictionary: 3992 types\n","2024-11-06 05:53:07 | INFO | fairseq_cli.train | TransformerModel(\n","  (encoder): TransformerEncoderBase(\n","    (dropout_module): FairseqDropout()\n","    (embed_tokens): Embedding(3992, 256, padding_idx=1)\n","    (embed_positions): SinusoidalPositionalEmbedding()\n","    (layers): ModuleList(\n","      (0-2): 3 x TransformerEncoderLayerBase(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (decoder): TransformerDecoderBase(\n","    (dropout_module): FairseqDropout()\n","    (embed_tokens): Embedding(3992, 256, padding_idx=1)\n","    (embed_positions): SinusoidalPositionalEmbedding()\n","    (layers): ModuleList(\n","      (0-2): 3 x TransformerDecoderLayerBase(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (output_projection): Linear(in_features=256, out_features=3992, bias=False)\n","  )\n",")\n","2024-11-06 05:53:07 | INFO | fairseq_cli.train | task: TranslationTask\n","2024-11-06 05:53:07 | INFO | fairseq_cli.train | model: TransformerModel\n","2024-11-06 05:53:07 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n","2024-11-06 05:53:07 | INFO | fairseq_cli.train | num. shared model params: 6,551,552 (num. trained: 6,551,552)\n","2024-11-06 05:53:07 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n","2024-11-06 05:53:08 | INFO | fairseq.data.data_utils | loaded 6,336 examples from: data-bin-nmt/valid.eng-nso.eng\n","2024-11-06 05:53:09 | INFO | fairseq.data.data_utils | loaded 6,336 examples from: data-bin-nmt/valid.eng-nso.nso\n","2024-11-06 05:53:09 | INFO | fairseq.tasks.translation | data-bin-nmt valid eng-nso 6336 examples\n","2024-11-06 05:53:09 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight \u003c- decoder.embed_tokens.weight\n","2024-11-06 05:53:09 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight \u003c- decoder.output_projection.weight\n","2024-11-06 05:53:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2024-11-06 05:53:09 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n","2024-11-06 05:53:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2024-11-06 05:53:09 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n","2024-11-06 05:53:09 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n","2024-11-06 05:53:09 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints-task-nmt/checkpoint_last.pt\n","2024-11-06 05:53:09 | INFO | fairseq.trainer | No existing checkpoint found checkpoints-task-nmt/checkpoint_last.pt\n","2024-11-06 05:53:09 | INFO | fairseq.trainer | loading train data for epoch 1\n","2024-11-06 05:53:10 | INFO | fairseq.data.data_utils | loaded 20,994 examples from: data-bin-nmt/train.eng-nso.eng\n","2024-11-06 05:53:10 | INFO | fairseq.data.data_utils | loaded 20,994 examples from: data-bin-nmt/train.eng-nso.nso\n","2024-11-06 05:53:10 | INFO | fairseq.tasks.translation | data-bin-nmt train eng-nso 20994 examples\n","2024-11-06 05:53:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 05:53:10 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2024-11-06 05:53:10 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2024-11-06 05:53:10 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2024-11-06 05:53:11 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n","2024-11-06 05:53:12 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n","2024-11-06 05:53:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 05:53:12 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n","2024-11-06 05:53:12 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n","2024-11-06 05:53:12 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n","2024-11-06 05:53:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 001:   0% 0/219 [00:00\u003c?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtyobeka-mandisa\u001b[0m (\u001b[33mtyobeka-mandisa-university-of-cape-town\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.5\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/Research/eng-to-nso/target-tok/wandb/run-20241106_055314-xau750io\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcheckpoints-task-nmt\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 猸锔 View project at \u001b[34m\u001b[4mhttps://wandb.ai/tyobeka-mandisa-university-of-cape-town/fairseq-standard-subword-tok-eng-to-nso\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/tyobeka-mandisa-university-of-cape-town/fairseq-standard-subword-tok-eng-to-nso/runs/xau750io\u001b[0m\n","2024-11-06 05:53:15 | INFO | fairseq.trainer | begin training epoch 1\n","2024-11-06 05:53:15 | INFO | fairseq_cli.train | Start iterating over samples\n","/content/drive/MyDrive/Research/eng-to-nso/fairseq/fairseq/tasks/fairseq_task.py:531: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","/content/drive/MyDrive/Research/eng-to-nso/fairseq/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n","  warnings.warn(\n","2024-11-06 05:53:34 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n","2024-11-06 05:53:34 | INFO | train | epoch 001 | loss 10.666 | nll_loss 10.431 | ppl 1380.35 | wps 40571.4 | ups 12.67 | wpb 3194.5 | bsz 95.9 | num_updates 219 | lr 6.57e-05 | gnorm 1.609 | train_wall 18 | gb_free 14.2 | wall 25\n","2024-11-06 05:53:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 05:53:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 002:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 05:53:34 | INFO | fairseq.trainer | begin training epoch 2\n","2024-11-06 05:53:34 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 05:53:53 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n","2024-11-06 05:53:53 | INFO | train | epoch 002 | loss 9.161 | nll_loss 8.68 | ppl 410.22 | wps 38317.1 | ups 11.99 | wpb 3194.5 | bsz 95.9 | num_updates 438 | lr 0.0001314 | gnorm 0.875 | train_wall 16 | gb_free 14.2 | wall 44\n","2024-11-06 05:53:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 05:53:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 003:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 05:53:53 | INFO | fairseq.trainer | begin training epoch 3\n","2024-11-06 05:53:53 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 05:54:11 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n","2024-11-06 05:54:11 | INFO | train | epoch 003 | loss 8.665 | nll_loss 8.084 | ppl 271.35 | wps 37111 | ups 11.62 | wpb 3194.5 | bsz 95.9 | num_updates 657 | lr 0.0001971 | gnorm 1.062 | train_wall 16 | gb_free 14.2 | wall 63\n","2024-11-06 05:54:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 05:54:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 004:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 05:54:12 | INFO | fairseq.trainer | begin training epoch 4\n","2024-11-06 05:54:12 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 05:54:31 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n","2024-11-06 05:54:31 | INFO | train | epoch 004 | loss 7.998 | nll_loss 7.305 | ppl 158.13 | wps 36706.5 | ups 11.49 | wpb 3194.5 | bsz 95.9 | num_updates 876 | lr 0.0002628 | gnorm 1.069 | train_wall 17 | gb_free 14.2 | wall 82\n","2024-11-06 05:54:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 05:54:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 005:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 05:54:31 | INFO | fairseq.trainer | begin training epoch 5\n","2024-11-06 05:54:31 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 05:54:50 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n","2024-11-06 05:54:50 | INFO | train | epoch 005 | loss 7.5 | nll_loss 6.719 | ppl 105.36 | wps 36626.9 | ups 11.47 | wpb 3194.5 | bsz 95.9 | num_updates 1095 | lr 0.000286691 | gnorm 0.973 | train_wall 17 | gb_free 14.1 | wall 101\n","2024-11-06 05:54:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 05:54:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 006:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 05:54:50 | INFO | fairseq.trainer | begin training epoch 6\n","2024-11-06 05:54:50 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 05:55:09 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n","2024-11-06 05:55:09 | INFO | train | epoch 006 | loss 7.171 | nll_loss 6.334 | ppl 80.66 | wps 36874.4 | ups 11.54 | wpb 3194.5 | bsz 95.9 | num_updates 1314 | lr 0.000261712 | gnorm 0.922 | train_wall 17 | gb_free 14.2 | wall 120\n","2024-11-06 05:55:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 05:55:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 007:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 05:55:09 | INFO | fairseq.trainer | begin training epoch 7\n","2024-11-06 05:55:09 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 05:55:27 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n","2024-11-06 05:55:27 | INFO | train | epoch 007 | loss 6.927 | nll_loss 6.048 | ppl 66.15 | wps 38728 | ups 12.12 | wpb 3194.5 | bsz 95.9 | num_updates 1533 | lr 0.000242298 | gnorm 0.886 | train_wall 17 | gb_free 14.1 | wall 138\n","2024-11-06 05:55:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 05:55:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 008:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 05:55:27 | INFO | fairseq.trainer | begin training epoch 8\n","2024-11-06 05:55:27 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 05:55:47 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n","2024-11-06 05:55:47 | INFO | train | epoch 008 | loss 6.743 | nll_loss 5.833 | ppl 57 | wps 35325.1 | ups 11.06 | wpb 3194.5 | bsz 95.9 | num_updates 1752 | lr 0.000226649 | gnorm 0.901 | train_wall 18 | gb_free 14.2 | wall 158\n","2024-11-06 05:55:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 05:55:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 009:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 05:55:47 | INFO | fairseq.trainer | begin training epoch 9\n","2024-11-06 05:55:47 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 05:56:07 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n","2024-11-06 05:56:07 | INFO | train | epoch 009 | loss 6.592 | nll_loss 5.657 | ppl 50.46 | wps 34606.6 | ups 10.83 | wpb 3194.5 | bsz 95.9 | num_updates 1971 | lr 0.000213687 | gnorm 0.901 | train_wall 18 | gb_free 14.1 | wall 178\n","2024-11-06 05:56:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 05:56:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 010:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 05:56:07 | INFO | fairseq.trainer | begin training epoch 10\n","2024-11-06 05:56:07 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 010:  12% 27/219 [00:02\u003c00:18, 10.14it/s]2024-11-06 05:56:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-11-06 05:56:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 010 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   1% 1/86 [00:00\u003c01:20,  1.05it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   2% 2/86 [00:01\u003c01:14,  1.13it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   3% 3/86 [00:02\u003c01:00,  1.37it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   5% 4/86 [00:02\u003c00:57,  1.43it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   6% 5/86 [00:03\u003c00:58,  1.37it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   7% 6/86 [00:04\u003c00:55,  1.44it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   8% 7/86 [00:05\u003c00:58,  1.35it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   9% 8/86 [00:05\u003c00:51,  1.51it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  10% 9/86 [00:06\u003c00:48,  1.58it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  12% 10/86 [00:07\u003c00:52,  1.45it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  13% 11/86 [00:07\u003c00:55,  1.36it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  14% 12/86 [00:08\u003c00:55,  1.32it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  15% 13/86 [00:09\u003c00:55,  1.31it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  16% 14/86 [00:10\u003c01:05,  1.11it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  17% 15/86 [00:11\u003c01:07,  1.05it/s]\u001b[A\n","epoch 010:  12% 27/219 [00:16\u003c00:18, 10.14it/s, loss=6.581, nll_loss=5.642, ppl=49.94, wps=35507.2, ups=10.98, wpb=3233.3, bsz=90.8, num_updates=2000, lr=0.000212132, gnorm=1.001, train_wall=8, gb_free=14.2, wall=181]\n","epoch 010 | valid on 'valid' subset:  20% 17/86 [00:14\u003c01:16,  1.11s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  21% 18/86 [00:15\u003c01:19,  1.17s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  22% 19/86 [00:16\u003c01:09,  1.04s/it]\u001b[A2024-11-06 05:56:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 05:56:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 05:56:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 010 | valid on 'valid' subset:  23% 20/86 [00:17\u003c01:07,  1.02s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  24% 21/86 [00:18\u003c01:02,  1.04it/s]\u001b[A2024-11-06 05:56:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 05:56:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 05:56:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 010 | valid on 'valid' subset:  26% 22/86 [00:19\u003c01:02,  1.03it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  27% 23/86 [00:20\u003c00:58,  1.08it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  28% 24/86 [00:20\u003c00:55,  1.13it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  29% 25/86 [00:21\u003c00:55,  1.10it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  30% 26/86 [00:22\u003c00:52,  1.14it/s]\u001b[A2024-11-06 05:56:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 05:56:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 05:56:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 010 | valid on 'valid' subset:  31% 27/86 [00:23\u003c00:54,  1.08it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  33% 28/86 [00:24\u003c00:53,  1.08it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  34% 29/86 [00:25\u003c00:51,  1.10it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  35% 30/86 [00:26\u003c00:58,  1.05s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  36% 31/86 [00:28\u003c01:00,  1.10s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  37% 32/86 [00:29\u003c01:01,  1.14s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  38% 33/86 [00:30\u003c01:05,  1.24s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  40% 34/86 [00:31\u003c01:00,  1.16s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  41% 35/86 [00:32\u003c00:56,  1.11s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  42% 36/86 [00:33\u003c00:52,  1.05s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  43% 37/86 [00:34\u003c00:50,  1.02s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  44% 38/86 [00:35\u003c00:47,  1.01it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  45% 39/86 [00:36\u003c00:44,  1.05it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  47% 40/86 [00:37\u003c00:43,  1.06it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  48% 41/86 [00:38\u003c00:41,  1.10it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  49% 42/86 [00:39\u003c00:41,  1.06it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  50% 43/86 [00:39\u003c00:39,  1.09it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  51% 44/86 [00:40\u003c00:39,  1.06it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  52% 45/86 [00:42\u003c00:41,  1.02s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  53% 46/86 [00:43\u003c00:44,  1.12s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  55% 47/86 [00:44\u003c00:46,  1.18s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  56% 48/86 [00:46\u003c00:48,  1.27s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  57% 49/86 [00:47\u003c00:46,  1.25s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  58% 50/86 [00:48\u003c00:42,  1.18s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  59% 51/86 [00:49\u003c00:38,  1.11s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  60% 52/86 [00:50\u003c00:36,  1.06s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  62% 53/86 [00:51\u003c00:32,  1.00it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  63% 54/86 [00:52\u003c00:31,  1.01it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  64% 55/86 [00:53\u003c00:29,  1.05it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  65% 56/86 [00:54\u003c00:29,  1.03it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  66% 57/86 [00:54\u003c00:26,  1.08it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  67% 58/86 [00:55\u003c00:26,  1.06it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  69% 59/86 [00:56\u003c00:25,  1.07it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  70% 60/86 [00:58\u003c00:26,  1.04s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  71% 61/86 [00:59\u003c00:26,  1.08s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  72% 62/86 [01:00\u003c00:27,  1.15s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  73% 63/86 [01:01\u003c00:27,  1.19s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  74% 64/86 [01:03\u003c00:27,  1.25s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  76% 65/86 [01:04\u003c00:24,  1.18s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  77% 66/86 [01:05\u003c00:21,  1.09s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  78% 67/86 [01:06\u003c00:19,  1.03s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  79% 68/86 [01:06\u003c00:17,  1.03it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  80% 69/86 [01:07\u003c00:16,  1.06it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  81% 70/86 [01:08\u003c00:15,  1.04it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  83% 71/86 [01:09\u003c00:13,  1.08it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  84% 72/86 [01:10\u003c00:12,  1.09it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  85% 73/86 [01:11\u003c00:11,  1.10it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  86% 74/86 [01:12\u003c00:10,  1.09it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  87% 75/86 [01:13\u003c00:09,  1.12it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  88% 76/86 [01:14\u003c00:09,  1.05it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  90% 77/86 [01:15\u003c00:09,  1.01s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  91% 78/86 [01:16\u003c00:08,  1.08s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  92% 79/86 [01:18\u003c00:07,  1.14s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  93% 80/86 [01:19\u003c00:07,  1.19s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  94% 81/86 [01:20\u003c00:05,  1.10s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  95% 82/86 [01:21\u003c00:04,  1.02s/it]\u001b[A\n","epoch 010 | valid on 'valid' subset:  97% 83/86 [01:21\u003c00:02,  1.03it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  98% 84/86 [01:22\u003c00:01,  1.06it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  99% 85/86 [01:23\u003c00:00,  1.09it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset: 100% 86/86 [01:24\u003c00:00,  1.14it/s]\u001b[A\n","                                                                        \u001b[A2024-11-06 05:57:34 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.32 | nll_loss 5.274 | ppl 38.7 | bleu 3.05 | wps 2445.5 | wpb 2386.4 | bsz 73.7 | num_updates 2000\n","2024-11-06 05:57:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 2000 updates\n","2024-11-06 05:57:34 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_10_2000.pt\n","2024-11-06 05:57:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_10_2000.pt\n","2024-11-06 05:57:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-task-nmt/checkpoint_10_2000.pt (epoch 10 @ 2000 updates, score 3.05) (writing took 1.096451997000031 seconds)\n","2024-11-06 05:57:53 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n","2024-11-06 05:57:53 | INFO | train | epoch 010 | loss 6.468 | nll_loss 5.51 | ppl 45.58 | wps 6590 | ups 2.06 | wpb 3194.5 | bsz 95.9 | num_updates 2190 | lr 0.000202721 | gnorm 0.928 | train_wall 19 | gb_free 14.1 | wall 284\n","2024-11-06 05:57:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 05:57:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 011:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 05:57:53 | INFO | fairseq.trainer | begin training epoch 11\n","2024-11-06 05:57:53 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 05:58:11 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n","2024-11-06 05:58:11 | INFO | train | epoch 011 | loss 6.356 | nll_loss 5.379 | ppl 41.61 | wps 37969.3 | ups 11.89 | wpb 3194.5 | bsz 95.9 | num_updates 2409 | lr 0.000193287 | gnorm 0.923 | train_wall 17 | gb_free 14.1 | wall 302\n","2024-11-06 05:58:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 05:58:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 012:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 05:58:11 | INFO | fairseq.trainer | begin training epoch 12\n","2024-11-06 05:58:11 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 05:58:30 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n","2024-11-06 05:58:30 | INFO | train | epoch 012 | loss 6.262 | nll_loss 5.269 | ppl 38.56 | wps 37102 | ups 11.61 | wpb 3194.5 | bsz 95.9 | num_updates 2628 | lr 0.000185058 | gnorm 0.95 | train_wall 17 | gb_free 14.1 | wall 321\n","2024-11-06 05:58:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 05:58:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 013:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 05:58:30 | INFO | fairseq.trainer | begin training epoch 13\n","2024-11-06 05:58:30 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 05:58:49 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n","2024-11-06 05:58:49 | INFO | train | epoch 013 | loss 6.165 | nll_loss 5.154 | ppl 35.62 | wps 36908.8 | ups 11.55 | wpb 3194.5 | bsz 95.9 | num_updates 2847 | lr 0.000177798 | gnorm 0.938 | train_wall 17 | gb_free 14.1 | wall 340\n","2024-11-06 05:58:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 05:58:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 014:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 05:58:49 | INFO | fairseq.trainer | begin training epoch 14\n","2024-11-06 05:58:49 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 05:59:08 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n","2024-11-06 05:59:08 | INFO | train | epoch 014 | loss 6.075 | nll_loss 5.05 | ppl 33.13 | wps 37389.9 | ups 11.7 | wpb 3194.5 | bsz 95.9 | num_updates 3066 | lr 0.000171331 | gnorm 0.937 | train_wall 17 | gb_free 14.1 | wall 359\n","2024-11-06 05:59:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 05:59:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 015:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 05:59:08 | INFO | fairseq.trainer | begin training epoch 15\n","2024-11-06 05:59:08 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 05:59:26 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n","2024-11-06 05:59:26 | INFO | train | epoch 015 | loss 6.002 | nll_loss 4.963 | ppl 31.19 | wps 38248.3 | ups 11.97 | wpb 3194.5 | bsz 95.9 | num_updates 3285 | lr 0.000165521 | gnorm 0.976 | train_wall 17 | gb_free 14 | wall 377\n","2024-11-06 05:59:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 05:59:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 016:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 05:59:26 | INFO | fairseq.trainer | begin training epoch 16\n","2024-11-06 05:59:26 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 05:59:45 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n","2024-11-06 05:59:45 | INFO | train | epoch 016 | loss 5.937 | nll_loss 4.886 | ppl 29.57 | wps 37387.6 | ups 11.7 | wpb 3194.5 | bsz 95.9 | num_updates 3504 | lr 0.000160265 | gnorm 0.981 | train_wall 17 | gb_free 14.1 | wall 396\n","2024-11-06 05:59:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 05:59:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 017:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 05:59:45 | INFO | fairseq.trainer | begin training epoch 17\n","2024-11-06 05:59:45 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:00:03 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n","2024-11-06 06:00:03 | INFO | train | epoch 017 | loss 5.866 | nll_loss 4.803 | ppl 27.91 | wps 37916.8 | ups 11.87 | wpb 3194.5 | bsz 95.9 | num_updates 3723 | lr 0.00015548 | gnorm 0.984 | train_wall 17 | gb_free 14.1 | wall 414\n","2024-11-06 06:00:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:00:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 018:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:00:03 | INFO | fairseq.trainer | begin training epoch 18\n","2024-11-06 06:00:03 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:00:22 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n","2024-11-06 06:00:22 | INFO | train | epoch 018 | loss 5.809 | nll_loss 4.737 | ppl 26.66 | wps 37236.9 | ups 11.66 | wpb 3194.5 | bsz 95.9 | num_updates 3942 | lr 0.000151099 | gnorm 0.986 | train_wall 17 | gb_free 14.1 | wall 433\n","2024-11-06 06:00:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:00:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 019:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:00:22 | INFO | fairseq.trainer | begin training epoch 19\n","2024-11-06 06:00:22 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 019:  26% 57/219 [00:05\u003c00:12, 13.03it/s]2024-11-06 06:00:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-11-06 06:00:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 019 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:   1% 1/86 [00:00\u003c00:46,  1.83it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:   2% 2/86 [00:01\u003c00:44,  1.90it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:   3% 3/86 [00:02\u003c01:00,  1.37it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:   5% 4/86 [00:02\u003c01:05,  1.25it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:   6% 5/86 [00:03\u003c01:06,  1.22it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:   7% 6/86 [00:04\u003c01:07,  1.19it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:   8% 7/86 [00:05\u003c01:06,  1.19it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:   9% 8/86 [00:06\u003c01:05,  1.19it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  10% 9/86 [00:07\u003c01:10,  1.10it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  12% 10/86 [00:09\u003c01:28,  1.16s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  13% 11/86 [00:10\u003c01:32,  1.23s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  14% 12/86 [00:11\u003c01:32,  1.25s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  15% 13/86 [00:12\u003c01:26,  1.18s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  16% 14/86 [00:13\u003c01:21,  1.14s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  17% 15/86 [00:14\u003c01:13,  1.03s/it]\u001b[A\n","epoch 019:  26% 57/219 [00:21\u003c00:12, 13.03it/s, loss=5.778, nll_loss=4.7, ppl=25.99, wps=36113.9, ups=11.08, wpb=3259.3, bsz=99.5, num_updates=4000, lr=0.00015, gnorm=0.983, train_wall=8, gb_free=14.1, wall=439]\n","epoch 019 | valid on 'valid' subset:  20% 17/86 [00:16\u003c01:07,  1.03it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  21% 18/86 [00:17\u003c01:06,  1.02it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  22% 19/86 [00:18\u003c01:02,  1.07it/s]\u001b[A2024-11-06 06:00:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:00:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:00:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 019 | valid on 'valid' subset:  23% 20/86 [00:19\u003c01:03,  1.03it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  24% 21/86 [00:20\u003c01:00,  1.07it/s]\u001b[A2024-11-06 06:00:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:00:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:00:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 019 | valid on 'valid' subset:  26% 22/86 [00:21\u003c01:00,  1.06it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  27% 23/86 [00:22\u003c00:58,  1.08it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  28% 24/86 [00:23\u003c00:59,  1.05it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  29% 25/86 [00:24\u003c01:06,  1.09s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  30% 26/86 [00:25\u003c01:06,  1.11s/it]\u001b[A2024-11-06 06:00:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:00:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:00:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 019 | valid on 'valid' subset:  31% 27/86 [00:27\u003c01:13,  1.25s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  33% 28/86 [00:28\u003c01:15,  1.30s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  34% 29/86 [00:30\u003c01:16,  1.34s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  35% 30/86 [00:31\u003c01:19,  1.42s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  36% 31/86 [00:32\u003c01:14,  1.36s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  37% 32/86 [00:33\u003c01:04,  1.19s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  38% 33/86 [00:34\u003c00:59,  1.12s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  40% 34/86 [00:35\u003c00:53,  1.03s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  41% 35/86 [00:36\u003c00:52,  1.03s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  42% 36/86 [00:37\u003c00:49,  1.00it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  43% 37/86 [00:38\u003c00:48,  1.00it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  44% 38/86 [00:39\u003c00:45,  1.05it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  45% 39/86 [00:40\u003c00:43,  1.09it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  47% 40/86 [00:41\u003c00:42,  1.08it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  48% 41/86 [00:41\u003c00:40,  1.12it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  49% 42/86 [00:43\u003c00:44,  1.02s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  50% 43/86 [00:44\u003c00:46,  1.07s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  51% 44/86 [00:45\u003c00:49,  1.17s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  52% 45/86 [00:47\u003c00:49,  1.21s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  53% 46/86 [00:48\u003c00:45,  1.14s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  55% 47/86 [00:48\u003c00:41,  1.06s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  56% 48/86 [00:50\u003c00:40,  1.05s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  57% 49/86 [00:50\u003c00:36,  1.02it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  58% 50/86 [00:51\u003c00:34,  1.03it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  59% 51/86 [00:52\u003c00:33,  1.05it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  60% 52/86 [00:53\u003c00:32,  1.06it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  62% 53/86 [00:54\u003c00:30,  1.10it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  63% 54/86 [00:55\u003c00:29,  1.08it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  64% 55/86 [00:56\u003c00:28,  1.10it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  65% 56/86 [00:57\u003c00:28,  1.04it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  66% 57/86 [00:58\u003c00:29,  1.00s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  67% 58/86 [00:59\u003c00:30,  1.10s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  69% 59/86 [01:01\u003c00:31,  1.17s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  70% 60/86 [01:02\u003c00:32,  1.24s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  71% 61/86 [01:03\u003c00:28,  1.15s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  72% 62/86 [01:04\u003c00:25,  1.07s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  73% 63/86 [01:05\u003c00:23,  1.01s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  74% 64/86 [01:06\u003c00:21,  1.01it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  76% 65/86 [01:07\u003c00:20,  1.03it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  77% 66/86 [01:07\u003c00:18,  1.06it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  78% 67/86 [01:08\u003c00:17,  1.09it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  79% 68/86 [01:09\u003c00:16,  1.12it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  80% 69/86 [01:10\u003c00:15,  1.12it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  81% 70/86 [01:11\u003c00:14,  1.11it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  83% 71/86 [01:12\u003c00:13,  1.13it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  84% 72/86 [01:13\u003c00:13,  1.07it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  85% 73/86 [01:14\u003c00:13,  1.01s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  86% 74/86 [01:15\u003c00:12,  1.07s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  87% 75/86 [01:17\u003c00:12,  1.16s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  88% 76/86 [01:18\u003c00:11,  1.20s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  90% 77/86 [01:19\u003c00:11,  1.24s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  91% 78/86 [01:21\u003c00:10,  1.28s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  92% 79/86 [01:22\u003c00:09,  1.30s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  93% 80/86 [01:23\u003c00:07,  1.30s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  94% 81/86 [01:24\u003c00:06,  1.26s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  95% 82/86 [01:25\u003c00:04,  1.14s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  97% 83/86 [01:26\u003c00:03,  1.05s/it]\u001b[A\n","epoch 019 | valid on 'valid' subset:  98% 84/86 [01:27\u003c00:01,  1.00it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset:  99% 85/86 [01:28\u003c00:00,  1.07it/s]\u001b[A\n","epoch 019 | valid on 'valid' subset: 100% 86/86 [01:29\u003c00:00,  1.14it/s]\u001b[A\n","                                                                        \u001b[A2024-11-06 06:01:56 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 5.662 | nll_loss 4.466 | ppl 22.09 | bleu 7.37 | wps 2306.6 | wpb 2386.4 | bsz 73.7 | num_updates 4000 | best_bleu 7.37\n","2024-11-06 06:01:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 4000 updates\n","2024-11-06 06:01:56 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_19_4000.pt\n","2024-11-06 06:01:57 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_19_4000.pt\n","2024-11-06 06:01:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-task-nmt/checkpoint_19_4000.pt (epoch 19 @ 4000 updates, score 7.37) (writing took 1.6216865330000019 seconds)\n","2024-11-06 06:02:14 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n","2024-11-06 06:02:14 | INFO | train | epoch 019 | loss 5.756 | nll_loss 4.674 | ppl 25.52 | wps 6276 | ups 1.96 | wpb 3194.5 | bsz 95.9 | num_updates 4161 | lr 0.000147069 | gnorm 1.02 | train_wall 19 | gb_free 14.2 | wall 545\n","2024-11-06 06:02:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:02:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 020:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:02:14 | INFO | fairseq.trainer | begin training epoch 20\n","2024-11-06 06:02:14 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:02:32 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n","2024-11-06 06:02:32 | INFO | train | epoch 020 | loss 5.704 | nll_loss 4.612 | ppl 24.45 | wps 37249.3 | ups 11.66 | wpb 3194.5 | bsz 95.9 | num_updates 4380 | lr 0.000143346 | gnorm 1.013 | train_wall 17 | gb_free 14.2 | wall 564\n","2024-11-06 06:02:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:02:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 021:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:02:33 | INFO | fairseq.trainer | begin training epoch 21\n","2024-11-06 06:02:33 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:02:51 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n","2024-11-06 06:02:51 | INFO | train | epoch 021 | loss 5.658 | nll_loss 4.557 | ppl 23.54 | wps 36945.4 | ups 11.57 | wpb 3194.5 | bsz 95.9 | num_updates 4599 | lr 0.000139891 | gnorm 1.026 | train_wall 17 | gb_free 14.1 | wall 582\n","2024-11-06 06:02:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:02:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 022:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:02:51 | INFO | fairseq.trainer | begin training epoch 22\n","2024-11-06 06:02:52 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:03:10 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n","2024-11-06 06:03:10 | INFO | train | epoch 022 | loss 5.611 | nll_loss 4.503 | ppl 22.67 | wps 37531.9 | ups 11.75 | wpb 3194.5 | bsz 95.9 | num_updates 4818 | lr 0.000136675 | gnorm 1.023 | train_wall 17 | gb_free 14.1 | wall 601\n","2024-11-06 06:03:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:03:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 023:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:03:10 | INFO | fairseq.trainer | begin training epoch 23\n","2024-11-06 06:03:10 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:03:29 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n","2024-11-06 06:03:29 | INFO | train | epoch 023 | loss 5.57 | nll_loss 4.453 | ppl 21.9 | wps 37724 | ups 11.81 | wpb 3194.5 | bsz 95.9 | num_updates 5037 | lr 0.00013367 | gnorm 1.057 | train_wall 17 | gb_free 14.1 | wall 620\n","2024-11-06 06:03:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:03:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 024:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:03:29 | INFO | fairseq.trainer | begin training epoch 24\n","2024-11-06 06:03:29 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:03:47 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n","2024-11-06 06:03:47 | INFO | train | epoch 024 | loss 5.532 | nll_loss 4.408 | ppl 21.23 | wps 37874.1 | ups 11.86 | wpb 3194.5 | bsz 95.9 | num_updates 5256 | lr 0.000130856 | gnorm 1.045 | train_wall 17 | gb_free 14 | wall 638\n","2024-11-06 06:03:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:03:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 025:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:03:47 | INFO | fairseq.trainer | begin training epoch 25\n","2024-11-06 06:03:47 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 025:  99% 217/219 [00:18\u003c00:00, 12.36it/s, loss=5.51, nll_loss=4.383, ppl=20.87, wps=37047.6, ups=11.72, wpb=3159.8, bsz=94.8, num_updates=5400, lr=0.000129099, gnorm=1.084, train_wall=8, gb_free=14.1, wall=651]2024-11-06 06:04:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-11-06 06:04:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 025 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:   1% 1/86 [00:00\u003c00:58,  1.45it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:   2% 2/86 [00:01\u003c00:54,  1.54it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:   3% 3/86 [00:01\u003c00:49,  1.66it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:   5% 4/86 [00:02\u003c00:47,  1.71it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:   6% 5/86 [00:02\u003c00:46,  1.73it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:   7% 6/86 [00:03\u003c00:46,  1.72it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:   8% 7/86 [00:04\u003c00:53,  1.48it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:   9% 8/86 [00:05\u003c00:50,  1.55it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  10% 9/86 [00:05\u003c00:48,  1.58it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  12% 10/86 [00:06\u003c00:42,  1.79it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  13% 11/86 [00:06\u003c00:43,  1.73it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  14% 12/86 [00:07\u003c00:39,  1.88it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  15% 13/86 [00:07\u003c00:38,  1.91it/s]\u001b[A2024-11-06 06:04:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:04:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:04:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 025 | valid on 'valid' subset:  16% 14/86 [00:08\u003c00:38,  1.88it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  17% 15/86 [00:08\u003c00:34,  2.04it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  19% 16/86 [00:09\u003c00:36,  1.92it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  20% 17/86 [00:09\u003c00:34,  2.02it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  21% 18/86 [00:10\u003c00:36,  1.87it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  22% 19/86 [00:10\u003c00:33,  1.98it/s]\u001b[A2024-11-06 06:04:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:04:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:04:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 025 | valid on 'valid' subset:  23% 20/86 [00:11\u003c00:38,  1.72it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  24% 21/86 [00:11\u003c00:35,  1.83it/s]\u001b[A2024-11-06 06:04:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:04:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:04:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 025 | valid on 'valid' subset:  26% 22/86 [00:12\u003c00:38,  1.66it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  27% 23/86 [00:13\u003c00:36,  1.70it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  28% 24/86 [00:13\u003c00:33,  1.83it/s]\u001b[A2024-11-06 06:04:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:04:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:04:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 025 | valid on 'valid' subset:  29% 25/86 [00:14\u003c00:34,  1.79it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  30% 26/86 [00:14\u003c00:35,  1.70it/s]\u001b[A2024-11-06 06:04:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:04:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:04:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 025 | valid on 'valid' subset:  31% 27/86 [00:15\u003c00:36,  1.62it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  33% 28/86 [00:16\u003c00:38,  1.50it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  34% 29/86 [00:16\u003c00:38,  1.48it/s]\u001b[A\n","epoch 025:  99% 217/219 [00:36\u003c00:00, 12.36it/s, loss=5.51, nll_loss=4.383, ppl=20.87, wps=37047.6, ups=11.72, wpb=3159.8, bsz=94.8, num_updates=5400, lr=0.000129099, gnorm=1.084, train_wall=8, gb_free=14.1, wall=651]\n","epoch 025 | valid on 'valid' subset:  36% 31/86 [00:18\u003c00:40,  1.36it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  37% 32/86 [00:19\u003c00:37,  1.45it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  38% 33/86 [00:20\u003c00:44,  1.18it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  40% 34/86 [00:21\u003c00:45,  1.15it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  41% 35/86 [00:22\u003c00:42,  1.20it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  42% 36/86 [00:22\u003c00:37,  1.33it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  43% 37/86 [00:23\u003c00:36,  1.33it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  44% 38/86 [00:24\u003c00:36,  1.33it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  45% 39/86 [00:24\u003c00:31,  1.48it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  47% 40/86 [00:25\u003c00:33,  1.38it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  48% 41/86 [00:25\u003c00:28,  1.56it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  49% 42/86 [00:26\u003c00:31,  1.40it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  50% 43/86 [00:27\u003c00:28,  1.54it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  51% 44/86 [00:28\u003c00:30,  1.39it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  52% 45/86 [00:28\u003c00:26,  1.53it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  53% 46/86 [00:29\u003c00:29,  1.38it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  55% 47/86 [00:30\u003c00:29,  1.33it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  56% 48/86 [00:31\u003c00:28,  1.35it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  57% 49/86 [00:31\u003c00:27,  1.33it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  58% 50/86 [00:32\u003c00:28,  1.28it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  59% 51/86 [00:33\u003c00:28,  1.25it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  60% 52/86 [00:34\u003c00:26,  1.27it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  62% 53/86 [00:35\u003c00:29,  1.11it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  63% 54/86 [00:36\u003c00:30,  1.04it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  64% 55/86 [00:37\u003c00:29,  1.04it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  65% 56/86 [00:38\u003c00:25,  1.18it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  66% 57/86 [00:38\u003c00:21,  1.32it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  67% 58/86 [00:39\u003c00:22,  1.26it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  69% 59/86 [00:40\u003c00:21,  1.25it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  70% 60/86 [00:41\u003c00:21,  1.21it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  71% 61/86 [00:42\u003c00:20,  1.21it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  72% 62/86 [00:42\u003c00:18,  1.26it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  73% 63/86 [00:43\u003c00:18,  1.24it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  74% 64/86 [00:44\u003c00:17,  1.29it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  76% 65/86 [00:45\u003c00:16,  1.25it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  77% 66/86 [00:46\u003c00:16,  1.23it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  78% 67/86 [00:46\u003c00:15,  1.24it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  79% 68/86 [00:47\u003c00:14,  1.22it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  80% 69/86 [00:48\u003c00:13,  1.22it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  81% 70/86 [00:49\u003c00:14,  1.08it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  83% 71/86 [00:50\u003c00:14,  1.06it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  84% 72/86 [00:51\u003c00:14,  1.02s/it]\u001b[A\n","epoch 025 | valid on 'valid' subset:  85% 73/86 [00:53\u003c00:14,  1.09s/it]\u001b[A\n","epoch 025 | valid on 'valid' subset:  86% 74/86 [00:54\u003c00:12,  1.06s/it]\u001b[A\n","epoch 025 | valid on 'valid' subset:  87% 75/86 [00:54\u003c00:10,  1.07it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  88% 76/86 [00:55\u003c00:08,  1.19it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  90% 77/86 [00:56\u003c00:07,  1.20it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  91% 78/86 [00:57\u003c00:06,  1.19it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  92% 79/86 [00:57\u003c00:05,  1.19it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  93% 80/86 [00:58\u003c00:05,  1.17it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  94% 81/86 [00:59\u003c00:04,  1.17it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  95% 82/86 [01:00\u003c00:03,  1.19it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  97% 83/86 [01:01\u003c00:02,  1.18it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  98% 84/86 [01:02\u003c00:01,  1.17it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset:  99% 85/86 [01:03\u003c00:00,  1.17it/s]\u001b[A\n","epoch 025 | valid on 'valid' subset: 100% 86/86 [01:03\u003c00:00,  1.12it/s]\u001b[A\n","                                                                        \u001b[A2024-11-06 06:05:10 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 5.413 | nll_loss 4.171 | ppl 18.01 | bleu 11.36 | wps 3225.6 | wpb 2386.4 | bsz 73.7 | num_updates 5475 | best_bleu 11.36\n","2024-11-06 06:05:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 5475 updates\n","2024-11-06 06:05:10 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_best.pt\n","2024-11-06 06:05:10 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_best.pt\n","2024-11-06 06:05:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-task-nmt/checkpoint_best.pt (epoch 25 @ 5475 updates, score 11.36) (writing took 1.0391356869999981 seconds)\n","2024-11-06 06:05:11 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n","2024-11-06 06:05:11 | INFO | train | epoch 025 | loss 5.497 | nll_loss 4.367 | ppl 20.64 | wps 8373.9 | ups 2.62 | wpb 3194.5 | bsz 95.9 | num_updates 5475 | lr 0.000128212 | gnorm 1.068 | train_wall 17 | gb_free 14.1 | wall 722\n","2024-11-06 06:05:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:05:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 026:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:05:11 | INFO | fairseq.trainer | begin training epoch 26\n","2024-11-06 06:05:11 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:05:31 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n","2024-11-06 06:05:31 | INFO | train | epoch 026 | loss 5.454 | nll_loss 4.317 | ppl 19.93 | wps 33970.5 | ups 10.63 | wpb 3194.5 | bsz 95.9 | num_updates 5694 | lr 0.000125722 | gnorm 1.062 | train_wall 18 | gb_free 14.1 | wall 742\n","2024-11-06 06:05:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:05:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 027:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:05:31 | INFO | fairseq.trainer | begin training epoch 27\n","2024-11-06 06:05:31 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:05:50 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n","2024-11-06 06:05:50 | INFO | train | epoch 027 | loss 5.425 | nll_loss 4.282 | ppl 19.45 | wps 37539.7 | ups 11.75 | wpb 3194.5 | bsz 95.9 | num_updates 5913 | lr 0.000123372 | gnorm 1.063 | train_wall 17 | gb_free 14.2 | wall 761\n","2024-11-06 06:05:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:05:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 028:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:05:50 | INFO | fairseq.trainer | begin training epoch 28\n","2024-11-06 06:05:50 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 028:  39% 86/219 [00:07\u003c00:12, 10.56it/s]2024-11-06 06:05:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-11-06 06:05:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 028 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:   1% 1/86 [00:00\u003c01:04,  1.31it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:   2% 2/86 [00:01\u003c00:57,  1.46it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:   3% 3/86 [00:02\u003c00:56,  1.47it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:   5% 4/86 [00:02\u003c00:53,  1.53it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:   6% 5/86 [00:03\u003c00:47,  1.72it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:   7% 6/86 [00:03\u003c00:43,  1.84it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:   8% 7/86 [00:04\u003c00:48,  1.63it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:   9% 8/86 [00:05\u003c00:55,  1.41it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  10% 9/86 [00:05\u003c00:50,  1.53it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  12% 10/86 [00:06\u003c00:53,  1.42it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  13% 11/86 [00:07\u003c00:50,  1.47it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  14% 12/86 [00:08\u003c00:53,  1.39it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  15% 13/86 [00:08\u003c00:46,  1.56it/s]\u001b[A2024-11-06 06:06:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:06:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:06:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 028 | valid on 'valid' subset:  16% 14/86 [00:09\u003c00:46,  1.54it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  17% 15/86 [00:09\u003c00:39,  1.81it/s]\u001b[A2024-11-06 06:06:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:06:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:06:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 028 | valid on 'valid' subset:  19% 16/86 [00:10\u003c00:41,  1.69it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  20% 17/86 [00:10\u003c00:37,  1.84it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  21% 18/86 [00:11\u003c00:38,  1.79it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  22% 19/86 [00:11\u003c00:34,  1.94it/s]\u001b[A2024-11-06 06:06:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:06:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:06:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 028 | valid on 'valid' subset:  23% 20/86 [00:12\u003c00:42,  1.57it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  24% 21/86 [00:13\u003c00:37,  1.73it/s]\u001b[A2024-11-06 06:06:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:06:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:06:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 028 | valid on 'valid' subset:  26% 22/86 [00:13\u003c00:38,  1.67it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  27% 23/86 [00:14\u003c00:36,  1.72it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  28% 24/86 [00:14\u003c00:32,  1.90it/s]\u001b[A2024-11-06 06:06:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:06:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:06:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 028 | valid on 'valid' subset:  29% 25/86 [00:15\u003c00:33,  1.80it/s]\u001b[A\n","epoch 028:  39% 86/219 [00:23\u003c00:12, 10.56it/s, loss=5.387, nll_loss=4.238, ppl=18.87, wps=36938.7, ups=11.34, wpb=3258.5, bsz=97.6, num_updates=6000, lr=0.000122474, gnorm=1.114, train_wall=8, gb_free=14.1, wall=769]2024-11-06 06:06:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:06:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:06:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 028 | valid on 'valid' subset:  31% 27/86 [00:16\u003c00:32,  1.80it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  33% 28/86 [00:17\u003c00:35,  1.62it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  34% 29/86 [00:17\u003c00:39,  1.45it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  35% 30/86 [00:18\u003c00:41,  1.36it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  36% 31/86 [00:19\u003c00:45,  1.21it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  37% 32/86 [00:20\u003c00:40,  1.33it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  38% 33/86 [00:21\u003c00:43,  1.21it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  40% 34/86 [00:22\u003c00:45,  1.15it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  41% 35/86 [00:23\u003c00:42,  1.21it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  42% 36/86 [00:23\u003c00:36,  1.37it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  43% 37/86 [00:24\u003c00:39,  1.26it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  44% 38/86 [00:25\u003c00:33,  1.42it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  45% 39/86 [00:25\u003c00:30,  1.56it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  47% 40/86 [00:26\u003c00:32,  1.40it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  48% 41/86 [00:26\u003c00:30,  1.49it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  49% 42/86 [00:27\u003c00:33,  1.33it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  50% 43/86 [00:28\u003c00:29,  1.46it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  51% 44/86 [00:29\u003c00:27,  1.52it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  52% 45/86 [00:29\u003c00:25,  1.62it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  53% 46/86 [00:30\u003c00:24,  1.66it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  55% 47/86 [00:30\u003c00:22,  1.71it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  56% 48/86 [00:31\u003c00:22,  1.68it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  57% 49/86 [00:31\u003c00:21,  1.71it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  58% 50/86 [00:32\u003c00:21,  1.68it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  59% 51/86 [00:33\u003c00:21,  1.60it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  60% 52/86 [00:34\u003c00:24,  1.38it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  62% 53/86 [00:35\u003c00:27,  1.20it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  63% 54/86 [00:36\u003c00:27,  1.17it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  64% 55/86 [00:37\u003c00:27,  1.14it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  65% 56/86 [00:37\u003c00:26,  1.14it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  66% 57/86 [00:38\u003c00:26,  1.10it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  67% 58/86 [00:39\u003c00:25,  1.10it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  69% 59/86 [00:40\u003c00:23,  1.13it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  70% 60/86 [00:41\u003c00:22,  1.17it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  71% 61/86 [00:42\u003c00:21,  1.19it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  72% 62/86 [00:42\u003c00:18,  1.29it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  73% 63/86 [00:43\u003c00:16,  1.42it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  74% 64/86 [00:44\u003c00:14,  1.48it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  76% 65/86 [00:44\u003c00:13,  1.51it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  77% 66/86 [00:45\u003c00:14,  1.41it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  78% 67/86 [00:46\u003c00:12,  1.50it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  79% 68/86 [00:46\u003c00:11,  1.54it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  80% 69/86 [00:47\u003c00:11,  1.54it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  81% 70/86 [00:48\u003c00:11,  1.42it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  83% 71/86 [00:48\u003c00:10,  1.46it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  84% 72/86 [00:49\u003c00:10,  1.34it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  85% 73/86 [00:50\u003c00:11,  1.14it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  86% 74/86 [00:52\u003c00:11,  1.01it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  87% 75/86 [00:52\u003c00:10,  1.05it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  88% 76/86 [00:53\u003c00:09,  1.03it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  90% 77/86 [00:55\u003c00:09,  1.01s/it]\u001b[A\n","epoch 028 | valid on 'valid' subset:  91% 78/86 [00:55\u003c00:07,  1.03it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  92% 79/86 [00:56\u003c00:06,  1.07it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  93% 80/86 [00:57\u003c00:05,  1.11it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  94% 81/86 [00:58\u003c00:04,  1.13it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  95% 82/86 [00:59\u003c00:03,  1.17it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  97% 83/86 [01:00\u003c00:02,  1.16it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  98% 84/86 [01:00\u003c00:01,  1.20it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset:  99% 85/86 [01:01\u003c00:00,  1.20it/s]\u001b[A\n","epoch 028 | valid on 'valid' subset: 100% 86/86 [01:02\u003c00:00,  1.23it/s]\u001b[A\n","                                                                        \u001b[A2024-11-06 06:07:00 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 5.375 | nll_loss 4.116 | ppl 17.34 | bleu 11.92 | wps 3307.6 | wpb 2386.4 | bsz 73.7 | num_updates 6000 | best_bleu 11.92\n","2024-11-06 06:07:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 6000 updates\n","2024-11-06 06:07:00 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_28_6000.pt\n","2024-11-06 06:07:01 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_28_6000.pt\n","2024-11-06 06:07:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-task-nmt/checkpoint_28_6000.pt (epoch 28 @ 6000 updates, score 11.92) (writing took 1.1300629249999474 seconds)\n","2024-11-06 06:07:14 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)\n","2024-11-06 06:07:14 | INFO | train | epoch 028 | loss 5.397 | nll_loss 4.25 | ppl 19.02 | wps 8294.5 | ups 2.6 | wpb 3194.5 | bsz 95.9 | num_updates 6132 | lr 0.000121149 | gnorm 1.101 | train_wall 19 | gb_free 14.1 | wall 845\n","2024-11-06 06:07:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:07:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 029:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:07:14 | INFO | fairseq.trainer | begin training epoch 29\n","2024-11-06 06:07:14 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:07:33 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)\n","2024-11-06 06:07:33 | INFO | train | epoch 029 | loss 5.367 | nll_loss 4.214 | ppl 18.56 | wps 37764.7 | ups 11.82 | wpb 3194.5 | bsz 95.9 | num_updates 6351 | lr 0.000119042 | gnorm 1.091 | train_wall 17 | gb_free 14.1 | wall 864\n","2024-11-06 06:07:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:07:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 030:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:07:33 | INFO | fairseq.trainer | begin training epoch 30\n","2024-11-06 06:07:33 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:07:51 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)\n","2024-11-06 06:07:51 | INFO | train | epoch 030 | loss 5.332 | nll_loss 4.173 | ppl 18.03 | wps 37686.3 | ups 11.8 | wpb 3194.5 | bsz 95.9 | num_updates 6570 | lr 0.000117041 | gnorm 1.081 | train_wall 17 | gb_free 14.1 | wall 882\n","2024-11-06 06:07:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:07:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 031:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:07:51 | INFO | fairseq.trainer | begin training epoch 31\n","2024-11-06 06:07:51 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:08:11 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)\n","2024-11-06 06:08:11 | INFO | train | epoch 031 | loss 5.305 | nll_loss 4.141 | ppl 17.64 | wps 35404.7 | ups 11.08 | wpb 3194.5 | bsz 95.9 | num_updates 6789 | lr 0.000115138 | gnorm 1.099 | train_wall 18 | gb_free 14.1 | wall 902\n","2024-11-06 06:08:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:08:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 032:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:08:11 | INFO | fairseq.trainer | begin training epoch 32\n","2024-11-06 06:08:11 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:08:29 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)\n","2024-11-06 06:08:29 | INFO | train | epoch 032 | loss 5.279 | nll_loss 4.11 | ppl 17.26 | wps 38099.8 | ups 11.93 | wpb 3194.5 | bsz 95.9 | num_updates 7008 | lr 0.000113325 | gnorm 1.102 | train_wall 17 | gb_free 14.1 | wall 921\n","2024-11-06 06:08:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:08:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 033:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:08:30 | INFO | fairseq.trainer | begin training epoch 33\n","2024-11-06 06:08:30 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:08:48 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)\n","2024-11-06 06:08:48 | INFO | train | epoch 033 | loss 5.25 | nll_loss 4.075 | ppl 16.86 | wps 37723.4 | ups 11.81 | wpb 3194.5 | bsz 95.9 | num_updates 7227 | lr 0.000111594 | gnorm 1.102 | train_wall 17 | gb_free 14 | wall 939\n","2024-11-06 06:08:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:08:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 034:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:08:48 | INFO | fairseq.trainer | begin training epoch 34\n","2024-11-06 06:08:48 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:09:06 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)\n","2024-11-06 06:09:06 | INFO | train | epoch 034 | loss 5.231 | nll_loss 4.053 | ppl 16.6 | wps 38254.5 | ups 11.97 | wpb 3194.5 | bsz 95.9 | num_updates 7446 | lr 0.000109941 | gnorm 1.126 | train_wall 17 | gb_free 14 | wall 957\n","2024-11-06 06:09:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:09:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 035:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:09:06 | INFO | fairseq.trainer | begin training epoch 35\n","2024-11-06 06:09:06 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:09:25 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)\n","2024-11-06 06:09:25 | INFO | train | epoch 035 | loss 5.203 | nll_loss 4.021 | ppl 16.23 | wps 37892.1 | ups 11.86 | wpb 3194.5 | bsz 95.9 | num_updates 7665 | lr 0.000108359 | gnorm 1.117 | train_wall 17 | gb_free 14.1 | wall 976\n","2024-11-06 06:09:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:09:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 036:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:09:25 | INFO | fairseq.trainer | begin training epoch 36\n","2024-11-06 06:09:25 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:09:43 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)\n","2024-11-06 06:09:43 | INFO | train | epoch 036 | loss 5.183 | nll_loss 3.997 | ppl 15.96 | wps 37437.6 | ups 11.72 | wpb 3194.5 | bsz 95.9 | num_updates 7884 | lr 0.000106843 | gnorm 1.144 | train_wall 17 | gb_free 14.2 | wall 995\n","2024-11-06 06:09:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:09:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 037:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:09:44 | INFO | fairseq.trainer | begin training epoch 37\n","2024-11-06 06:09:44 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 037:  53% 115/219 [00:09\u003c00:07, 13.02it/s, loss=5.137, nll_loss=3.944, ppl=15.4, wps=35209.2, ups=11.15, wpb=3156.4, bsz=101.7, num_updates=7900, lr=0.000106735, gnorm=1.155, train_wall=8, gb_free=14.1, wall=996]2024-11-06 06:09:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-11-06 06:09:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 037 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:   1% 1/86 [00:00\u003c00:46,  1.83it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:   2% 2/86 [00:00\u003c00:40,  2.06it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:   3% 3/86 [00:01\u003c00:38,  2.15it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:   5% 4/86 [00:01\u003c00:36,  2.22it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:   6% 5/86 [00:02\u003c00:37,  2.17it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:   7% 6/86 [00:02\u003c00:39,  2.04it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:   8% 7/86 [00:03\u003c00:46,  1.68it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:   9% 8/86 [00:04\u003c00:58,  1.34it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  10% 9/86 [00:05\u003c00:52,  1.46it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  12% 10/86 [00:06\u003c00:54,  1.39it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  13% 11/86 [00:07\u003c01:01,  1.21it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  14% 12/86 [00:07\u003c00:53,  1.39it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  15% 13/86 [00:08\u003c00:47,  1.55it/s]\u001b[A2024-11-06 06:10:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:10:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:10:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 037 | valid on 'valid' subset:  16% 14/86 [00:08\u003c00:44,  1.62it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  17% 15/86 [00:09\u003c00:38,  1.86it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  19% 16/86 [00:09\u003c00:39,  1.79it/s]\u001b[A\n","epoch 037:  53% 115/219 [00:20\u003c00:07, 13.02it/s, loss=5.144, nll_loss=3.952, ppl=15.48, wps=42070.4, ups=12.55, wpb=3351.4, bsz=100.9, num_updates=8000, lr=0.000106066, gnorm=1.134, train_wall=8, gb_free=14.1, wall=1004]\n","epoch 037 | valid on 'valid' subset:  21% 18/86 [00:10\u003c00:36,  1.88it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  22% 19/86 [00:11\u003c00:33,  2.00it/s]\u001b[A2024-11-06 06:10:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:10:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:10:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 037 | valid on 'valid' subset:  23% 20/86 [00:11\u003c00:39,  1.67it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  24% 21/86 [00:12\u003c00:38,  1.69it/s]\u001b[A2024-11-06 06:10:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:10:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:10:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 037 | valid on 'valid' subset:  26% 22/86 [00:13\u003c00:39,  1.64it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  27% 23/86 [00:13\u003c00:36,  1.73it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  28% 24/86 [00:14\u003c00:32,  1.88it/s]\u001b[A2024-11-06 06:10:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:10:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:10:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 037 | valid on 'valid' subset:  29% 25/86 [00:14\u003c00:34,  1.78it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  30% 26/86 [00:15\u003c00:32,  1.82it/s]\u001b[A2024-11-06 06:10:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:10:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:10:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 037 | valid on 'valid' subset:  31% 27/86 [00:15\u003c00:34,  1.71it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  33% 28/86 [00:16\u003c00:35,  1.63it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  34% 29/86 [00:17\u003c00:33,  1.72it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  35% 30/86 [00:17\u003c00:38,  1.47it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  36% 31/86 [00:18\u003c00:37,  1.47it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  37% 32/86 [00:19\u003c00:34,  1.57it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  38% 33/86 [00:20\u003c00:38,  1.39it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  40% 34/86 [00:21\u003c00:44,  1.17it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  41% 35/86 [00:22\u003c00:44,  1.15it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  42% 36/86 [00:22\u003c00:42,  1.17it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  43% 37/86 [00:23\u003c00:41,  1.19it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  44% 38/86 [00:24\u003c00:36,  1.32it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  45% 39/86 [00:25\u003c00:36,  1.30it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  47% 40/86 [00:25\u003c00:32,  1.40it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  48% 41/86 [00:26\u003c00:28,  1.56it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  49% 42/86 [00:27\u003c00:31,  1.39it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  50% 43/86 [00:27\u003c00:31,  1.35it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  51% 44/86 [00:28\u003c00:31,  1.35it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  52% 45/86 [00:29\u003c00:28,  1.44it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  53% 46/86 [00:30\u003c00:29,  1.37it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  55% 47/86 [00:30\u003c00:29,  1.33it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  56% 48/86 [00:31\u003c00:28,  1.34it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  57% 49/86 [00:32\u003c00:25,  1.46it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  58% 50/86 [00:32\u003c00:23,  1.50it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  59% 51/86 [00:33\u003c00:24,  1.44it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  60% 52/86 [00:34\u003c00:24,  1.39it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  62% 53/86 [00:35\u003c00:26,  1.24it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  63% 54/86 [00:36\u003c00:26,  1.21it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  64% 55/86 [00:37\u003c00:27,  1.12it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  65% 56/86 [00:38\u003c00:27,  1.10it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  66% 57/86 [00:39\u003c00:25,  1.12it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  67% 58/86 [00:39\u003c00:23,  1.21it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  69% 59/86 [00:40\u003c00:20,  1.34it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  70% 60/86 [00:40\u003c00:18,  1.38it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  71% 61/86 [00:41\u003c00:18,  1.33it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  72% 62/86 [00:42\u003c00:17,  1.39it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  73% 63/86 [00:42\u003c00:15,  1.46it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  74% 64/86 [00:43\u003c00:15,  1.45it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  76% 65/86 [00:44\u003c00:14,  1.47it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  77% 66/86 [00:45\u003c00:14,  1.37it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  78% 67/86 [00:45\u003c00:13,  1.43it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  79% 68/86 [00:46\u003c00:12,  1.41it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  80% 69/86 [00:47\u003c00:11,  1.50it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  81% 70/86 [00:47\u003c00:11,  1.39it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  83% 71/86 [00:48\u003c00:10,  1.47it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  84% 72/86 [00:49\u003c00:09,  1.48it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  85% 73/86 [00:50\u003c00:10,  1.26it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  86% 74/86 [00:51\u003c00:09,  1.22it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  87% 75/86 [00:52\u003c00:10,  1.09it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  88% 76/86 [00:53\u003c00:09,  1.10it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  90% 77/86 [00:54\u003c00:08,  1.02it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  91% 78/86 [00:55\u003c00:07,  1.07it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  92% 79/86 [00:56\u003c00:06,  1.11it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  93% 80/86 [00:56\u003c00:05,  1.17it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  94% 81/86 [00:57\u003c00:04,  1.19it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  95% 82/86 [00:58\u003c00:03,  1.25it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  97% 83/86 [00:59\u003c00:02,  1.23it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  98% 84/86 [00:59\u003c00:01,  1.21it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset:  99% 85/86 [01:00\u003c00:00,  1.20it/s]\u001b[A\n","epoch 037 | valid on 'valid' subset: 100% 86/86 [01:01\u003c00:00,  1.23it/s]\u001b[A\n","                                                                        \u001b[A2024-11-06 06:10:55 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 5.212 | nll_loss 3.908 | ppl 15.01 | bleu 13.8 | wps 3345.8 | wpb 2386.4 | bsz 73.7 | num_updates 8000 | best_bleu 13.8\n","2024-11-06 06:10:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 8000 updates\n","2024-11-06 06:10:55 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_37_8000.pt\n","2024-11-06 06:10:55 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_37_8000.pt\n","2024-11-06 06:10:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-task-nmt/checkpoint_37_8000.pt (epoch 37 @ 8000 updates, score 13.8) (writing took 1.1429223380000622 seconds)\n","2024-11-06 06:11:08 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)\n","2024-11-06 06:11:08 | INFO | train | epoch 037 | loss 5.161 | nll_loss 3.971 | ppl 15.68 | wps 8256 | ups 2.58 | wpb 3194.5 | bsz 95.9 | num_updates 8103 | lr 0.00010539 | gnorm 1.147 | train_wall 20 | gb_free 14.1 | wall 1079\n","2024-11-06 06:11:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:11:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 038:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:11:08 | INFO | fairseq.trainer | begin training epoch 38\n","2024-11-06 06:11:08 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:11:27 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)\n","2024-11-06 06:11:27 | INFO | train | epoch 038 | loss 5.134 | nll_loss 3.939 | ppl 15.34 | wps 37602.8 | ups 11.77 | wpb 3194.5 | bsz 95.9 | num_updates 8322 | lr 0.000103994 | gnorm 1.124 | train_wall 17 | gb_free 14.1 | wall 1098\n","2024-11-06 06:11:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:11:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 039:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:11:27 | INFO | fairseq.trainer | begin training epoch 39\n","2024-11-06 06:11:27 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:11:45 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)\n","2024-11-06 06:11:45 | INFO | train | epoch 039 | loss 5.117 | nll_loss 3.918 | ppl 15.12 | wps 37986 | ups 11.89 | wpb 3194.5 | bsz 95.9 | num_updates 8541 | lr 0.000102652 | gnorm 1.145 | train_wall 17 | gb_free 14.2 | wall 1116\n","2024-11-06 06:11:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:11:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 040:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:11:45 | INFO | fairseq.trainer | begin training epoch 40\n","2024-11-06 06:11:45 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:12:04 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)\n","2024-11-06 06:12:04 | INFO | train | epoch 040 | loss 5.092 | nll_loss 3.889 | ppl 14.82 | wps 38053.9 | ups 11.91 | wpb 3194.5 | bsz 95.9 | num_updates 8760 | lr 0.000101361 | gnorm 1.131 | train_wall 17 | gb_free 14 | wall 1135\n","2024-11-06 06:12:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:12:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 041:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:12:04 | INFO | fairseq.trainer | begin training epoch 41\n","2024-11-06 06:12:04 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:12:23 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)\n","2024-11-06 06:12:23 | INFO | train | epoch 041 | loss 5.074 | nll_loss 3.868 | ppl 14.61 | wps 36887.3 | ups 11.55 | wpb 3194.5 | bsz 95.9 | num_updates 8979 | lr 0.000100117 | gnorm 1.148 | train_wall 17 | gb_free 14.1 | wall 1154\n","2024-11-06 06:12:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:12:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 042:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:12:23 | INFO | fairseq.trainer | begin training epoch 42\n","2024-11-06 06:12:23 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:12:41 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)\n","2024-11-06 06:12:41 | INFO | train | epoch 042 | loss 5.059 | nll_loss 3.851 | ppl 14.43 | wps 37252.4 | ups 11.66 | wpb 3194.5 | bsz 95.9 | num_updates 9198 | lr 9.89178e-05 | gnorm 1.178 | train_wall 17 | gb_free 14.1 | wall 1172\n","2024-11-06 06:12:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:12:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 043:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:12:41 | INFO | fairseq.trainer | begin training epoch 43\n","2024-11-06 06:12:41 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:13:00 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)\n","2024-11-06 06:13:00 | INFO | train | epoch 043 | loss 5.039 | nll_loss 3.827 | ppl 14.19 | wps 38121.7 | ups 11.93 | wpb 3194.5 | bsz 95.9 | num_updates 9417 | lr 9.77609e-05 | gnorm 1.153 | train_wall 17 | gb_free 14.1 | wall 1191\n","2024-11-06 06:13:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:13:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 044:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:13:00 | INFO | fairseq.trainer | begin training epoch 44\n","2024-11-06 06:13:00 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:13:18 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)\n","2024-11-06 06:13:18 | INFO | train | epoch 044 | loss 5.022 | nll_loss 3.807 | ppl 13.99 | wps 38012.4 | ups 11.9 | wpb 3194.5 | bsz 95.9 | num_updates 9636 | lr 9.66435e-05 | gnorm 1.167 | train_wall 17 | gb_free 14 | wall 1209\n","2024-11-06 06:13:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:13:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 045:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:13:18 | INFO | fairseq.trainer | begin training epoch 45\n","2024-11-06 06:13:18 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:13:36 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)\n","2024-11-06 06:13:36 | INFO | train | epoch 045 | loss 5.001 | nll_loss 3.782 | ppl 13.76 | wps 38129.8 | ups 11.94 | wpb 3194.5 | bsz 95.9 | num_updates 9855 | lr 9.55637e-05 | gnorm 1.17 | train_wall 17 | gb_free 14.1 | wall 1228\n","2024-11-06 06:13:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:13:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 046:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:13:37 | INFO | fairseq.trainer | begin training epoch 46\n","2024-11-06 06:13:37 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 046:  65% 143/219 [00:12\u003c00:06, 12.47it/s, loss=5.048, nll_loss=3.834, ppl=14.26, wps=36059.1, ups=11.29, wpb=3193.9, bsz=87.5, num_updates=9900, lr=9.53463e-05, gnorm=1.151, train_wall=8, gb_free=14.1, wall=1232]2024-11-06 06:13:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-11-06 06:13:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 046 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:   1% 1/86 [00:00\u003c00:48,  1.77it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:   2% 2/86 [00:00\u003c00:38,  2.16it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:   3% 3/86 [00:01\u003c00:39,  2.10it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:   5% 4/86 [00:01\u003c00:36,  2.26it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:   6% 5/86 [00:02\u003c00:36,  2.21it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:   7% 6/86 [00:03\u003c00:53,  1.49it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:   8% 7/86 [00:03\u003c00:50,  1.56it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:   9% 8/86 [00:05\u003c01:00,  1.29it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  10% 9/86 [00:05\u003c00:54,  1.41it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  12% 10/86 [00:06\u003c00:51,  1.49it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  13% 11/86 [00:06\u003c00:51,  1.45it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  14% 12/86 [00:07\u003c00:48,  1.51it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  15% 13/86 [00:07\u003c00:43,  1.69it/s]\u001b[A2024-11-06 06:13:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:13:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:13:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 046 | valid on 'valid' subset:  16% 14/86 [00:08\u003c00:42,  1.69it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  17% 15/86 [00:08\u003c00:37,  1.91it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  19% 16/86 [00:09\u003c00:36,  1.89it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  20% 17/86 [00:09\u003c00:34,  2.00it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  21% 18/86 [00:10\u003c00:41,  1.63it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  22% 19/86 [00:11\u003c00:36,  1.84it/s]\u001b[A2024-11-06 06:14:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:14:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:14:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 046 | valid on 'valid' subset:  23% 20/86 [00:11\u003c00:38,  1.71it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  24% 21/86 [00:12\u003c00:35,  1.81it/s]\u001b[A2024-11-06 06:14:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:14:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:14:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 046 | valid on 'valid' subset:  26% 22/86 [00:12\u003c00:36,  1.76it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  27% 23/86 [00:13\u003c00:34,  1.84it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  28% 24/86 [00:13\u003c00:31,  1.96it/s]\u001b[A2024-11-06 06:14:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:14:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:14:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 046:  65% 143/219 [00:27\u003c00:06, 12.47it/s, loss=4.983, nll_loss=3.762, ppl=13.57, wps=40697.5, ups=12.77, wpb=3187.4, bsz=94.4, num_updates=10000, lr=9.48683e-05, gnorm=1.194, train_wall=7, gb_free=14.1, wall=1240]\n","epoch 046 | valid on 'valid' subset:  30% 26/86 [00:14\u003c00:31,  1.90it/s]\u001b[A2024-11-06 06:14:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:14:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:14:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 046 | valid on 'valid' subset:  31% 27/86 [00:15\u003c00:33,  1.78it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  33% 28/86 [00:16\u003c00:32,  1.76it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  34% 29/86 [00:16\u003c00:30,  1.88it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  35% 30/86 [00:17\u003c00:32,  1.74it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  36% 31/86 [00:17\u003c00:33,  1.63it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  37% 32/86 [00:18\u003c00:31,  1.69it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  38% 33/86 [00:19\u003c00:36,  1.47it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  40% 34/86 [00:20\u003c00:43,  1.19it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  41% 35/86 [00:21\u003c00:45,  1.13it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  42% 36/86 [00:22\u003c00:43,  1.16it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  43% 37/86 [00:23\u003c00:43,  1.13it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  44% 38/86 [00:24\u003c00:49,  1.02s/it]\u001b[A\n","epoch 046 | valid on 'valid' subset:  45% 39/86 [00:25\u003c00:43,  1.09it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  47% 40/86 [00:26\u003c00:44,  1.03it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  48% 41/86 [00:27\u003c00:39,  1.13it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  49% 42/86 [00:28\u003c00:44,  1.01s/it]\u001b[A\n","epoch 046 | valid on 'valid' subset:  50% 43/86 [00:29\u003c00:43,  1.02s/it]\u001b[A\n","epoch 046 | valid on 'valid' subset:  51% 44/86 [00:30\u003c00:39,  1.07it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  52% 45/86 [00:30\u003c00:34,  1.20it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  53% 46/86 [00:31\u003c00:33,  1.20it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  55% 47/86 [00:32\u003c00:32,  1.21it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  56% 48/86 [00:33\u003c00:29,  1.30it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  57% 49/86 [00:33\u003c00:25,  1.43it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  58% 50/86 [00:34\u003c00:24,  1.45it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  59% 51/86 [00:35\u003c00:24,  1.45it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  60% 52/86 [00:35\u003c00:23,  1.45it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  62% 53/86 [00:36\u003c00:23,  1.39it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  63% 54/86 [00:37\u003c00:24,  1.30it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  64% 55/86 [00:38\u003c00:24,  1.28it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  65% 56/86 [00:39\u003c00:25,  1.17it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  66% 57/86 [00:40\u003c00:24,  1.20it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  67% 58/86 [00:40\u003c00:24,  1.15it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  69% 59/86 [00:41\u003c00:23,  1.17it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  70% 60/86 [00:42\u003c00:21,  1.20it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  71% 61/86 [00:43\u003c00:20,  1.21it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  72% 62/86 [00:44\u003c00:19,  1.25it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  73% 63/86 [00:44\u003c00:16,  1.36it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  74% 64/86 [00:45\u003c00:15,  1.40it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  76% 65/86 [00:46\u003c00:14,  1.42it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  77% 66/86 [00:46\u003c00:14,  1.37it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  78% 67/86 [00:47\u003c00:13,  1.43it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  79% 68/86 [00:48\u003c00:12,  1.48it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  80% 69/86 [00:48\u003c00:11,  1.53it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  81% 70/86 [00:49\u003c00:11,  1.39it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  83% 71/86 [00:50\u003c00:10,  1.40it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  84% 72/86 [00:50\u003c00:09,  1.42it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  85% 73/86 [00:51\u003c00:09,  1.34it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  86% 74/86 [00:52\u003c00:09,  1.24it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  87% 75/86 [00:53\u003c00:09,  1.10it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  88% 76/86 [00:54\u003c00:08,  1.13it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  90% 77/86 [00:55\u003c00:08,  1.07it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  91% 78/86 [00:56\u003c00:07,  1.04it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  92% 79/86 [00:57\u003c00:06,  1.02it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  93% 80/86 [00:58\u003c00:05,  1.06it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  94% 81/86 [00:59\u003c00:04,  1.11it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  95% 82/86 [01:00\u003c00:03,  1.15it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  97% 83/86 [01:01\u003c00:02,  1.17it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  98% 84/86 [01:01\u003c00:01,  1.17it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset:  99% 85/86 [01:02\u003c00:00,  1.18it/s]\u001b[A\n","epoch 046 | valid on 'valid' subset: 100% 86/86 [01:03\u003c00:00,  1.22it/s]\u001b[A\n","                                                                        \u001b[A2024-11-06 06:14:52 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 5.107 | nll_loss 3.77 | ppl 13.65 | bleu 15.37 | wps 3242.9 | wpb 2386.4 | bsz 73.7 | num_updates 10000 | best_bleu 15.37\n","2024-11-06 06:14:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 10000 updates\n","2024-11-06 06:14:52 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_46_10000.pt\n","2024-11-06 06:14:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_46_10000.pt\n","2024-11-06 06:14:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-task-nmt/checkpoint_46_10000.pt (epoch 46 @ 10000 updates, score 15.37) (writing took 1.1348657860003186 seconds)\n","2024-11-06 06:15:00 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)\n","2024-11-06 06:15:00 | INFO | train | epoch 046 | loss 4.983 | nll_loss 3.761 | ppl 13.56 | wps 8362 | ups 2.62 | wpb 3194.5 | bsz 95.9 | num_updates 10074 | lr 9.45193e-05 | gnorm 1.165 | train_wall 17 | gb_free 14.1 | wall 1311\n","2024-11-06 06:15:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:15:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 047:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:15:00 | INFO | fairseq.trainer | begin training epoch 47\n","2024-11-06 06:15:00 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:15:20 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)\n","2024-11-06 06:15:20 | INFO | train | epoch 047 | loss 4.97 | nll_loss 3.745 | ppl 13.41 | wps 35167 | ups 11.01 | wpb 3194.5 | bsz 95.9 | num_updates 10293 | lr 9.35083e-05 | gnorm 1.182 | train_wall 18 | gb_free 14.1 | wall 1331\n","2024-11-06 06:15:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:15:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 048:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:15:20 | INFO | fairseq.trainer | begin training epoch 48\n","2024-11-06 06:15:20 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:15:38 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)\n","2024-11-06 06:15:38 | INFO | train | epoch 048 | loss 4.955 | nll_loss 3.728 | ppl 13.25 | wps 38346.6 | ups 12 | wpb 3194.5 | bsz 95.9 | num_updates 10512 | lr 9.25292e-05 | gnorm 1.192 | train_wall 17 | gb_free 14.1 | wall 1349\n","2024-11-06 06:15:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:15:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 049:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:15:38 | INFO | fairseq.trainer | begin training epoch 49\n","2024-11-06 06:15:38 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:15:57 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)\n","2024-11-06 06:15:57 | INFO | train | epoch 049 | loss 4.932 | nll_loss 3.701 | ppl 13.01 | wps 37926.6 | ups 11.87 | wpb 3194.5 | bsz 95.9 | num_updates 10731 | lr 9.15801e-05 | gnorm 1.178 | train_wall 17 | gb_free 14.1 | wall 1368\n","2024-11-06 06:15:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:15:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 050:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:15:57 | INFO | fairseq.trainer | begin training epoch 50\n","2024-11-06 06:15:57 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 050: 100% 218/219 [00:19\u003c00:00, 12.17it/s, loss=4.956, nll_loss=3.727, ppl=13.24, wps=37694, ups=11.53, wpb=3267.9, bsz=96, num_updates=10900, lr=9.08674e-05, gnorm=1.181, train_wall=8, gb_free=14, wall=1384]2024-11-06 06:16:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-11-06 06:16:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 050 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:   1% 1/86 [00:00\u003c00:59,  1.43it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:   2% 2/86 [00:01\u003c00:51,  1.63it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:   3% 3/86 [00:01\u003c00:51,  1.62it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:   5% 4/86 [00:02\u003c00:46,  1.76it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:   6% 5/86 [00:02\u003c00:45,  1.77it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:   7% 6/86 [00:03\u003c00:46,  1.74it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:   8% 7/86 [00:04\u003c00:45,  1.75it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:   9% 8/86 [00:04\u003c00:43,  1.79it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  10% 9/86 [00:04\u003c00:37,  2.05it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  12% 10/86 [00:05\u003c00:34,  2.23it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  13% 11/86 [00:05\u003c00:34,  2.17it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  14% 12/86 [00:06\u003c00:33,  2.18it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  15% 13/86 [00:06\u003c00:33,  2.17it/s]\u001b[A2024-11-06 06:16:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:16:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:16:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 050 | valid on 'valid' subset:  16% 14/86 [00:07\u003c00:33,  2.12it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  17% 15/86 [00:07\u003c00:31,  2.23it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  19% 16/86 [00:08\u003c00:33,  2.10it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  20% 17/86 [00:08\u003c00:31,  2.16it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  21% 18/86 [00:09\u003c00:33,  2.06it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  22% 19/86 [00:09\u003c00:30,  2.20it/s]\u001b[A2024-11-06 06:16:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:16:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:16:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 050 | valid on 'valid' subset:  23% 20/86 [00:10\u003c00:32,  2.00it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  24% 21/86 [00:10\u003c00:31,  2.06it/s]\u001b[A2024-11-06 06:16:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:16:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:16:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 050 | valid on 'valid' subset:  26% 22/86 [00:11\u003c00:32,  1.98it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  27% 23/86 [00:11\u003c00:31,  1.98it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  28% 24/86 [00:12\u003c00:29,  2.09it/s]\u001b[A2024-11-06 06:16:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:16:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:16:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 050 | valid on 'valid' subset:  29% 25/86 [00:12\u003c00:31,  1.92it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  30% 26/86 [00:13\u003c00:31,  1.89it/s]\u001b[A2024-11-06 06:16:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:16:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:16:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 050 | valid on 'valid' subset:  31% 27/86 [00:13\u003c00:34,  1.73it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  33% 28/86 [00:14\u003c00:41,  1.39it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  34% 29/86 [00:15\u003c00:39,  1.44it/s]\u001b[A\n","epoch 050: 100% 218/219 [00:36\u003c00:00, 12.17it/s, loss=4.956, nll_loss=3.727, ppl=13.24, wps=37694, ups=11.53, wpb=3267.9, bsz=96, num_updates=10900, lr=9.08674e-05, gnorm=1.181, train_wall=8, gb_free=14, wall=1384]\n","epoch 050 | valid on 'valid' subset:  36% 31/86 [00:17\u003c00:45,  1.21it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  37% 32/86 [00:18\u003c00:40,  1.33it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  38% 33/86 [00:19\u003c00:44,  1.20it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  40% 34/86 [00:20\u003c00:43,  1.19it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  41% 35/86 [00:20\u003c00:38,  1.32it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  42% 36/86 [00:21\u003c00:39,  1.28it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  43% 37/86 [00:22\u003c00:35,  1.38it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  44% 38/86 [00:22\u003c00:36,  1.31it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  45% 39/86 [00:23\u003c00:32,  1.47it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  47% 40/86 [00:24\u003c00:31,  1.46it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  48% 41/86 [00:24\u003c00:28,  1.59it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  49% 42/86 [00:25\u003c00:31,  1.41it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  50% 43/86 [00:26\u003c00:28,  1.50it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  51% 44/86 [00:26\u003c00:27,  1.51it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  52% 45/86 [00:27\u003c00:26,  1.56it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  53% 46/86 [00:27\u003c00:26,  1.50it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  55% 47/86 [00:28\u003c00:28,  1.38it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  56% 48/86 [00:29\u003c00:28,  1.33it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  57% 49/86 [00:30\u003c00:27,  1.35it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  58% 50/86 [00:31\u003c00:29,  1.24it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  59% 51/86 [00:32\u003c00:29,  1.20it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  60% 52/86 [00:33\u003c00:29,  1.17it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  62% 53/86 [00:34\u003c00:29,  1.11it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  63% 54/86 [00:35\u003c00:31,  1.02it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  64% 55/86 [00:35\u003c00:27,  1.14it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  65% 56/86 [00:36\u003c00:24,  1.24it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  66% 57/86 [00:37\u003c00:21,  1.37it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  67% 58/86 [00:37\u003c00:20,  1.38it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  69% 59/86 [00:38\u003c00:18,  1.45it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  70% 60/86 [00:39\u003c00:18,  1.41it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  71% 61/86 [00:40\u003c00:18,  1.34it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  72% 62/86 [00:40\u003c00:17,  1.35it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  73% 63/86 [00:41\u003c00:16,  1.41it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  74% 64/86 [00:42\u003c00:15,  1.43it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  76% 65/86 [00:42\u003c00:14,  1.42it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  77% 66/86 [00:43\u003c00:14,  1.41it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  78% 67/86 [00:44\u003c00:13,  1.43it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  79% 68/86 [00:44\u003c00:11,  1.52it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  80% 69/86 [00:45\u003c00:11,  1.42it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  81% 70/86 [00:46\u003c00:13,  1.19it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  83% 71/86 [00:47\u003c00:13,  1.10it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  84% 72/86 [00:48\u003c00:13,  1.06it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  85% 73/86 [00:49\u003c00:12,  1.02it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  86% 74/86 [00:50\u003c00:11,  1.05it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  87% 75/86 [00:51\u003c00:09,  1.12it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  88% 76/86 [00:52\u003c00:08,  1.25it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  90% 77/86 [00:52\u003c00:07,  1.24it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  91% 78/86 [00:53\u003c00:06,  1.30it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  92% 79/86 [00:54\u003c00:05,  1.24it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  93% 80/86 [00:55\u003c00:04,  1.22it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  94% 81/86 [00:56\u003c00:04,  1.22it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  95% 82/86 [00:56\u003c00:03,  1.25it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  97% 83/86 [00:57\u003c00:02,  1.25it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  98% 84/86 [00:58\u003c00:01,  1.23it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset:  99% 85/86 [00:59\u003c00:00,  1.22it/s]\u001b[A\n","epoch 050 | valid on 'valid' subset: 100% 86/86 [01:00\u003c00:00,  1.25it/s]\u001b[A\n","                                                                        \u001b[A2024-11-06 06:17:17 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 5.055 | nll_loss 3.712 | ppl 13.11 | bleu 15.27 | wps 3433.1 | wpb 2386.4 | bsz 73.7 | num_updates 10950 | best_bleu 15.37\n","2024-11-06 06:17:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 10950 updates\n","2024-11-06 06:17:17 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_last.pt\n","2024-11-06 06:17:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_last.pt\n","2024-11-06 06:17:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-task-nmt/checkpoint_last.pt (epoch 50 @ 10950 updates, score 15.27) (writing took 0.39231663700002173 seconds)\n","2024-11-06 06:17:17 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)\n","2024-11-06 06:17:17 | INFO | train | epoch 050 | loss 4.919 | nll_loss 3.685 | ppl 12.86 | wps 8680.9 | ups 2.72 | wpb 3194.5 | bsz 95.9 | num_updates 10950 | lr 9.06597e-05 | gnorm 1.177 | train_wall 18 | gb_free 14.1 | wall 1448\n","2024-11-06 06:17:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:17:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 051:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:17:17 | INFO | fairseq.trainer | begin training epoch 51\n","2024-11-06 06:17:17 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:17:37 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)\n","2024-11-06 06:17:37 | INFO | train | epoch 051 | loss 4.905 | nll_loss 3.669 | ppl 12.72 | wps 35337.2 | ups 11.06 | wpb 3194.5 | bsz 95.9 | num_updates 11169 | lr 8.97665e-05 | gnorm 1.181 | train_wall 18 | gb_free 14.2 | wall 1468\n","2024-11-06 06:17:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:17:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 052:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:17:37 | INFO | fairseq.trainer | begin training epoch 52\n","2024-11-06 06:17:37 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:17:56 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)\n","2024-11-06 06:17:56 | INFO | train | epoch 052 | loss 4.886 | nll_loss 3.647 | ppl 12.53 | wps 37054.5 | ups 11.6 | wpb 3194.5 | bsz 95.9 | num_updates 11388 | lr 8.88991e-05 | gnorm 1.166 | train_wall 17 | gb_free 14.2 | wall 1487\n","2024-11-06 06:17:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:17:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 053:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:17:56 | INFO | fairseq.trainer | begin training epoch 53\n","2024-11-06 06:17:56 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:18:14 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)\n","2024-11-06 06:18:14 | INFO | train | epoch 053 | loss 4.874 | nll_loss 3.632 | ppl 12.4 | wps 37892.2 | ups 11.86 | wpb 3194.5 | bsz 95.9 | num_updates 11607 | lr 8.80565e-05 | gnorm 1.19 | train_wall 17 | gb_free 14.1 | wall 1506\n","2024-11-06 06:18:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:18:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 054:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:18:15 | INFO | fairseq.trainer | begin training epoch 54\n","2024-11-06 06:18:15 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:18:33 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)\n","2024-11-06 06:18:33 | INFO | train | epoch 054 | loss 4.863 | nll_loss 3.619 | ppl 12.29 | wps 37736.6 | ups 11.81 | wpb 3194.5 | bsz 95.9 | num_updates 11826 | lr 8.72373e-05 | gnorm 1.209 | train_wall 17 | gb_free 14.2 | wall 1524\n","2024-11-06 06:18:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:18:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 055:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:18:33 | INFO | fairseq.trainer | begin training epoch 55\n","2024-11-06 06:18:33 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 055:  79% 172/219 [00:14\u003c00:03, 12.60it/s, loss=4.869, nll_loss=3.626, ppl=12.34, wps=37969.7, ups=11.64, wpb=3262.1, bsz=95.6, num_updates=11900, lr=8.69657e-05, gnorm=1.201, train_wall=8, gb_free=14.2, wall=1531]2024-11-06 06:18:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-11-06 06:18:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 055 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:   1% 1/86 [00:00\u003c00:45,  1.86it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:   2% 2/86 [00:00\u003c00:39,  2.13it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:   3% 3/86 [00:01\u003c00:42,  1.97it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:   5% 4/86 [00:01\u003c00:39,  2.06it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:   6% 5/86 [00:02\u003c00:35,  2.25it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:   7% 6/86 [00:02\u003c00:33,  2.35it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:   8% 7/86 [00:03\u003c00:32,  2.41it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:   9% 8/86 [00:03\u003c00:36,  2.13it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  10% 9/86 [00:04\u003c00:33,  2.32it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  12% 10/86 [00:04\u003c00:30,  2.47it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  13% 11/86 [00:04\u003c00:34,  2.20it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  14% 12/86 [00:05\u003c00:32,  2.24it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  15% 13/86 [00:06\u003c00:36,  1.98it/s]\u001b[A2024-11-06 06:18:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:18:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:18:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 055 | valid on 'valid' subset:  16% 14/86 [00:06\u003c00:39,  1.81it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  17% 15/86 [00:07\u003c00:38,  1.84it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  19% 16/86 [00:07\u003c00:41,  1.68it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  20% 17/86 [00:08\u003c00:41,  1.67it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  21% 18/86 [00:09\u003c00:45,  1.49it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  22% 19/86 [00:10\u003c00:51,  1.30it/s]\u001b[A2024-11-06 06:18:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:18:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:18:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 055 | valid on 'valid' subset:  23% 20/86 [00:11\u003c00:52,  1.25it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  24% 21/86 [00:11\u003c00:47,  1.37it/s]\u001b[A2024-11-06 06:19:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:19:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:19:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 055 | valid on 'valid' subset:  26% 22/86 [00:12\u003c00:43,  1.47it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  27% 23/86 [00:12\u003c00:39,  1.61it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  28% 24/86 [00:13\u003c00:34,  1.78it/s]\u001b[A2024-11-06 06:19:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:19:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:19:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 055 | valid on 'valid' subset:  29% 25/86 [00:13\u003c00:34,  1.75it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  30% 26/86 [00:14\u003c00:32,  1.83it/s]\u001b[A2024-11-06 06:19:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:19:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:19:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 055:  79% 172/219 [00:30\u003c00:03, 12.60it/s, loss=4.833, nll_loss=3.585, ppl=12, wps=36292.4, ups=11.59, wpb=3132.3, bsz=96, num_updates=12000, lr=8.66025e-05, gnorm=1.215, train_wall=8, gb_free=14.1, wall=1539]     \n","epoch 055 | valid on 'valid' subset:  33% 28/86 [00:15\u003c00:35,  1.65it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  34% 29/86 [00:16\u003c00:32,  1.76it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  35% 30/86 [00:16\u003c00:33,  1.66it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  36% 31/86 [00:17\u003c00:36,  1.50it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  37% 32/86 [00:18\u003c00:31,  1.73it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  38% 33/86 [00:18\u003c00:31,  1.68it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  40% 34/86 [00:19\u003c00:30,  1.70it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  41% 35/86 [00:19\u003c00:31,  1.63it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  42% 36/86 [00:20\u003c00:29,  1.68it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  43% 37/86 [00:21\u003c00:29,  1.67it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  44% 38/86 [00:21\u003c00:28,  1.69it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  45% 39/86 [00:22\u003c00:29,  1.61it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  47% 40/86 [00:23\u003c00:33,  1.39it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  48% 41/86 [00:23\u003c00:31,  1.41it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  49% 42/86 [00:25\u003c00:39,  1.12it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  50% 43/86 [00:26\u003c00:37,  1.14it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  51% 44/86 [00:27\u003c00:38,  1.08it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  52% 45/86 [00:27\u003c00:34,  1.19it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  53% 46/86 [00:28\u003c00:31,  1.29it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  55% 47/86 [00:29\u003c00:30,  1.26it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  56% 48/86 [00:29\u003c00:28,  1.35it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  57% 49/86 [00:30\u003c00:25,  1.45it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  58% 50/86 [00:31\u003c00:24,  1.49it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  59% 51/86 [00:31\u003c00:22,  1.53it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  60% 52/86 [00:32\u003c00:22,  1.54it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  62% 53/86 [00:33\u003c00:22,  1.50it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  63% 54/86 [00:33\u003c00:21,  1.52it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  64% 55/86 [00:34\u003c00:19,  1.58it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  65% 56/86 [00:34\u003c00:18,  1.58it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  66% 57/86 [00:35\u003c00:18,  1.60it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  67% 58/86 [00:36\u003c00:18,  1.54it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  69% 59/86 [00:36\u003c00:17,  1.58it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  70% 60/86 [00:37\u003c00:18,  1.42it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  71% 61/86 [00:38\u003c00:20,  1.22it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  72% 62/86 [00:39\u003c00:21,  1.10it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  73% 63/86 [00:40\u003c00:21,  1.06it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  74% 64/86 [00:41\u003c00:21,  1.04it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  76% 65/86 [00:43\u003c00:21,  1.01s/it]\u001b[A\n","epoch 055 | valid on 'valid' subset:  77% 66/86 [00:43\u003c00:18,  1.09it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  78% 67/86 [00:44\u003c00:15,  1.21it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  79% 68/86 [00:44\u003c00:13,  1.32it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  80% 69/86 [00:45\u003c00:12,  1.38it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  81% 70/86 [00:46\u003c00:12,  1.31it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  83% 71/86 [00:47\u003c00:11,  1.33it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  84% 72/86 [00:47\u003c00:10,  1.40it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  85% 73/86 [00:48\u003c00:09,  1.41it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  86% 74/86 [00:49\u003c00:08,  1.42it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  87% 75/86 [00:49\u003c00:07,  1.43it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  88% 76/86 [00:50\u003c00:06,  1.49it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  90% 77/86 [00:51\u003c00:06,  1.45it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  91% 78/86 [00:51\u003c00:05,  1.44it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  92% 79/86 [00:52\u003c00:05,  1.34it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  93% 80/86 [00:53\u003c00:05,  1.17it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  94% 81/86 [00:55\u003c00:04,  1.06it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  95% 82/86 [00:56\u003c00:03,  1.02it/s]\u001b[A\n","epoch 055 | valid on 'valid' subset:  97% 83/86 [00:57\u003c00:03,  1.06s/it]\u001b[A\n","epoch 055 | valid on 'valid' subset:  98% 84/86 [00:58\u003c00:02,  1.10s/it]\u001b[A\n","epoch 055 | valid on 'valid' subset:  99% 85/86 [00:59\u003c00:01,  1.02s/it]\u001b[A\n","epoch 055 | valid on 'valid' subset: 100% 86/86 [01:00\u003c00:00,  1.06it/s]\u001b[A\n","                                                                        \u001b[A2024-11-06 06:19:48 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 5.024 | nll_loss 3.671 | ppl 12.74 | bleu 16.2 | wps 3424.7 | wpb 2386.4 | bsz 73.7 | num_updates 12000 | best_bleu 16.2\n","2024-11-06 06:19:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 12000 updates\n","2024-11-06 06:19:48 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_55_12000.pt\n","2024-11-06 06:19:49 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_55_12000.pt\n","2024-11-06 06:19:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-task-nmt/checkpoint_55_12000.pt (epoch 55 @ 12000 updates, score 16.2) (writing took 1.167754314999911 seconds)\n","2024-11-06 06:19:53 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)\n","2024-11-06 06:19:53 | INFO | train | epoch 055 | loss 4.85 | nll_loss 3.604 | ppl 12.16 | wps 8716.7 | ups 2.73 | wpb 3194.5 | bsz 95.9 | num_updates 12045 | lr 8.64406e-05 | gnorm 1.216 | train_wall 17 | gb_free 14.1 | wall 1604\n","2024-11-06 06:19:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:19:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 056:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:19:53 | INFO | fairseq.trainer | begin training epoch 56\n","2024-11-06 06:19:53 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:20:13 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)\n","2024-11-06 06:20:13 | INFO | train | epoch 056 | loss 4.832 | nll_loss 3.583 | ppl 11.99 | wps 34669.9 | ups 10.85 | wpb 3194.5 | bsz 95.9 | num_updates 12264 | lr 8.56653e-05 | gnorm 1.198 | train_wall 18 | gb_free 14.1 | wall 1625\n","2024-11-06 06:20:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:20:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 057:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:20:14 | INFO | fairseq.trainer | begin training epoch 57\n","2024-11-06 06:20:14 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:20:34 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)\n","2024-11-06 06:20:34 | INFO | train | epoch 057 | loss 4.816 | nll_loss 3.565 | ppl 11.84 | wps 34573.9 | ups 10.82 | wpb 3194.5 | bsz 95.9 | num_updates 12483 | lr 8.49106e-05 | gnorm 1.204 | train_wall 18 | gb_free 14.2 | wall 1645\n","2024-11-06 06:20:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:20:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 058:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:20:34 | INFO | fairseq.trainer | begin training epoch 58\n","2024-11-06 06:20:34 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:20:52 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)\n","2024-11-06 06:20:52 | INFO | train | epoch 058 | loss 4.803 | nll_loss 3.549 | ppl 11.71 | wps 37302.8 | ups 11.68 | wpb 3194.5 | bsz 95.9 | num_updates 12702 | lr 8.41754e-05 | gnorm 1.195 | train_wall 17 | gb_free 14.1 | wall 1664\n","2024-11-06 06:20:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:20:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 059:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:20:53 | INFO | fairseq.trainer | begin training epoch 59\n","2024-11-06 06:20:53 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:21:12 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)\n","2024-11-06 06:21:12 | INFO | train | epoch 059 | loss 4.794 | nll_loss 3.537 | ppl 11.61 | wps 36482.4 | ups 11.42 | wpb 3194.5 | bsz 95.9 | num_updates 12921 | lr 8.3459e-05 | gnorm 1.201 | train_wall 17 | gb_free 14.1 | wall 1683\n","2024-11-06 06:21:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:21:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 060:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:21:12 | INFO | fairseq.trainer | begin training epoch 60\n","2024-11-06 06:21:12 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:21:30 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)\n","2024-11-06 06:21:30 | INFO | train | epoch 060 | loss 4.782 | nll_loss 3.525 | ppl 11.51 | wps 37473.7 | ups 11.73 | wpb 3194.5 | bsz 95.9 | num_updates 13140 | lr 8.27606e-05 | gnorm 1.215 | train_wall 17 | gb_free 14.1 | wall 1701\n","2024-11-06 06:21:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:21:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 061:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:21:30 | INFO | fairseq.trainer | begin training epoch 61\n","2024-11-06 06:21:31 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:21:49 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)\n","2024-11-06 06:21:49 | INFO | train | epoch 061 | loss 4.768 | nll_loss 3.507 | ppl 11.37 | wps 37733.1 | ups 11.81 | wpb 3194.5 | bsz 95.9 | num_updates 13359 | lr 8.20794e-05 | gnorm 1.221 | train_wall 17 | gb_free 14.2 | wall 1720\n","2024-11-06 06:21:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:21:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 062:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:21:49 | INFO | fairseq.trainer | begin training epoch 62\n","2024-11-06 06:21:49 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:22:08 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)\n","2024-11-06 06:22:08 | INFO | train | epoch 062 | loss 4.757 | nll_loss 3.494 | ppl 11.27 | wps 36843.7 | ups 11.53 | wpb 3194.5 | bsz 95.9 | num_updates 13578 | lr 8.14148e-05 | gnorm 1.237 | train_wall 17 | gb_free 14.1 | wall 1739\n","2024-11-06 06:22:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:22:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 063:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:22:08 | INFO | fairseq.trainer | begin training epoch 63\n","2024-11-06 06:22:08 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:22:27 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)\n","2024-11-06 06:22:27 | INFO | train | epoch 063 | loss 4.744 | nll_loss 3.479 | ppl 11.15 | wps 37119.5 | ups 11.62 | wpb 3194.5 | bsz 95.9 | num_updates 13797 | lr 8.07661e-05 | gnorm 1.212 | train_wall 17 | gb_free 14.1 | wall 1758\n","2024-11-06 06:22:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:22:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 064:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:22:27 | INFO | fairseq.trainer | begin training epoch 64\n","2024-11-06 06:22:27 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 064:  92% 202/219 [00:17\u003c00:01, 12.05it/s, loss=4.739, nll_loss=3.473, ppl=11.1, wps=41518.1, ups=12.55, wpb=3307.8, bsz=97.1, num_updates=13900, lr=8.04663e-05, gnorm=1.205, train_wall=8, gb_free=14, wall=1766]2024-11-06 06:22:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-11-06 06:22:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 064 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:   1% 1/86 [00:00\u003c00:50,  1.68it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:   2% 2/86 [00:01\u003c00:57,  1.47it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:   3% 3/86 [00:01\u003c00:49,  1.67it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:   5% 4/86 [00:02\u003c00:43,  1.90it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:   6% 5/86 [00:02\u003c00:38,  2.12it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:   7% 6/86 [00:03\u003c00:35,  2.23it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:   8% 7/86 [00:03\u003c00:33,  2.35it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:   9% 8/86 [00:03\u003c00:32,  2.40it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  10% 9/86 [00:04\u003c00:29,  2.57it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  12% 10/86 [00:04\u003c00:28,  2.65it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  13% 11/86 [00:04\u003c00:30,  2.47it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  14% 12/86 [00:05\u003c00:29,  2.49it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  15% 13/86 [00:05\u003c00:29,  2.46it/s]\u001b[A2024-11-06 06:22:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:22:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:22:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 064 | valid on 'valid' subset:  16% 14/86 [00:06\u003c00:30,  2.34it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  17% 15/86 [00:06\u003c00:28,  2.47it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  19% 16/86 [00:07\u003c00:30,  2.31it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  20% 17/86 [00:07\u003c00:31,  2.18it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  21% 18/86 [00:08\u003c00:35,  1.91it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  22% 19/86 [00:08\u003c00:35,  1.91it/s]\u001b[A2024-11-06 06:22:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:22:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:22:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 064 | valid on 'valid' subset:  23% 20/86 [00:09\u003c00:38,  1.71it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  24% 21/86 [00:10\u003c00:39,  1.64it/s]\u001b[A2024-11-06 06:22:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:22:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:22:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 064 | valid on 'valid' subset:  26% 22/86 [00:11\u003c00:43,  1.47it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  27% 23/86 [00:11\u003c00:42,  1.47it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  28% 24/86 [00:12\u003c00:40,  1.52it/s]\u001b[A2024-11-06 06:22:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:22:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:22:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 064 | valid on 'valid' subset:  29% 25/86 [00:13\u003c00:41,  1.49it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  30% 26/86 [00:13\u003c00:35,  1.68it/s]\u001b[A2024-11-06 06:22:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:22:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:22:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 064 | valid on 'valid' subset:  31% 27/86 [00:14\u003c00:35,  1.64it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  33% 28/86 [00:14\u003c00:33,  1.71it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  34% 29/86 [00:15\u003c00:30,  1.86it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  35% 30/86 [00:15\u003c00:31,  1.78it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  36% 31/86 [00:16\u003c00:29,  1.86it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  37% 32/86 [00:16\u003c00:26,  2.07it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  38% 33/86 [00:17\u003c00:27,  1.92it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  40% 34/86 [00:17\u003c00:26,  1.97it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  41% 35/86 [00:18\u003c00:26,  1.89it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  42% 36/86 [00:18\u003c00:25,  1.93it/s]\u001b[A\n","epoch 064:  92% 202/219 [00:36\u003c00:01, 12.05it/s, loss=4.731, nll_loss=3.465, ppl=11.04, wps=35275.2, ups=11.48, wpb=3071.5, bsz=93.7, num_updates=14000, lr=8.01784e-05, gnorm=1.229, train_wall=8, gb_free=14.1, wall=1775]\n","epoch 064 | valid on 'valid' subset:  44% 38/86 [00:19\u003c00:25,  1.89it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  45% 39/86 [00:20\u003c00:23,  1.97it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  47% 40/86 [00:20\u003c00:23,  1.95it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  48% 41/86 [00:21\u003c00:21,  2.07it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  49% 42/86 [00:22\u003c00:26,  1.65it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  50% 43/86 [00:22\u003c00:25,  1.72it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  51% 44/86 [00:23\u003c00:26,  1.56it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  52% 45/86 [00:24\u003c00:27,  1.51it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  53% 46/86 [00:24\u003c00:28,  1.41it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  55% 47/86 [00:25\u003c00:28,  1.35it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  56% 48/86 [00:26\u003c00:29,  1.28it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  57% 49/86 [00:27\u003c00:28,  1.29it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  58% 50/86 [00:28\u003c00:28,  1.25it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  59% 51/86 [00:28\u003c00:26,  1.32it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  60% 52/86 [00:29\u003c00:23,  1.42it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  62% 53/86 [00:30\u003c00:22,  1.47it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  63% 54/86 [00:30\u003c00:21,  1.50it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  64% 55/86 [00:31\u003c00:19,  1.59it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  65% 56/86 [00:31\u003c00:18,  1.62it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  66% 57/86 [00:32\u003c00:17,  1.70it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  67% 58/86 [00:32\u003c00:16,  1.71it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  69% 59/86 [00:33\u003c00:15,  1.75it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  70% 60/86 [00:34\u003c00:15,  1.73it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  71% 61/86 [00:34\u003c00:15,  1.64it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  72% 62/86 [00:35\u003c00:14,  1.66it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  73% 63/86 [00:35\u003c00:13,  1.71it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  74% 64/86 [00:36\u003c00:13,  1.69it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  76% 65/86 [00:37\u003c00:12,  1.66it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  77% 66/86 [00:37\u003c00:11,  1.72it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  78% 67/86 [00:38\u003c00:10,  1.74it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  79% 68/86 [00:38\u003c00:10,  1.69it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  80% 69/86 [00:39\u003c00:10,  1.56it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  81% 70/86 [00:40\u003c00:12,  1.28it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  83% 71/86 [00:41\u003c00:12,  1.24it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  84% 72/86 [00:42\u003c00:11,  1.20it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  85% 73/86 [00:43\u003c00:11,  1.17it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  86% 74/86 [00:44\u003c00:09,  1.22it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  87% 75/86 [00:44\u003c00:08,  1.33it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  88% 76/86 [00:45\u003c00:06,  1.44it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  90% 77/86 [00:45\u003c00:05,  1.51it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  91% 78/86 [00:46\u003c00:05,  1.53it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  92% 79/86 [00:47\u003c00:04,  1.42it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  93% 80/86 [00:47\u003c00:04,  1.48it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  94% 81/86 [00:48\u003c00:03,  1.39it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  95% 82/86 [00:49\u003c00:02,  1.43it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  97% 83/86 [00:50\u003c00:02,  1.36it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  98% 84/86 [00:50\u003c00:01,  1.38it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset:  99% 85/86 [00:51\u003c00:00,  1.32it/s]\u001b[A\n","epoch 064 | valid on 'valid' subset: 100% 86/86 [00:52\u003c00:00,  1.32it/s]\u001b[A\n","                                                                        \u001b[A2024-11-06 06:23:36 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 4.971 | nll_loss 3.598 | ppl 12.11 | bleu 16.62 | wps 3935.9 | wpb 2386.4 | bsz 73.7 | num_updates 14000 | best_bleu 16.62\n","2024-11-06 06:23:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 14000 updates\n","2024-11-06 06:23:36 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_64_14000.pt\n","2024-11-06 06:23:37 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_64_14000.pt\n","2024-11-06 06:23:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-task-nmt/checkpoint_64_14000.pt (epoch 64 @ 14000 updates, score 16.62) (writing took 1.1754697840001427 seconds)\n","2024-11-06 06:23:39 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)\n","2024-11-06 06:23:39 | INFO | train | epoch 064 | loss 4.73 | nll_loss 3.463 | ppl 11.03 | wps 9673.4 | ups 3.03 | wpb 3194.5 | bsz 95.9 | num_updates 14016 | lr 8.01326e-05 | gnorm 1.215 | train_wall 17 | gb_free 14.1 | wall 1830\n","2024-11-06 06:23:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:23:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 065:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:23:39 | INFO | fairseq.trainer | begin training epoch 65\n","2024-11-06 06:23:39 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:24:00 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)\n","2024-11-06 06:24:00 | INFO | train | epoch 065 | loss 4.721 | nll_loss 3.452 | ppl 10.94 | wps 33353.5 | ups 10.44 | wpb 3194.5 | bsz 95.9 | num_updates 14235 | lr 7.95138e-05 | gnorm 1.226 | train_wall 19 | gb_free 14.1 | wall 1851\n","2024-11-06 06:24:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:24:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 066:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:24:00 | INFO | fairseq.trainer | begin training epoch 66\n","2024-11-06 06:24:00 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:24:18 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)\n","2024-11-06 06:24:18 | INFO | train | epoch 066 | loss 4.71 | nll_loss 3.439 | ppl 10.85 | wps 37946.8 | ups 11.88 | wpb 3194.5 | bsz 95.9 | num_updates 14454 | lr 7.89091e-05 | gnorm 1.242 | train_wall 17 | gb_free 14.1 | wall 1870\n","2024-11-06 06:24:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:24:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 067:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:24:19 | INFO | fairseq.trainer | begin training epoch 67\n","2024-11-06 06:24:19 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:24:37 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)\n","2024-11-06 06:24:37 | INFO | train | epoch 067 | loss 4.7 | nll_loss 3.428 | ppl 10.76 | wps 38352.9 | ups 12.01 | wpb 3194.5 | bsz 95.9 | num_updates 14673 | lr 7.8318e-05 | gnorm 1.245 | train_wall 17 | gb_free 14.2 | wall 1888\n","2024-11-06 06:24:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:24:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 068:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:24:37 | INFO | fairseq.trainer | begin training epoch 68\n","2024-11-06 06:24:37 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:24:55 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)\n","2024-11-06 06:24:55 | INFO | train | epoch 068 | loss 4.689 | nll_loss 3.414 | ppl 10.66 | wps 37968.7 | ups 11.89 | wpb 3194.5 | bsz 95.9 | num_updates 14892 | lr 7.774e-05 | gnorm 1.226 | train_wall 17 | gb_free 14.1 | wall 1906\n","2024-11-06 06:24:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:24:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 069:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:24:55 | INFO | fairseq.trainer | begin training epoch 69\n","2024-11-06 06:24:55 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:25:14 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)\n","2024-11-06 06:25:14 | INFO | train | epoch 069 | loss 4.679 | nll_loss 3.402 | ppl 10.57 | wps 37659.7 | ups 11.79 | wpb 3194.5 | bsz 95.9 | num_updates 15111 | lr 7.71746e-05 | gnorm 1.247 | train_wall 17 | gb_free 14.1 | wall 1925\n","2024-11-06 06:25:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:25:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 070:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:25:14 | INFO | fairseq.trainer | begin training epoch 70\n","2024-11-06 06:25:14 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:25:33 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)\n","2024-11-06 06:25:33 | INFO | train | epoch 070 | loss 4.668 | nll_loss 3.389 | ppl 10.48 | wps 36645.2 | ups 11.47 | wpb 3194.5 | bsz 95.9 | num_updates 15330 | lr 7.66214e-05 | gnorm 1.248 | train_wall 17 | gb_free 14.1 | wall 1944\n","2024-11-06 06:25:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:25:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 071:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:25:33 | INFO | fairseq.trainer | begin training epoch 71\n","2024-11-06 06:25:33 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:25:51 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)\n","2024-11-06 06:25:51 | INFO | train | epoch 071 | loss 4.653 | nll_loss 3.372 | ppl 10.35 | wps 37551.5 | ups 11.75 | wpb 3194.5 | bsz 95.9 | num_updates 15549 | lr 7.60799e-05 | gnorm 1.229 | train_wall 17 | gb_free 14.1 | wall 1963\n","2024-11-06 06:25:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:25:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 072:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:25:52 | INFO | fairseq.trainer | begin training epoch 72\n","2024-11-06 06:25:52 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:26:10 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)\n","2024-11-06 06:26:10 | INFO | train | epoch 072 | loss 4.647 | nll_loss 3.364 | ppl 10.3 | wps 38265.2 | ups 11.98 | wpb 3194.5 | bsz 95.9 | num_updates 15768 | lr 7.55497e-05 | gnorm 1.232 | train_wall 17 | gb_free 14.1 | wall 1981\n","2024-11-06 06:26:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:26:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 073:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:26:10 | INFO | fairseq.trainer | begin training epoch 73\n","2024-11-06 06:26:10 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:26:29 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)\n","2024-11-06 06:26:29 | INFO | train | epoch 073 | loss 4.636 | nll_loss 3.352 | ppl 10.21 | wps 36197.9 | ups 11.33 | wpb 3194.5 | bsz 95.9 | num_updates 15987 | lr 7.50305e-05 | gnorm 1.24 | train_wall 17 | gb_free 14.1 | wall 2000\n","2024-11-06 06:26:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:26:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 074:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:26:29 | INFO | fairseq.trainer | begin training epoch 74\n","2024-11-06 06:26:29 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 074:   5% 11/219 [00:01\u003c00:18, 11.52it/s]2024-11-06 06:26:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-11-06 06:26:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 074 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:   1% 1/86 [00:00\u003c01:06,  1.27it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:   2% 2/86 [00:01\u003c00:53,  1.57it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:   3% 3/86 [00:01\u003c00:54,  1.52it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:   5% 4/86 [00:02\u003c00:51,  1.58it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:   6% 5/86 [00:03\u003c00:47,  1.72it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:   7% 6/86 [00:03\u003c00:46,  1.72it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:   8% 7/86 [00:04\u003c00:46,  1.70it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:   9% 8/86 [00:04\u003c00:46,  1.68it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  10% 9/86 [00:05\u003c00:42,  1.80it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  12% 10/86 [00:05\u003c00:40,  1.88it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  13% 11/86 [00:06\u003c00:38,  1.96it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  14% 12/86 [00:06\u003c00:35,  2.06it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  15% 13/86 [00:07\u003c00:33,  2.20it/s]\u001b[A2024-11-06 06:26:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:26:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:26:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 074 | valid on 'valid' subset:  16% 14/86 [00:07\u003c00:33,  2.15it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  17% 15/86 [00:07\u003c00:31,  2.29it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  19% 16/86 [00:08\u003c00:32,  2.14it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  20% 17/86 [00:08\u003c00:30,  2.23it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  21% 18/86 [00:09\u003c00:33,  2.03it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  22% 19/86 [00:09\u003c00:30,  2.20it/s]\u001b[A2024-11-06 06:26:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:26:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:26:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 074 | valid on 'valid' subset:  23% 20/86 [00:10\u003c00:31,  2.10it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  24% 21/86 [00:10\u003c00:30,  2.12it/s]\u001b[A2024-11-06 06:26:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:26:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:26:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 074 | valid on 'valid' subset:  26% 22/86 [00:11\u003c00:31,  2.06it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  27% 23/86 [00:11\u003c00:30,  2.06it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  28% 24/86 [00:12\u003c00:28,  2.19it/s]\u001b[A2024-11-06 06:26:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:26:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:26:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 074 | valid on 'valid' subset:  29% 25/86 [00:12\u003c00:30,  2.02it/s]\u001b[A\n","epoch 074:   5% 11/219 [00:14\u003c00:18, 11.52it/s, loss=4.643, nll_loss=3.36, ppl=10.26, wps=36671, ups=11.58, wpb=3166.4, bsz=94, num_updates=16000, lr=7.5e-05, gnorm=1.243, train_wall=8, gb_free=14.2, wall=2001]2024-11-06 06:26:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:26:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:26:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 074 | valid on 'valid' subset:  31% 27/86 [00:13\u003c00:31,  1.89it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  33% 28/86 [00:14\u003c00:31,  1.83it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  34% 29/86 [00:14\u003c00:28,  1.98it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  35% 30/86 [00:15\u003c00:31,  1.76it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  36% 31/86 [00:16\u003c00:35,  1.54it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  37% 32/86 [00:17\u003c00:33,  1.61it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  38% 33/86 [00:17\u003c00:37,  1.42it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  40% 34/86 [00:18\u003c00:37,  1.40it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  41% 35/86 [00:19\u003c00:38,  1.32it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  42% 36/86 [00:20\u003c00:38,  1.29it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  43% 37/86 [00:21\u003c00:38,  1.26it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  44% 38/86 [00:22\u003c00:40,  1.18it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  45% 39/86 [00:23\u003c00:40,  1.17it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  47% 40/86 [00:23\u003c00:39,  1.18it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  48% 41/86 [00:24\u003c00:35,  1.27it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  49% 42/86 [00:25\u003c00:43,  1.02it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  50% 43/86 [00:26\u003c00:39,  1.08it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  51% 44/86 [00:27\u003c00:37,  1.13it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  52% 45/86 [00:28\u003c00:32,  1.26it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  53% 46/86 [00:28\u003c00:29,  1.36it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  55% 47/86 [00:29\u003c00:27,  1.43it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  56% 48/86 [00:29\u003c00:25,  1.50it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  57% 49/86 [00:30\u003c00:23,  1.59it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  58% 50/86 [00:31\u003c00:22,  1.61it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  59% 51/86 [00:31\u003c00:21,  1.62it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  60% 52/86 [00:32\u003c00:20,  1.67it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  62% 53/86 [00:32\u003c00:20,  1.64it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  63% 54/86 [00:33\u003c00:19,  1.64it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  64% 55/86 [00:34\u003c00:18,  1.67it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  65% 56/86 [00:34\u003c00:17,  1.67it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  66% 57/86 [00:35\u003c00:17,  1.70it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  67% 58/86 [00:35\u003c00:16,  1.65it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  69% 59/86 [00:36\u003c00:17,  1.52it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  70% 60/86 [00:37\u003c00:18,  1.42it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  71% 61/86 [00:38\u003c00:19,  1.30it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  72% 62/86 [00:39\u003c00:19,  1.24it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  73% 63/86 [00:40\u003c00:18,  1.24it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  74% 64/86 [00:41\u003c00:18,  1.17it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  76% 65/86 [00:41\u003c00:18,  1.15it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  77% 66/86 [00:42\u003c00:15,  1.27it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  78% 67/86 [00:43\u003c00:13,  1.38it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  79% 68/86 [00:43\u003c00:11,  1.51it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  80% 69/86 [00:44\u003c00:10,  1.57it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  81% 70/86 [00:45\u003c00:11,  1.41it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  83% 71/86 [00:45\u003c00:10,  1.43it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  84% 72/86 [00:46\u003c00:09,  1.48it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  85% 73/86 [00:46\u003c00:08,  1.50it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  86% 74/86 [00:47\u003c00:07,  1.56it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  87% 75/86 [00:48\u003c00:06,  1.60it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  88% 76/86 [00:48\u003c00:06,  1.61it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  90% 77/86 [00:49\u003c00:05,  1.59it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  91% 78/86 [00:50\u003c00:05,  1.58it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  92% 79/86 [00:50\u003c00:04,  1.45it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  93% 80/86 [00:51\u003c00:04,  1.48it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  94% 81/86 [00:52\u003c00:03,  1.28it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  95% 82/86 [00:53\u003c00:03,  1.20it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  97% 83/86 [00:54\u003c00:02,  1.08it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  98% 84/86 [00:55\u003c00:01,  1.02it/s]\u001b[A\n","epoch 074 | valid on 'valid' subset:  99% 85/86 [00:56\u003c00:01,  1.05s/it]\u001b[A\n","epoch 074 | valid on 'valid' subset: 100% 86/86 [00:58\u003c00:00,  1.05s/it]\u001b[A\n","                                                                        \u001b[A2024-11-06 06:27:28 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 4.905 | nll_loss 3.514 | ppl 11.42 | bleu 17.26 | wps 3566.8 | wpb 2386.4 | bsz 73.7 | num_updates 16000 | best_bleu 17.26\n","2024-11-06 06:27:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 16000 updates\n","2024-11-06 06:27:28 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_74_16000.pt\n","2024-11-06 06:27:29 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_74_16000.pt\n","2024-11-06 06:27:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-task-nmt/checkpoint_74_16000.pt (epoch 74 @ 16000 updates, score 17.26) (writing took 1.2675236360000781 seconds)\n","2024-11-06 06:27:49 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)\n","2024-11-06 06:27:49 | INFO | train | epoch 074 | loss 4.631 | nll_loss 3.346 | ppl 10.16 | wps 8777.3 | ups 2.75 | wpb 3194.5 | bsz 95.9 | num_updates 16206 | lr 7.45218e-05 | gnorm 1.268 | train_wall 19 | gb_free 14.1 | wall 2080\n","2024-11-06 06:27:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:27:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 075:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:27:49 | INFO | fairseq.trainer | begin training epoch 75\n","2024-11-06 06:27:49 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 075:  99% 217/219 [00:18\u003c00:00, 12.75it/s, loss=4.612, nll_loss=3.325, ppl=10.02, wps=36948.4, ups=11.86, wpb=3114.5, bsz=92.6, num_updates=16400, lr=7.40797e-05, gnorm=1.25, train_wall=8, gb_free=14.1, wall=2096]2024-11-06 06:28:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-11-06 06:28:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 075 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:   1% 1/86 [00:00\u003c00:49,  1.70it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:   2% 2/86 [00:01\u003c00:41,  2.01it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:   3% 3/86 [00:01\u003c00:46,  1.77it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:   5% 4/86 [00:02\u003c00:42,  1.93it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:   6% 5/86 [00:02\u003c00:38,  2.12it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:   7% 6/86 [00:03\u003c00:41,  1.91it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:   8% 7/86 [00:03\u003c00:41,  1.92it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:   9% 8/86 [00:04\u003c00:53,  1.45it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  10% 9/86 [00:05\u003c00:46,  1.65it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  12% 10/86 [00:05\u003c00:43,  1.73it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  13% 11/86 [00:06\u003c00:45,  1.64it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  14% 12/86 [00:06\u003c00:45,  1.62it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  15% 13/86 [00:07\u003c00:44,  1.64it/s]\u001b[A2024-11-06 06:28:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:28:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:28:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 075 | valid on 'valid' subset:  16% 14/86 [00:08\u003c00:47,  1.53it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  17% 15/86 [00:08\u003c00:43,  1.63it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  19% 16/86 [00:09\u003c00:41,  1.68it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  20% 17/86 [00:09\u003c00:37,  1.85it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  21% 18/86 [00:10\u003c00:38,  1.76it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  22% 19/86 [00:10\u003c00:34,  1.94it/s]\u001b[A2024-11-06 06:28:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:28:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:28:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 075 | valid on 'valid' subset:  23% 20/86 [00:11\u003c00:34,  1.89it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  24% 21/86 [00:11\u003c00:33,  1.95it/s]\u001b[A2024-11-06 06:28:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:28:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:28:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 075 | valid on 'valid' subset:  26% 22/86 [00:12\u003c00:33,  1.90it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  27% 23/86 [00:12\u003c00:32,  1.93it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  28% 24/86 [00:13\u003c00:30,  2.05it/s]\u001b[A2024-11-06 06:28:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:28:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:28:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 075 | valid on 'valid' subset:  29% 25/86 [00:13\u003c00:31,  1.93it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  30% 26/86 [00:14\u003c00:30,  1.98it/s]\u001b[A2024-11-06 06:28:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:28:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:28:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 075 | valid on 'valid' subset:  31% 27/86 [00:15\u003c00:32,  1.79it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  33% 28/86 [00:15\u003c00:32,  1.80it/s]\u001b[A\n","epoch 075:  99% 217/219 [00:34\u003c00:00, 12.75it/s, loss=4.612, nll_loss=3.325, ppl=10.02, wps=36948.4, ups=11.86, wpb=3114.5, bsz=92.6, num_updates=16400, lr=7.40797e-05, gnorm=1.25, train_wall=8, gb_free=14.1, wall=2096]\n","epoch 075 | valid on 'valid' subset:  35% 30/86 [00:16\u003c00:34,  1.64it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  36% 31/86 [00:17\u003c00:33,  1.66it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  37% 32/86 [00:17\u003c00:29,  1.83it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  38% 33/86 [00:18\u003c00:30,  1.73it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  40% 34/86 [00:19\u003c00:33,  1.55it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  41% 35/86 [00:20\u003c00:35,  1.43it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  42% 36/86 [00:20\u003c00:35,  1.40it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  43% 37/86 [00:21\u003c00:36,  1.32it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  44% 38/86 [00:22\u003c00:36,  1.32it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  45% 39/86 [00:23\u003c00:36,  1.30it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  47% 40/86 [00:24\u003c00:36,  1.27it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  48% 41/86 [00:24\u003c00:32,  1.40it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  49% 42/86 [00:25\u003c00:33,  1.30it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  50% 43/86 [00:26\u003c00:29,  1.44it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  51% 44/86 [00:26\u003c00:28,  1.46it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  52% 45/86 [00:27\u003c00:27,  1.52it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  53% 46/86 [00:27\u003c00:25,  1.56it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  55% 47/86 [00:28\u003c00:24,  1.59it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  56% 48/86 [00:29\u003c00:27,  1.38it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  57% 49/86 [00:30\u003c00:24,  1.51it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  58% 50/86 [00:30\u003c00:22,  1.57it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  59% 51/86 [00:31\u003c00:22,  1.59it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  60% 52/86 [00:31\u003c00:21,  1.62it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  62% 53/86 [00:32\u003c00:20,  1.61it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  63% 54/86 [00:33\u003c00:19,  1.62it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  64% 55/86 [00:33\u003c00:18,  1.71it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  65% 56/86 [00:34\u003c00:17,  1.68it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  66% 57/86 [00:34\u003c00:18,  1.59it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  67% 58/86 [00:35\u003c00:18,  1.48it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  69% 59/86 [00:36\u003c00:18,  1.43it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  70% 60/86 [00:37\u003c00:19,  1.34it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  71% 61/86 [00:38\u003c00:22,  1.13it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  72% 62/86 [00:39\u003c00:21,  1.11it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  73% 63/86 [00:40\u003c00:20,  1.13it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  74% 64/86 [00:40\u003c00:18,  1.19it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  76% 65/86 [00:41\u003c00:16,  1.28it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  77% 66/86 [00:42\u003c00:14,  1.39it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  78% 67/86 [00:42\u003c00:12,  1.50it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  79% 68/86 [00:43\u003c00:11,  1.60it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  80% 69/86 [00:43\u003c00:10,  1.65it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  81% 70/86 [00:44\u003c00:09,  1.60it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  83% 71/86 [00:45\u003c00:09,  1.64it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  84% 72/86 [00:45\u003c00:08,  1.66it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  85% 73/86 [00:46\u003c00:07,  1.63it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  86% 74/86 [00:46\u003c00:07,  1.67it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  87% 75/86 [00:47\u003c00:06,  1.66it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  88% 76/86 [00:48\u003c00:05,  1.72it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  90% 77/86 [00:48\u003c00:05,  1.68it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  91% 78/86 [00:49\u003c00:04,  1.65it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  92% 79/86 [00:50\u003c00:04,  1.50it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  93% 80/86 [00:50\u003c00:04,  1.47it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  94% 81/86 [00:51\u003c00:03,  1.39it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  95% 82/86 [00:52\u003c00:03,  1.27it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  97% 83/86 [00:53\u003c00:02,  1.12it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  98% 84/86 [00:54\u003c00:01,  1.06it/s]\u001b[A\n","epoch 075 | valid on 'valid' subset:  99% 85/86 [00:55\u003c00:01,  1.01s/it]\u001b[A\n","epoch 075 | valid on 'valid' subset: 100% 86/86 [00:56\u003c00:00,  1.01it/s]\u001b[A\n","                                                                        \u001b[A2024-11-06 06:29:04 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 4.894 | nll_loss 3.502 | ppl 11.33 | bleu 17.93 | wps 3627.3 | wpb 2386.4 | bsz 73.7 | num_updates 16425 | best_bleu 17.93\n","2024-11-06 06:29:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 16425 updates\n","2024-11-06 06:29:04 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_best.pt\n","2024-11-06 06:29:05 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_best.pt\n","2024-11-06 06:29:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-task-nmt/checkpoint_best.pt (epoch 75 @ 16425 updates, score 17.93) (writing took 0.7938892789998135 seconds)\n","2024-11-06 06:29:05 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)\n","2024-11-06 06:29:05 | INFO | train | epoch 075 | loss 4.617 | nll_loss 3.329 | ppl 10.05 | wps 9184.4 | ups 2.88 | wpb 3194.5 | bsz 95.9 | num_updates 16425 | lr 7.40233e-05 | gnorm 1.244 | train_wall 17 | gb_free 14.1 | wall 2156\n","2024-11-06 06:29:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:29:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 076:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:29:05 | INFO | fairseq.trainer | begin training epoch 76\n","2024-11-06 06:29:05 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:29:25 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)\n","2024-11-06 06:29:25 | INFO | train | epoch 076 | loss 4.609 | nll_loss 3.32 | ppl 9.98 | wps 35421.2 | ups 11.09 | wpb 3194.5 | bsz 95.9 | num_updates 16644 | lr 7.35347e-05 | gnorm 1.259 | train_wall 18 | gb_free 14.1 | wall 2176\n","2024-11-06 06:29:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:29:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 077:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:29:25 | INFO | fairseq.trainer | begin training epoch 77\n","2024-11-06 06:29:25 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:29:43 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)\n","2024-11-06 06:29:43 | INFO | train | epoch 077 | loss 4.599 | nll_loss 3.309 | ppl 9.91 | wps 37586.6 | ups 11.77 | wpb 3194.5 | bsz 95.9 | num_updates 16863 | lr 7.30557e-05 | gnorm 1.269 | train_wall 17 | gb_free 14.1 | wall 2194\n","2024-11-06 06:29:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:29:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 078:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:29:44 | INFO | fairseq.trainer | begin training epoch 78\n","2024-11-06 06:29:44 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:30:02 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)\n","2024-11-06 06:30:02 | INFO | train | epoch 078 | loss 4.59 | nll_loss 3.297 | ppl 9.83 | wps 37579.6 | ups 11.76 | wpb 3194.5 | bsz 95.9 | num_updates 17082 | lr 7.25858e-05 | gnorm 1.255 | train_wall 17 | gb_free 14.1 | wall 2213\n","2024-11-06 06:30:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:30:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 079:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:30:02 | INFO | fairseq.trainer | begin training epoch 79\n","2024-11-06 06:30:02 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:30:21 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)\n","2024-11-06 06:30:21 | INFO | train | epoch 079 | loss 4.581 | nll_loss 3.287 | ppl 9.76 | wps 37024.7 | ups 11.59 | wpb 3194.5 | bsz 95.9 | num_updates 17301 | lr 7.2125e-05 | gnorm 1.261 | train_wall 17 | gb_free 14.1 | wall 2232\n","2024-11-06 06:30:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:30:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 080:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:30:21 | INFO | fairseq.trainer | begin training epoch 80\n","2024-11-06 06:30:21 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:30:40 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)\n","2024-11-06 06:30:40 | INFO | train | epoch 080 | loss 4.571 | nll_loss 3.274 | ppl 9.67 | wps 36789.8 | ups 11.52 | wpb 3194.5 | bsz 95.9 | num_updates 17520 | lr 7.16728e-05 | gnorm 1.246 | train_wall 17 | gb_free 14.1 | wall 2251\n","2024-11-06 06:30:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:30:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 081:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:30:40 | INFO | fairseq.trainer | begin training epoch 81\n","2024-11-06 06:30:40 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:30:58 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)\n","2024-11-06 06:30:58 | INFO | train | epoch 081 | loss 4.561 | nll_loss 3.264 | ppl 9.61 | wps 37690.7 | ups 11.8 | wpb 3194.5 | bsz 95.9 | num_updates 17739 | lr 7.1229e-05 | gnorm 1.258 | train_wall 17 | gb_free 14.1 | wall 2270\n","2024-11-06 06:30:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:30:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 082:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:30:59 | INFO | fairseq.trainer | begin training epoch 82\n","2024-11-06 06:30:59 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:31:17 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)\n","2024-11-06 06:31:17 | INFO | train | epoch 082 | loss 4.553 | nll_loss 3.254 | ppl 9.54 | wps 37512 | ups 11.74 | wpb 3194.5 | bsz 95.9 | num_updates 17958 | lr 7.07933e-05 | gnorm 1.259 | train_wall 17 | gb_free 14.1 | wall 2288\n","2024-11-06 06:31:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:31:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 083:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:31:17 | INFO | fairseq.trainer | begin training epoch 83\n","2024-11-06 06:31:17 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 083:  19% 41/219 [00:03\u003c00:13, 13.09it/s]2024-11-06 06:31:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-11-06 06:31:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 083 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:   1% 1/86 [00:00\u003c00:54,  1.55it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:   2% 2/86 [00:01\u003c00:50,  1.65it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:   3% 3/86 [00:01\u003c00:49,  1.67it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:   5% 4/86 [00:02\u003c00:45,  1.80it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:   6% 5/86 [00:02\u003c00:43,  1.88it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:   7% 6/86 [00:03\u003c00:44,  1.81it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:   8% 7/86 [00:04\u003c00:45,  1.73it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:   9% 8/86 [00:05\u003c00:59,  1.32it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  10% 9/86 [00:05\u003c00:53,  1.45it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  12% 10/86 [00:06\u003c00:48,  1.58it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  13% 11/86 [00:06\u003c00:46,  1.63it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  14% 12/86 [00:07\u003c00:41,  1.78it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  15% 13/86 [00:07\u003c00:37,  1.94it/s]\u001b[A2024-11-06 06:31:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:31:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:31:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 083 | valid on 'valid' subset:  16% 14/86 [00:08\u003c00:42,  1.71it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  17% 15/86 [00:08\u003c00:41,  1.72it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  19% 16/86 [00:09\u003c00:44,  1.57it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  20% 17/86 [00:10\u003c00:42,  1.63it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  21% 18/86 [00:11\u003c00:50,  1.34it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  22% 19/86 [00:11\u003c00:46,  1.45it/s]\u001b[A2024-11-06 06:31:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:31:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:31:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 083:  19% 41/219 [00:16\u003c00:13, 13.09it/s, loss=4.574, nll_loss=3.277, ppl=9.7, wps=39538.5, ups=12.26, wpb=3225.5, bsz=92, num_updates=18000, lr=7.07107e-05, gnorm=1.227, train_wall=8, gb_free=14.1, wall=2292]\n","epoch 083 | valid on 'valid' subset:  24% 21/86 [00:13\u003c00:47,  1.37it/s]\u001b[A2024-11-06 06:31:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:31:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:31:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 083 | valid on 'valid' subset:  26% 22/86 [00:14\u003c00:46,  1.38it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  27% 23/86 [00:14\u003c00:41,  1.51it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  28% 24/86 [00:15\u003c00:36,  1.70it/s]\u001b[A2024-11-06 06:31:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:31:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:31:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 083 | valid on 'valid' subset:  29% 25/86 [00:15\u003c00:36,  1.67it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  30% 26/86 [00:16\u003c00:33,  1.81it/s]\u001b[A2024-11-06 06:31:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:31:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:31:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 083 | valid on 'valid' subset:  31% 27/86 [00:17\u003c00:38,  1.51it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  33% 28/86 [00:17\u003c00:39,  1.46it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  34% 29/86 [00:18\u003c00:38,  1.46it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  35% 30/86 [00:19\u003c00:48,  1.16it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  36% 31/86 [00:20\u003c00:50,  1.09it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  37% 32/86 [00:21\u003c00:44,  1.21it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  38% 33/86 [00:22\u003c00:47,  1.11it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  40% 34/86 [00:23\u003c00:40,  1.27it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  41% 35/86 [00:23\u003c00:37,  1.35it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  42% 36/86 [00:24\u003c00:38,  1.29it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  43% 37/86 [00:25\u003c00:35,  1.39it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  44% 38/86 [00:25\u003c00:33,  1.43it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  45% 39/86 [00:26\u003c00:30,  1.54it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  47% 40/86 [00:26\u003c00:28,  1.61it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  48% 41/86 [00:27\u003c00:25,  1.77it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  49% 42/86 [00:28\u003c00:29,  1.51it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  50% 43/86 [00:28\u003c00:27,  1.58it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  51% 44/86 [00:29\u003c00:27,  1.53it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  52% 45/86 [00:30\u003c00:27,  1.50it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  53% 46/86 [00:30\u003c00:27,  1.46it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  55% 47/86 [00:31\u003c00:28,  1.38it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  56% 48/86 [00:32\u003c00:28,  1.35it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  57% 49/86 [00:33\u003c00:26,  1.39it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  58% 50/86 [00:33\u003c00:27,  1.32it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  59% 51/86 [00:34\u003c00:28,  1.21it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  60% 52/86 [00:35\u003c00:28,  1.18it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  62% 53/86 [00:36\u003c00:29,  1.10it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  63% 54/86 [00:37\u003c00:28,  1.11it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  64% 55/86 [00:38\u003c00:24,  1.26it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  65% 56/86 [00:38\u003c00:22,  1.34it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  66% 57/86 [00:39\u003c00:21,  1.38it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  67% 58/86 [00:40\u003c00:19,  1.43it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  69% 59/86 [00:40\u003c00:18,  1.48it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  70% 60/86 [00:41\u003c00:17,  1.51it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  71% 61/86 [00:42\u003c00:17,  1.41it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  72% 62/86 [00:42\u003c00:16,  1.46it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  73% 63/86 [00:43\u003c00:15,  1.50it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  74% 64/86 [00:44\u003c00:14,  1.49it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  76% 65/86 [00:44\u003c00:14,  1.49it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  77% 66/86 [00:45\u003c00:13,  1.51it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  78% 67/86 [00:46\u003c00:12,  1.55it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  79% 68/86 [00:46\u003c00:11,  1.60it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  80% 69/86 [00:47\u003c00:10,  1.62it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  81% 70/86 [00:48\u003c00:12,  1.32it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  83% 71/86 [00:49\u003c00:12,  1.24it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  84% 72/86 [00:50\u003c00:11,  1.21it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  85% 73/86 [00:51\u003c00:11,  1.11it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  86% 74/86 [00:52\u003c00:10,  1.10it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  87% 75/86 [00:53\u003c00:10,  1.08it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  88% 76/86 [00:54\u003c00:09,  1.10it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  90% 77/86 [00:54\u003c00:07,  1.14it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  91% 78/86 [00:55\u003c00:06,  1.22it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  92% 79/86 [00:56\u003c00:05,  1.18it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  93% 80/86 [00:57\u003c00:04,  1.25it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  94% 81/86 [00:57\u003c00:04,  1.24it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  95% 82/86 [00:58\u003c00:03,  1.29it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  97% 83/86 [00:59\u003c00:02,  1.26it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  98% 84/86 [01:00\u003c00:01,  1.23it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset:  99% 85/86 [01:01\u003c00:00,  1.21it/s]\u001b[A\n","epoch 083 | valid on 'valid' subset: 100% 86/86 [01:02\u003c00:00,  1.24it/s]\u001b[A\n","                                                                        \u001b[A2024-11-06 06:32:23 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 4.869 | nll_loss 3.469 | ppl 11.07 | bleu 18.64 | wps 3327 | wpb 2386.4 | bsz 73.7 | num_updates 18000 | best_bleu 18.64\n","2024-11-06 06:32:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 18000 updates\n","2024-11-06 06:32:23 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_83_18000.pt\n","2024-11-06 06:32:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_83_18000.pt\n","2024-11-06 06:32:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-task-nmt/checkpoint_83_18000.pt (epoch 83 @ 18000 updates, score 18.64) (writing took 1.1844893090001278 seconds)\n","2024-11-06 06:32:40 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)\n","2024-11-06 06:32:40 | INFO | train | epoch 083 | loss 4.544 | nll_loss 3.243 | ppl 9.46 | wps 8402.7 | ups 2.63 | wpb 3194.5 | bsz 95.9 | num_updates 18177 | lr 7.03656e-05 | gnorm 1.268 | train_wall 18 | gb_free 14 | wall 2371\n","2024-11-06 06:32:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:32:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 084:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:32:41 | INFO | fairseq.trainer | begin training epoch 84\n","2024-11-06 06:32:41 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:32:59 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)\n","2024-11-06 06:32:59 | INFO | train | epoch 084 | loss 4.537 | nll_loss 3.235 | ppl 9.42 | wps 36970 | ups 11.57 | wpb 3194.5 | bsz 95.9 | num_updates 18396 | lr 6.99455e-05 | gnorm 1.282 | train_wall 17 | gb_free 14.2 | wall 2390\n","2024-11-06 06:32:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:32:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 085:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:33:00 | INFO | fairseq.trainer | begin training epoch 85\n","2024-11-06 06:33:00 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:33:19 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)\n","2024-11-06 06:33:19 | INFO | train | epoch 085 | loss 4.527 | nll_loss 3.223 | ppl 9.34 | wps 34911.9 | ups 10.93 | wpb 3194.5 | bsz 95.9 | num_updates 18615 | lr 6.95328e-05 | gnorm 1.285 | train_wall 18 | gb_free 14 | wall 2410\n","2024-11-06 06:33:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:33:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 086:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:33:20 | INFO | fairseq.trainer | begin training epoch 86\n","2024-11-06 06:33:20 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:33:38 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)\n","2024-11-06 06:33:38 | INFO | train | epoch 086 | loss 4.52 | nll_loss 3.214 | ppl 9.28 | wps 37297.1 | ups 11.68 | wpb 3194.5 | bsz 95.9 | num_updates 18834 | lr 6.91274e-05 | gnorm 1.278 | train_wall 17 | gb_free 14.1 | wall 2429\n","2024-11-06 06:33:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:33:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 087:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:33:38 | INFO | fairseq.trainer | begin training epoch 87\n","2024-11-06 06:33:38 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:33:57 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)\n","2024-11-06 06:33:57 | INFO | train | epoch 087 | loss 4.513 | nll_loss 3.207 | ppl 9.24 | wps 37734.1 | ups 11.81 | wpb 3194.5 | bsz 95.9 | num_updates 19053 | lr 6.87289e-05 | gnorm 1.276 | train_wall 17 | gb_free 14.1 | wall 2448\n","2024-11-06 06:33:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:33:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 088:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:33:57 | INFO | fairseq.trainer | begin training epoch 88\n","2024-11-06 06:33:57 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:34:15 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)\n","2024-11-06 06:34:15 | INFO | train | epoch 088 | loss 4.498 | nll_loss 3.19 | ppl 9.12 | wps 37917.8 | ups 11.87 | wpb 3194.5 | bsz 95.9 | num_updates 19272 | lr 6.83373e-05 | gnorm 1.261 | train_wall 17 | gb_free 14 | wall 2466\n","2024-11-06 06:34:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:34:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 089:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:34:15 | INFO | fairseq.trainer | begin training epoch 89\n","2024-11-06 06:34:15 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:34:34 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)\n","2024-11-06 06:34:34 | INFO | train | epoch 089 | loss 4.499 | nll_loss 3.189 | ppl 9.12 | wps 37713.3 | ups 11.81 | wpb 3194.5 | bsz 95.9 | num_updates 19491 | lr 6.79523e-05 | gnorm 1.3 | train_wall 17 | gb_free 14.1 | wall 2485\n","2024-11-06 06:34:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:34:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 090:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:34:34 | INFO | fairseq.trainer | begin training epoch 90\n","2024-11-06 06:34:34 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:34:53 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)\n","2024-11-06 06:34:53 | INFO | train | epoch 090 | loss 4.487 | nll_loss 3.177 | ppl 9.04 | wps 37067.5 | ups 11.6 | wpb 3194.5 | bsz 95.9 | num_updates 19710 | lr 6.75737e-05 | gnorm 1.277 | train_wall 17 | gb_free 14.2 | wall 2504\n","2024-11-06 06:34:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:34:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 091:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:34:53 | INFO | fairseq.trainer | begin training epoch 91\n","2024-11-06 06:34:53 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:35:11 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)\n","2024-11-06 06:35:11 | INFO | train | epoch 091 | loss 4.478 | nll_loss 3.165 | ppl 8.97 | wps 37085.4 | ups 11.61 | wpb 3194.5 | bsz 95.9 | num_updates 19929 | lr 6.72014e-05 | gnorm 1.273 | train_wall 17 | gb_free 14.1 | wall 2523\n","2024-11-06 06:35:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:35:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 092:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:35:12 | INFO | fairseq.trainer | begin training epoch 92\n","2024-11-06 06:35:12 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 092:  32% 69/219 [00:05\u003c00:11, 12.84it/s]2024-11-06 06:35:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-11-06 06:35:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 092 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:   1% 1/86 [00:00\u003c00:54,  1.57it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:   2% 2/86 [00:01\u003c00:45,  1.86it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:   3% 3/86 [00:01\u003c00:43,  1.93it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:   5% 4/86 [00:02\u003c00:39,  2.09it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:   6% 5/86 [00:02\u003c00:35,  2.29it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:   7% 6/86 [00:03\u003c00:42,  1.89it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:   8% 7/86 [00:03\u003c00:42,  1.88it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:   9% 8/86 [00:04\u003c00:41,  1.90it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  10% 9/86 [00:04\u003c00:39,  1.95it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  12% 10/86 [00:05\u003c00:37,  2.03it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  13% 11/86 [00:05\u003c00:42,  1.77it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  14% 12/86 [00:06\u003c00:43,  1.68it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  15% 13/86 [00:07\u003c00:42,  1.70it/s]\u001b[A2024-11-06 06:35:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:35:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:35:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 092 | valid on 'valid' subset:  16% 14/86 [00:07\u003c00:48,  1.47it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  17% 15/86 [00:08\u003c00:44,  1.58it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  19% 16/86 [00:09\u003c00:43,  1.63it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  20% 17/86 [00:09\u003c00:38,  1.79it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  21% 18/86 [00:10\u003c00:38,  1.77it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  22% 19/86 [00:10\u003c00:34,  1.95it/s]\u001b[A2024-11-06 06:35:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:35:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:35:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 092 | valid on 'valid' subset:  23% 20/86 [00:10\u003c00:34,  1.90it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  24% 21/86 [00:11\u003c00:33,  1.96it/s]\u001b[A2024-11-06 06:35:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:35:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:35:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 092 | valid on 'valid' subset:  26% 22/86 [00:12\u003c00:33,  1.88it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  27% 23/86 [00:12\u003c00:33,  1.90it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  28% 24/86 [00:12\u003c00:30,  2.04it/s]\u001b[A2024-11-06 06:35:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:35:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:35:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 092 | valid on 'valid' subset:  29% 25/86 [00:13\u003c00:32,  1.87it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  30% 26/86 [00:14\u003c00:29,  2.01it/s]\u001b[A2024-11-06 06:35:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:35:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:35:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 092 | valid on 'valid' subset:  31% 27/86 [00:14\u003c00:33,  1.78it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  33% 28/86 [00:15\u003c00:33,  1.74it/s]\u001b[A\n","epoch 092:  32% 69/219 [00:22\u003c00:11, 12.84it/s, loss=4.442, nll_loss=3.124, ppl=8.72, wps=38433.7, ups=12.01, wpb=3199.6, bsz=102.9, num_updates=20000, lr=6.7082e-05, gnorm=1.283, train_wall=8, gb_free=14.1, wall=2529]\n","epoch 092 | valid on 'valid' subset:  35% 30/86 [00:16\u003c00:34,  1.61it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  36% 31/86 [00:17\u003c00:33,  1.64it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  37% 32/86 [00:17\u003c00:29,  1.82it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  38% 33/86 [00:18\u003c00:29,  1.79it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  40% 34/86 [00:18\u003c00:30,  1.69it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  41% 35/86 [00:19\u003c00:32,  1.55it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  42% 36/86 [00:20\u003c00:33,  1.50it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  43% 37/86 [00:21\u003c00:34,  1.42it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  44% 38/86 [00:22\u003c00:36,  1.30it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  45% 39/86 [00:22\u003c00:35,  1.33it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  47% 40/86 [00:23\u003c00:35,  1.31it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  48% 41/86 [00:24\u003c00:32,  1.40it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  49% 42/86 [00:24\u003c00:33,  1.31it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  50% 43/86 [00:25\u003c00:30,  1.43it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  51% 44/86 [00:26\u003c00:28,  1.47it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  52% 45/86 [00:26\u003c00:26,  1.55it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  53% 46/86 [00:27\u003c00:25,  1.55it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  55% 47/86 [00:27\u003c00:24,  1.58it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  56% 48/86 [00:28\u003c00:23,  1.60it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  57% 49/86 [00:29\u003c00:22,  1.66it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  58% 50/86 [00:29\u003c00:21,  1.64it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  59% 51/86 [00:30\u003c00:21,  1.63it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  60% 52/86 [00:31\u003c00:21,  1.62it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  62% 53/86 [00:31\u003c00:20,  1.59it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  63% 54/86 [00:32\u003c00:20,  1.57it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  64% 55/86 [00:32\u003c00:18,  1.66it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  65% 56/86 [00:33\u003c00:18,  1.65it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  66% 57/86 [00:34\u003c00:18,  1.53it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  67% 58/86 [00:35\u003c00:19,  1.43it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  69% 59/86 [00:35\u003c00:20,  1.34it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  70% 60/86 [00:36\u003c00:20,  1.29it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  71% 61/86 [00:37\u003c00:20,  1.23it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  72% 62/86 [00:38\u003c00:19,  1.20it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  73% 63/86 [00:39\u003c00:18,  1.21it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  74% 64/86 [00:40\u003c00:17,  1.24it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  76% 65/86 [00:40\u003c00:15,  1.31it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  77% 66/86 [00:41\u003c00:14,  1.41it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  78% 67/86 [00:41\u003c00:12,  1.48it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  79% 68/86 [00:42\u003c00:11,  1.58it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  80% 69/86 [00:43\u003c00:10,  1.62it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  81% 70/86 [00:43\u003c00:10,  1.50it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  83% 71/86 [00:44\u003c00:09,  1.53it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  84% 72/86 [00:45\u003c00:09,  1.53it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  85% 73/86 [00:45\u003c00:08,  1.52it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  86% 74/86 [00:46\u003c00:07,  1.55it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  87% 75/86 [00:47\u003c00:07,  1.55it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  88% 76/86 [00:47\u003c00:06,  1.55it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  90% 77/86 [00:48\u003c00:05,  1.56it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  91% 78/86 [00:48\u003c00:05,  1.54it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  92% 79/86 [00:49\u003c00:05,  1.36it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  93% 80/86 [00:50\u003c00:04,  1.26it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  94% 81/86 [00:51\u003c00:04,  1.13it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  95% 82/86 [00:52\u003c00:03,  1.08it/s]\u001b[A\n","epoch 092 | valid on 'valid' subset:  97% 83/86 [00:54\u003c00:03,  1.01s/it]\u001b[A\n","epoch 092 | valid on 'valid' subset:  98% 84/86 [00:55\u003c00:02,  1.07s/it]\u001b[A\n","epoch 092 | valid on 'valid' subset:  99% 85/86 [00:56\u003c00:01,  1.00s/it]\u001b[A\n","epoch 092 | valid on 'valid' subset: 100% 86/86 [00:56\u003c00:00,  1.08it/s]\u001b[A\n","                                                                        \u001b[A2024-11-06 06:36:14 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 4.818 | nll_loss 3.403 | ppl 10.58 | bleu 19.37 | wps 3624.2 | wpb 2386.4 | bsz 73.7 | num_updates 20000 | best_bleu 19.37\n","2024-11-06 06:36:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 20000 updates\n","2024-11-06 06:36:14 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_92_20000.pt\n","2024-11-06 06:36:15 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_92_20000.pt\n","2024-11-06 06:36:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-task-nmt/checkpoint_92_20000.pt (epoch 92 @ 20000 updates, score 19.37) (writing took 1.6066765040000064 seconds)\n","2024-11-06 06:36:30 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)\n","2024-11-06 06:36:30 | INFO | train | epoch 092 | loss 4.475 | nll_loss 3.161 | ppl 8.94 | wps 8851.7 | ups 2.77 | wpb 3194.5 | bsz 95.9 | num_updates 20148 | lr 6.68352e-05 | gnorm 1.301 | train_wall 19 | gb_free 14.1 | wall 2602\n","2024-11-06 06:36:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:36:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 093:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:36:31 | INFO | fairseq.trainer | begin training epoch 93\n","2024-11-06 06:36:31 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:36:50 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)\n","2024-11-06 06:36:50 | INFO | train | epoch 093 | loss 4.464 | nll_loss 3.149 | ppl 8.87 | wps 35139.6 | ups 11 | wpb 3194.5 | bsz 95.9 | num_updates 20367 | lr 6.64749e-05 | gnorm 1.267 | train_wall 18 | gb_free 14 | wall 2621\n","2024-11-06 06:36:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:36:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 094:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:36:51 | INFO | fairseq.trainer | begin training epoch 94\n","2024-11-06 06:36:51 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:37:09 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)\n","2024-11-06 06:37:09 | INFO | train | epoch 094 | loss 4.457 | nll_loss 3.141 | ppl 8.82 | wps 37834.4 | ups 11.84 | wpb 3194.5 | bsz 95.9 | num_updates 20586 | lr 6.61204e-05 | gnorm 1.29 | train_wall 17 | gb_free 14.1 | wall 2640\n","2024-11-06 06:37:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:37:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 095:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:37:09 | INFO | fairseq.trainer | begin training epoch 95\n","2024-11-06 06:37:09 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:37:28 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)\n","2024-11-06 06:37:28 | INFO | train | epoch 095 | loss 4.455 | nll_loss 3.137 | ppl 8.8 | wps 37397.3 | ups 11.71 | wpb 3194.5 | bsz 95.9 | num_updates 20805 | lr 6.57714e-05 | gnorm 1.291 | train_wall 17 | gb_free 14.1 | wall 2659\n","2024-11-06 06:37:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:37:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 096:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:37:28 | INFO | fairseq.trainer | begin training epoch 96\n","2024-11-06 06:37:28 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:37:47 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)\n","2024-11-06 06:37:47 | INFO | train | epoch 096 | loss 4.448 | nll_loss 3.13 | ppl 8.76 | wps 36714 | ups 11.49 | wpb 3194.5 | bsz 95.9 | num_updates 21024 | lr 6.5428e-05 | gnorm 1.308 | train_wall 17 | gb_free 14.1 | wall 2678\n","2024-11-06 06:37:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:37:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 097:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:37:47 | INFO | fairseq.trainer | begin training epoch 97\n","2024-11-06 06:37:47 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:38:06 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)\n","2024-11-06 06:38:06 | INFO | train | epoch 097 | loss 4.436 | nll_loss 3.115 | ppl 8.66 | wps 36985.5 | ups 11.58 | wpb 3194.5 | bsz 95.9 | num_updates 21243 | lr 6.50899e-05 | gnorm 1.305 | train_wall 17 | gb_free 14.1 | wall 2697\n","2024-11-06 06:38:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:38:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 098:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:38:06 | INFO | fairseq.trainer | begin training epoch 98\n","2024-11-06 06:38:06 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:38:24 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)\n","2024-11-06 06:38:24 | INFO | train | epoch 098 | loss 4.432 | nll_loss 3.11 | ppl 8.63 | wps 37954.3 | ups 11.88 | wpb 3194.5 | bsz 95.9 | num_updates 21462 | lr 6.47569e-05 | gnorm 1.301 | train_wall 17 | gb_free 14.1 | wall 2715\n","2024-11-06 06:38:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:38:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 099:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:38:24 | INFO | fairseq.trainer | begin training epoch 99\n","2024-11-06 06:38:24 | INFO | fairseq_cli.train | Start iterating over samples\n","2024-11-06 06:38:43 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)\n","2024-11-06 06:38:43 | INFO | train | epoch 099 | loss 4.424 | nll_loss 3.101 | ppl 8.58 | wps 37561.5 | ups 11.76 | wpb 3194.5 | bsz 95.9 | num_updates 21681 | lr 6.4429e-05 | gnorm 1.297 | train_wall 17 | gb_free 14.1 | wall 2734\n","2024-11-06 06:38:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:38:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 219\n","epoch 100:   0% 0/219 [00:00\u003c?, ?it/s]2024-11-06 06:38:43 | INFO | fairseq.trainer | begin training epoch 100\n","2024-11-06 06:38:43 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 100:  99% 217/219 [00:18\u003c00:00, 12.52it/s, loss=4.411, nll_loss=3.086, ppl=8.49, wps=36685.4, ups=11.43, wpb=3210.8, bsz=94.9, num_updates=21800, lr=6.42529e-05, gnorm=1.263, train_wall=8, gb_free=14.1, wall=2744]2024-11-06 06:39:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","2024-11-06 06:39:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","\n","epoch 100 | valid on 'valid' subset:   0% 0/86 [00:00\u003c?, ?it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:   1% 1/86 [00:00\u003c01:02,  1.37it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:   2% 2/86 [00:01\u003c00:54,  1.54it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:   3% 3/86 [00:01\u003c00:54,  1.51it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:   5% 4/86 [00:02\u003c00:51,  1.59it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:   6% 5/86 [00:03\u003c00:47,  1.72it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:   7% 6/86 [00:03\u003c00:52,  1.54it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:   8% 7/86 [00:04\u003c00:50,  1.58it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:   9% 8/86 [00:04\u003c00:46,  1.67it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  10% 9/86 [00:05\u003c00:42,  1.82it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  12% 10/86 [00:05\u003c00:37,  2.03it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  13% 11/86 [00:06\u003c00:37,  2.01it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  14% 12/86 [00:06\u003c00:35,  2.08it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  15% 13/86 [00:07\u003c00:33,  2.18it/s]\u001b[A2024-11-06 06:39:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:39:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:39:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 100 | valid on 'valid' subset:  16% 14/86 [00:07\u003c00:34,  2.10it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  17% 15/86 [00:08\u003c00:32,  2.20it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  19% 16/86 [00:08\u003c00:32,  2.12it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  20% 17/86 [00:09\u003c00:31,  2.18it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  21% 18/86 [00:09\u003c00:33,  2.01it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  22% 19/86 [00:09\u003c00:30,  2.17it/s]\u001b[A2024-11-06 06:39:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:39:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:39:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 100 | valid on 'valid' subset:  23% 20/86 [00:10\u003c00:32,  2.04it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  24% 21/86 [00:11\u003c00:38,  1.68it/s]\u001b[A2024-11-06 06:39:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:39:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:39:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 100 | valid on 'valid' subset:  26% 22/86 [00:11\u003c00:37,  1.71it/s]\u001b[A\n","epoch 100:  99% 217/219 [00:31\u003c00:00, 12.52it/s, loss=4.407, nll_loss=3.083, ppl=8.47, wps=40132.6, ups=12.66, wpb=3170.7, bsz=97.7, num_updates=21900, lr=6.41061e-05, gnorm=1.3, train_wall=7, gb_free=14.1, wall=2752]  \n","epoch 100 | valid on 'valid' subset:  28% 24/86 [00:12\u003c00:32,  1.94it/s]\u001b[A2024-11-06 06:39:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:39:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:39:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 100 | valid on 'valid' subset:  29% 25/86 [00:13\u003c00:34,  1.78it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  30% 26/86 [00:13\u003c00:31,  1.89it/s]\u001b[A2024-11-06 06:39:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n","2024-11-06 06:39:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n","2024-11-06 06:39:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","\n","epoch 100 | valid on 'valid' subset:  31% 27/86 [00:14\u003c00:34,  1.72it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  33% 28/86 [00:15\u003c00:33,  1.73it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  34% 29/86 [00:15\u003c00:32,  1.73it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  35% 30/86 [00:16\u003c00:41,  1.35it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  36% 31/86 [00:17\u003c00:41,  1.34it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  37% 32/86 [00:18\u003c00:37,  1.44it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  38% 33/86 [00:19\u003c00:40,  1.32it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  40% 34/86 [00:19\u003c00:39,  1.31it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  41% 35/86 [00:20\u003c00:42,  1.20it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  42% 36/86 [00:21\u003c00:38,  1.31it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  43% 37/86 [00:22\u003c00:34,  1.41it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  44% 38/86 [00:22\u003c00:32,  1.50it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  45% 39/86 [00:23\u003c00:28,  1.63it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  47% 40/86 [00:23\u003c00:27,  1.68it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  48% 41/86 [00:24\u003c00:24,  1.83it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  49% 42/86 [00:24\u003c00:27,  1.63it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  50% 43/86 [00:25\u003c00:25,  1.68it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  51% 44/86 [00:26\u003c00:25,  1.67it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  52% 45/86 [00:26\u003c00:24,  1.70it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  53% 46/86 [00:27\u003c00:26,  1.51it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  55% 47/86 [00:28\u003c00:25,  1.51it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  56% 48/86 [00:28\u003c00:24,  1.53it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  57% 49/86 [00:29\u003c00:23,  1.60it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  58% 50/86 [00:29\u003c00:22,  1.61it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  59% 51/86 [00:30\u003c00:21,  1.62it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  60% 52/86 [00:31\u003c00:21,  1.61it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  62% 53/86 [00:32\u003c00:24,  1.33it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  63% 54/86 [00:33\u003c00:25,  1.24it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  64% 55/86 [00:34\u003c00:25,  1.24it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  65% 56/86 [00:34\u003c00:25,  1.16it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  66% 57/86 [00:35\u003c00:24,  1.17it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  67% 58/86 [00:36\u003c00:25,  1.10it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  69% 59/86 [00:37\u003c00:25,  1.08it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  70% 60/86 [00:38\u003c00:23,  1.09it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  71% 61/86 [00:39\u003c00:22,  1.09it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  72% 62/86 [00:40\u003c00:22,  1.07it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  73% 63/86 [00:41\u003c00:20,  1.11it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  74% 64/86 [00:42\u003c00:20,  1.09it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  76% 65/86 [00:43\u003c00:17,  1.19it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  77% 66/86 [00:43\u003c00:15,  1.30it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  78% 67/86 [00:44\u003c00:13,  1.41it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  79% 68/86 [00:44\u003c00:12,  1.49it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  80% 69/86 [00:45\u003c00:10,  1.55it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  81% 70/86 [00:46\u003c00:11,  1.42it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  83% 71/86 [00:46\u003c00:09,  1.50it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  84% 72/86 [00:47\u003c00:09,  1.52it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  85% 73/86 [00:48\u003c00:08,  1.54it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  86% 74/86 [00:48\u003c00:07,  1.54it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  87% 75/86 [00:49\u003c00:07,  1.54it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  88% 76/86 [00:49\u003c00:06,  1.61it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  90% 77/86 [00:50\u003c00:05,  1.56it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  91% 78/86 [00:51\u003c00:05,  1.55it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  92% 79/86 [00:52\u003c00:04,  1.42it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  93% 80/86 [00:53\u003c00:04,  1.31it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  94% 81/86 [00:53\u003c00:04,  1.24it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  95% 82/86 [00:54\u003c00:03,  1.20it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  97% 83/86 [00:56\u003c00:02,  1.06it/s]\u001b[A\n","epoch 100 | valid on 'valid' subset:  98% 84/86 [00:57\u003c00:02,  1.03s/it]\u001b[A\n","epoch 100 | valid on 'valid' subset:  99% 85/86 [00:58\u003c00:01,  1.04s/it]\u001b[A\n","epoch 100 | valid on 'valid' subset: 100% 86/86 [00:59\u003c00:00,  1.04it/s]\u001b[A\n","                                                                        \u001b[A2024-11-06 06:40:00 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 4.798 | nll_loss 3.374 | ppl 10.37 | bleu 19.71 | wps 3496.5 | wpb 2386.4 | bsz 73.7 | num_updates 21900 | best_bleu 19.71\n","2024-11-06 06:40:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 21900 updates\n","2024-11-06 06:40:00 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_best.pt\n","2024-11-06 06:40:01 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Research/eng-to-nso/target-tok/checkpoints-task-nmt/checkpoint_best.pt\n","2024-11-06 06:40:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-task-nmt/checkpoint_best.pt (epoch 100 @ 21900 updates, score 19.71) (writing took 0.8203324410001187 seconds)\n","2024-11-06 06:40:01 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)\n","2024-11-06 06:40:01 | INFO | train | epoch 100 | loss 4.412 | nll_loss 3.087 | ppl 8.5 | wps 8917.4 | ups 2.79 | wpb 3194.5 | bsz 95.9 | num_updates 21900 | lr 6.41061e-05 | gnorm 1.283 | train_wall 17 | gb_free 14.1 | wall 2812\n","2024-11-06 06:40:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n","2024-11-06 06:40:01 | INFO | fairseq_cli.train | done training in 2807.6 seconds\n"]}],"source":["!fairseq-train data-bin-nmt \\\n","--arch transformer \\\n","--activation-fn relu \\\n","--share-decoder-input-output-embed \\\n","--share-all-embeddings \\\n","--encoder-layers 3 \\\n","--encoder-attention-heads 4 \\\n","--encoder-embed-dim 256 \\\n","--encoder-ffn-embed-dim 1024 \\\n","--decoder-layers 3 \\\n","--decoder-attention-heads 4 \\\n","--decoder-embed-dim 256 \\\n","--decoder-ffn-embed-dim 1024 \\\n","--dropout 0.25 \\\n","--seed 2024 \\\n","--optimizer 'adam' \\\n","--adam-betas '(0.9, 0.999)' \\\n","--lr-scheduler 'inverse_sqrt' \\\n","--patience 5 \\\n","--warmup-updates 1000 \\\n","--criterion 'label_smoothed_cross_entropy' \\\n","--label-smoothing 0.1 \\\n","--lr 0.0003 \\\n","--weight-decay 0.0 \\\n","--max-tokens 4096 \\\n","--max-tokens-valid 3600 \\\n","--required-batch-size-multiple 1 \\\n","--best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n","--max-epoch 100 \\\n","--validate-interval 25 \\\n","--save-interval 25 \\\n","--validate-interval-updates 2000 \\\n","--save-interval-updates 2000 \\\n","--log-interval 100 \\\n","--curriculum 0 \\\n","--no-epoch-checkpoints \\\n","--eval-bleu \\\n","--eval-bleu-args '{\"beam\": 5, \"max_len_a\": 0, \"max_len_b\": 100, \"len_pen\": 1}' \\\n","--eval-bleu-detok space \\\n","--eval-bleu-remove-bpe sentencepiece \\\n","--save-dir checkpoints-task-nmt \\\n","--ddp-backend=no_c10d \\\n","--wandb-project 'fairseq-standard-subword-tok-eng-to-nso'"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNbQkS4zaEKPvEV3VYFg2k3","collapsed_sections":["pEIv3gY-aNRk","5tmOGRAFauY9","cIUaICtsUHwW"],"gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}